{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBpHNI3wthDr"
      },
      "source": [
        "<center><h1 style=\"text-align: center;\"><u>TFM - Detección temprana del cáncer a partir de los resultados de análisis de sangre</u></h1>\n",
        "<center><img src=\"https://nachocarnes.es/wp-content/uploads/2018/04/ejWGXui6_400x400.png\" alt=\"Drawing\" style=\"align=left\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INTRODUCCIÓN\n",
        "\n",
        "En este notebook se lleva a cabo el contraste del modelo CancerA1DE para la detección prematura de cáncer frente a otros modelos de predicción.\n",
        "\n",
        "El artículo sobre el que se basa esta investigación es \"Early Cancer Detection from Multianalyte Blood Test Results\" presente en : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6548890/\n",
        "\n",
        "El contenido abarca un repaso de los datos, su análisis a todos los niveles y los diferentes enfoques para entrenar, validar y testear los distintos modelos.\n",
        "\n",
        "Los datos se encontrarán en la siguiente ruta en local : C:\\Users\\danie\\OneDrive\\Documentos\\Master\\Lusku\\TFM\\Proposiciones\\Deteccion Cancer\\Datos\n",
        "\n",
        "IMPORTANTE : Cada vez que tenga que rellenarse a mano el valor de una variable, se mostrará con : \n",
        "\n",
        "                    (I) Introducir valor de 'nombreDeLaVariable'\n",
        "\n",
        "Para descargar las librerías usadas en este proyecto se puede hacer usar el comando \"pip install -r requirements.txt\" en el terminal\n",
        "\n",
        "Los bloques de código que empiezan por # P , forman parte del proceso principal y son necesarios ejecutarse para seguir el flujo del proceso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# P\n",
        "# Carga de librerias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "# Entrenar el modelo\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Selección de las variables por tipo\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import make_column_selector\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Modelos\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, roc_curve, auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score, r2_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functions\n",
        "\n",
        "# Conversion del dataFrame a tipo numeric\n",
        "def convert_to_numeric(column):\n",
        "    if column.dtype in ['object','category']:\n",
        "        # Verificar si hay letras en todos los registros\n",
        "        contains_letters = any(isinstance(val, str) and any(c.isalpha() for c in val) for val in column)\n",
        "        if not contains_letters :\n",
        "            return pd.to_numeric(column, errors='coerce')\n",
        "    return column\n",
        "\n",
        "def discretizar_df_arboles_1(df, max_depth=40, n_bins=18, rango_discretizacion=(-np.inf, np.inf)):\n",
        "    df_discretizado = pd.DataFrame()\n",
        "    \n",
        "    # Iterar sobre todas las columnas del dataframe original\n",
        "    for columna in df.columns:\n",
        "        if df[columna].dtype.kind in 'biufc' or columna.name == 'Tumor type': # (CHULO) Comprueba si el tipo de datos de la columna es numérico ('b' para booleano, 'i' para entero, 'u' para sin signo, 'f' para flotante o 'c' para complejo)\n",
        "            # Si la columna es numérica, realizar la discretización\n",
        "            dt = DecisionTreeRegressor(max_depth=max_depth)\n",
        "            dt.fit(df[columna].values.reshape(-1, 1), df[columna])\n",
        "            puntos_corte = dt.tree_.threshold[dt.tree_.threshold != -2] # Extrae los puntos de corte del árbol de decisión para la columna numérica específica, ignorando aquellos puntos de corte asociados con nodos hoja (-2)\n",
        "            puntos_corte = np.sort(puntos_corte)\n",
        "            puntos_corte = np.concatenate(([rango_discretizacion[0]], puntos_corte, [rango_discretizacion[1]]))\n",
        "            df_discretizado[f'{columna}'] = pd.cut(df[columna], bins=puntos_corte, labels=range(len(puntos_corte)-1))\n",
        "        else:\n",
        "            # Si la columna no es numérica, simplemente copiarla al dataframe resultante\n",
        "            df_discretizado[columna] = df[columna]\n",
        "            \n",
        "    return df_discretizado.apply(convert_to_numeric)\n",
        "\n",
        "# Discretizar dataFrame y mostrar correlación respecto a la variable objetivo\n",
        "def discretizar_df_arboles(df, imprimir=\"SI\", max_depth=15, rango_discretizacion=(-np.inf, np.inf)):\n",
        "    df_discretizado = pd.DataFrame()\n",
        "    \n",
        "    # Iterar sobre todas las columnas del dataframe original\n",
        "    for columna in df.select_dtypes(include=['number']).columns:\n",
        "        if df[columna].dtype.kind in 'biufc' or columna.name != 'Tumor type': # (CHULO) Comprueba si el tipo de datos de la columna es numérico ('b' para booleano, 'i' para entero, 'u' para sin signo, 'f' para flotante o 'c' para complejo)\n",
        "            # Si la columna es numérica, realizar la discretización\n",
        "            dt = DecisionTreeRegressor(max_depth=max_depth)\n",
        "            dt.fit(df[columna].values.reshape(-1, 1), df[columna])\n",
        "            puntos_corte = dt.tree_.threshold[dt.tree_.threshold != -2] # Extrae los puntos de corte del árbol de decisión para la columna numérica específica, ignorando aquellos puntos de corte asociados con nodos hoja (-2)\n",
        "            puntos_corte = np.sort(puntos_corte)\n",
        "            puntos_corte = np.concatenate(([rango_discretizacion[0]], puntos_corte, [rango_discretizacion[1]]))\n",
        "            # print(f\"\\t Columna : {columna} \\n Puntos de Corte : \\n {puntos_corte}\")\n",
        "            df_discretizado[f'{columna}'] = pd.cut(df[columna], bins=puntos_corte, labels=range(len(puntos_corte)-1))\n",
        "        else:\n",
        "            # Si la columna no es numérica, simplemente copiarla al dataframe resultante\n",
        "            df_discretizado[columna] = df[columna]\n",
        "            \n",
        "   # Calcular el coeficiente de correlación entre las variables numéricas discretas y la variable objetivo binaria\n",
        "    correlaciones_discretas = df_discretizado.corrwith(df_discretizado['Tumor type'])\n",
        "\n",
        "    # Ordenar las correlaciones de mayor a menor\n",
        "    correlaciones_discretas_ordenadas = correlaciones_discretas.abs().sort_values(ascending=False)\n",
        "\n",
        "    # Obtener las top 20 variables numéricas discretas con las correlaciones más altas\n",
        "    top_20_correlaciones_discretas = correlaciones_discretas_ordenadas.nlargest(20)\n",
        "\n",
        "    if imprimir == \"SI\":\n",
        "        # Imprimir las top 20 correlaciones\n",
        "        print(top_20_correlaciones_discretas)\n",
        "\n",
        "    return df_discretizado\n",
        "\n",
        "\n",
        "def escalado_dataFrame(df) :\n",
        "    if df.empty:\n",
        "        raise ValueError(\"El DataFrame está vacío, no se puede realizar el escalado.\")\n",
        "\n",
        "    # Crear un objeto StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Normalizar df_imputacion_iterativa\n",
        "    df_normalized = df.select_dtypes(include=['number']).copy()  # Crear una copia del DataFrame original\n",
        "    \n",
        "    if not df_normalized.empty:\n",
        "        df_normalized[df_normalized.columns] = scaler.fit_transform(df_normalized)\n",
        "    else :\n",
        "        print(\"Esto está vacío\")\n",
        "    return df_normalized\n",
        "\n",
        "def calcular_ganancia_informacion(df_features, target, imprimir = 'SI'):\n",
        "    \n",
        "    # Extraer las características de interés del DataFrame\n",
        "    X_interest = df_features.values\n",
        "    \n",
        "    # Extraer la variable objetivo del DataFrame principal\n",
        "    y = target.values\n",
        "    \n",
        "    # Calcular la Ganancia de Información utilizando Mutual Information\n",
        "    information_gain = mutual_info_classif(X_interest, y, discrete_features=False, random_state=42, n_neighbors=7)\n",
        "    \n",
        "    # Crear un DataFrame para visualizar los resultados\n",
        "    ig_results = pd.DataFrame({'Feature': df_features.columns, 'Information Gain': information_gain})\n",
        "    \n",
        "    # Ordenar los resultados por Ganancia de Información en orden descendente\n",
        "    ig_results_sorted = ig_results.sort_values(by='Information Gain', ascending=False)\n",
        "\n",
        "    if imprimir == \"SI\":\n",
        "        print(ig_results_sorted)\n",
        "\n",
        "# División del conjunto de datos en entrenamiento, validacion y test \n",
        "\n",
        "\n",
        "def split_data(X, y, train_size=0.6, val_size=0.2, test_size=0.2, random_state=None):\n",
        "    \"\"\"\n",
        "    Divide un conjunto de datos en entrenamiento, validación y test.\n",
        "\n",
        "    Args:\n",
        "        X: Matriz de características.\n",
        "        y: Vector de etiquetas.\n",
        "        train_size: Porcentaje de datos para entrenamiento (por defecto: 0.6).\n",
        "        val_size: Porcentaje de datos para validación (por defecto: 0.2).\n",
        "        test_size: Porcentaje de datos para test (por defecto: 0.2).\n",
        "        random_state: Semilla para la aleatorización (por defecto: None).\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (X_train, X_val, X_test, y_train, y_val, y_test).\n",
        "    \"\"\"\n",
        "    assert train_size + val_size + test_size == 1.0, \"La suma de train_size, val_size y test_size debe ser igual a 1.0\"\n",
        "\n",
        "    # Dividir los datos en entrenamiento y test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Calcular porcentaje respecto al tamaño original\n",
        "    val_size_relative = val_size / (1.0 - test_size)\n",
        "\n",
        "    # Dividir los datos de entrenamiento en entrenamiento y validación\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size_relative, random_state=random_state)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "def mostrar_cross_validation(model, X_train, y_train):\n",
        "    cv_scores = cross_val_score(\n",
        "                estimator = model,\n",
        "                X         = X_train,\n",
        "                y         = y_train,\n",
        "                scoring   = 'neg_root_mean_squared_error',\n",
        "                cv        = 5\n",
        "            )\n",
        "    print(\"Cross validation : \")\n",
        "    print(f\"Métricas validación cruzada: {cv_scores}\")\n",
        "    print(f\"Média métricas de validación cruzada: {cv_scores.mean()}\")\n",
        "    print(\"\")\n",
        "\n",
        "def mostrar_score(X_train, y_train, model):\n",
        "    score = round(model.score(X_train, y_train), 3)*100\n",
        "    print(f\"Tanto por ciento de acierto : {score} %\")\n",
        "    print(\"\")\n",
        "\n",
        "def mostrar_estadisticas(y_val, y_pred) :\n",
        "    mse = round(mean_squared_error(y_val, y_pred),3)\n",
        "    accuracy = round(accuracy_score(y_val, y_pred),3)\n",
        "    precision = round(precision_score(y_val, y_pred),3)\n",
        "    recall = round(recall_score(y_val, y_pred),3)\n",
        "    f1 = round(f1_score(y_val, y_pred),3)\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(\"Error cuadrático medio en el conjunto de validación:\", mse)\n",
        "    print(\"Matriz de Confusión :\\n\", conf_matrix)\n",
        "    print(\"\")\n",
        "\n",
        "def mostrar_curva_ROC(y_val, y_pred) :\n",
        "    fpr, tpr, _ = roc_curve(y_val, y_pred)\n",
        "\n",
        "    # Calcular el área bajo la curva ROC (AUC)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plotear la curva ROC\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Tasa de Falsos Positivos')\n",
        "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "    plt.title('Curva ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    \n",
        "def evaluate_model(model, X, y, set_name):\n",
        "    y_pred = model.predict(X)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    precision = precision_score(y, y_pred)\n",
        "    recall = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "    print(f\"Metrics for {set_name} set:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Función para mostrar métricas y añadirlas a un DataFrame\n",
        "def mostrar_estadisticas_guardar_tabla(y_val, y_pred, set_name, model_name, print_roc = 'NO'):\n",
        "    '''\n",
        "    Ejemplo de uso :\n",
        "\n",
        "    # Fase de entrenamiento\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(y_train, kmeans.predict(X_train_prep), \"Training\", results_df, model_name)\n",
        "\n",
        "    # Fase de validación\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(y_val, kmeans.predict(X_val_prep), \"Validation\", results_df, model_name)\n",
        "\n",
        "    # Fase de prueba\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(y_test, kmeans.predict(X_test_prep), \"Test\", results_df, model_name)\n",
        "\n",
        "    # Importante: Al final de todos los modelos (fuera del método): Guardar los resultados en un archivo Excel\n",
        "    results_df.to_excel('model_results.xlsx', index=False)\n",
        "\n",
        "    '''\n",
        "    global tabla_results_df\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred, average='weighted')\n",
        "    recall = recall_score(y_val, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "    rand_index = adjusted_rand_score(y_val, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "    fpr, tpr, _ = roc_curve(y_val, y_pred)\n",
        "    # Calcular el área bajo la curva ROC (AUC)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    if conf_matrix.shape == (2, 2):  # Asegúrate de que es una matriz de confusión 2x2\n",
        "        tn, fp, fn, tp = conf_matrix.ravel()\n",
        "    else:\n",
        "        tn = fp = fn = tp = None  # Si no es una matriz 2x2, asigna valores None\n",
        "    \n",
        "    global_score = calcular_puntuacion_global(accuracy, precision, recall, f1, rand_index, r2, mse, tn, fp, fn, tp, roc_auc)\n",
        "    \n",
        "    # Imprimir todas las métricas\n",
        "    print(f\"Metrics for {set_name} set :\")\n",
        "    print(f\" - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\" - Precision: {precision:.4f}\")\n",
        "    print(f\" - Recall: {recall:.4f}\")\n",
        "    print(f\" - F1-Score: {f1:.4f}\")\n",
        "    print(f\" - Adjusted Rand Index: {rand_index:.4f}\")\n",
        "    print(f\" - Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\" - R-squared: {r2:.4f}\")\n",
        "    print(f\" - Área bajo la curva : {roc_auc:.3f}\")\n",
        "    print(f\" - Confusion Matrix: \\n{conf_matrix}\")\n",
        "    #if tn is not None and fp is not None and fn is not None and tp is not None:\n",
        "    #   print(f\"\\tTN: {tn}\\n\\tFP: {fp}\\n\\tFN: {fn}\\n\\tTP: {tp}\")\n",
        "    print(f\" - Global Score : {global_score}\")\n",
        "    print(f\"\")\n",
        "    \n",
        "    \n",
        "    new_row = pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Set': [set_name],\n",
        "        'Accuracy': [accuracy],\n",
        "        'Precision': [precision],\n",
        "        'Recall': [recall],\n",
        "        'F1-Score': [f1],\n",
        "        'Adjusted Rand Index': [rand_index],\n",
        "        'Mean Squared Error': [mse],\n",
        "        'R-squared': [r2],\n",
        "        'AUC-ROC': [roc_auc],\n",
        "        'TN': [tn],\n",
        "        'FP': [fp],\n",
        "        'FN': [fn],\n",
        "        'TP': [tp],\n",
        "        'Global Score' : [global_score]\n",
        "    })\n",
        "\n",
        "    if (print_roc == 'SI') :\n",
        "        plot_ROC(fpr, tpr, roc_auc)\n",
        "    \n",
        "    tabla_results_df = pd.concat([tabla_results_df, new_row], ignore_index=True)\n",
        "    \n",
        "    return tabla_results_df\n",
        "\n",
        "def plot_ROC(fpr, tpr, roc_auc):\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Tasa de Falsos Positivos')\n",
        "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "    plt.title('Curva ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def calcular_puntuacion_global(accuracy, precision, recall, f1, rand_index, r2, mse, tn, fp, fn, tp, auc_roc):\n",
        "    # Definir ponderaciones para cada métrica\n",
        "    weights = {\n",
        "        'accuracy': 0.12,\n",
        "        'precision': 0.12,\n",
        "        'recall': 0.12,\n",
        "        'f1': 0.12,\n",
        "        'rand_index': 0.1,\n",
        "        'r2': 0.05,\n",
        "        'mse': 0.05,\n",
        "        'tpr': 0.1,\n",
        "        'fpr': 0.1,\n",
        "        'auc_roc': 0.12,\n",
        "    }\n",
        "    \n",
        "    # Normalizar las métricas\n",
        "    mse_norm = (1 - mse)  # Invertir MSE ya que menor es mejor\n",
        "    r2_norm = (r2 + 1) / 2  # Normalizar R2 para que esté entre 0 y 1\n",
        "    tpr_norm = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "    fpr_norm = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
        "    \n",
        "    # Calcular la puntuación global\n",
        "    global_score = (accuracy * weights['accuracy'] +\n",
        "                    precision * weights['precision'] +\n",
        "                    recall * weights['recall'] +\n",
        "                    f1 * weights['f1'] +\n",
        "                    rand_index * weights['rand_index'] +\n",
        "                    r2_norm * weights['r2'] +\n",
        "                    mse_norm * weights['mse'] +\n",
        "                    tpr_norm * weights['tpr'] +\n",
        "                    (1 - fpr_norm) * weights['fpr'] +\n",
        "                    auc_roc * weights['auc_roc'])  # Invertir FPR ya que menor es mejor\n",
        "    \n",
        "    return round(global_score * 100, 2)\n",
        "\n",
        "# Funciones para aprendizaje no supervisado\n",
        "def optimal_cluster_number(X_train, X_val, model, max_clusters=10, method='elbow', plot_grafica = 'NO'):\n",
        "    \"\"\"\n",
        "    Encuentra el número óptimo de clusters para un modelo de clustering utilizando el método del codo (Elbow Method)\n",
        "    u otros métodos.\n",
        "    \"\"\"\n",
        "    if method == 'elbow':\n",
        "        distortions = []\n",
        "        for i in range(1, max_clusters + 1):\n",
        "            model.n_clusters = i\n",
        "            model.fit(X_train)\n",
        "            distortions.append(model.inertia_)\n",
        "        if plot_grafica == 'SI':\n",
        "            # Plotting the elbow curve\n",
        "            plt.plot(range(1, len(distortions) + 1), distortions, marker='o')\n",
        "            plt.xlabel('Número de clusters')\n",
        "            plt.ylabel('Distorsión')\n",
        "            plt.title('Método del codo para encontrar el número óptimo de clusters')\n",
        "            plt.show()\n",
        "\n",
        "        # Finding the optimal number of clusters based on the elbow point\n",
        "        optimal_k = np.argmin(np.gradient(distortions)) + 1\n",
        "        if optimal_k == 1:  # Ensure that the optimal number of clusters is greater than 1\n",
        "            optimal_k = 2\n",
        "        return optimal_k\n",
        "\n",
        "    elif method == 'silhouette':\n",
        "        silhouette_scores = []\n",
        "        for i in range(2, max_clusters + 1):\n",
        "            model.n_clusters = i\n",
        "            model.fit(X_train)\n",
        "            labels = model.predict(X_val)\n",
        "            silhouette_scores.append(silhouette_score(X_val, labels))\n",
        "        \n",
        "        if plot_grafica == 'SI':\n",
        "            # Plotting the silhouette scores\n",
        "            plt.plot(range(2, len(silhouette_scores) + 2), silhouette_scores, marker='o')\n",
        "            plt.xlabel('Número de clusters')\n",
        "            plt.ylabel('Silhouette Score')\n",
        "            plt.title('Silhouette Score para encontrar el número óptimo de clusters')\n",
        "            plt.show()\n",
        "\n",
        "        # Finding the optimal number of clusters based on silhouette score\n",
        "        optimal_k = np.argmax(silhouette_scores) + 2\n",
        "        return optimal_k\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Método no válido. Métodos disponibles: 'elbow', 'silhouette', etc.\")\n",
        "\n",
        "\n",
        "# Función para motrar estadísticas para modelos de aprendizaje no supervisado   \n",
        "def mostrar_estadisticas_guardar_tabla_NS(X, labels, set_name, model_name):\n",
        "    '''\n",
        "    Ejemplo de uso :\n",
        "\n",
        "    # Fase de entrenamiento\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(X_train, kmeans.predict(X_train), \"Training\", model_name, results_df)\n",
        "\n",
        "    # Fase de validación\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(X_val, kmeans.predict(X_val), \"Validation\", model_name, results_df)\n",
        "\n",
        "    # Fase de prueba\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(X_test, kmeans.predict(X_test), \"Test\", model_name, results_df)\n",
        "\n",
        "    # Importante: Al final de todos los modelos (fuera del método): Guardar los resultados en un archivo Excel\n",
        "    results_df.to_excel('model_results.xlsx', index=False)\n",
        "    '''\n",
        "    global tabla_results_NS_df\n",
        "    silhouette_avg = silhouette_score(X, labels)\n",
        "    db_score = davies_bouldin_score(X, labels)\n",
        "    ch_score = calinski_harabasz_score(X, labels)\n",
        "\n",
        "    global_score = calcular_puntuacion_global_NS(silhouette_avg, db_score, ch_score)\n",
        "    \n",
        "    # Imprimir todas las métricas\n",
        "    print(f\"Metrics for {set_name} set ({model_name}):\")\n",
        "    print(f\" - Silhouette Score: {silhouette_avg:.4f}\")\n",
        "    print(f\" - Davies-Bouldin Index: {db_score:.4f}\")\n",
        "    print(f\" - Calinski-Harabasz Index: {ch_score:.4f}\")\n",
        "    print(f\" - Global Score: {global_score:.4f}\")\n",
        "    print(f\"\")\n",
        "    \n",
        "    new_row = pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Set': [set_name],\n",
        "        'Silhouette Score': [silhouette_avg],\n",
        "        'Davies-Bouldin Index': [db_score],\n",
        "        'Calinski-Harabasz Index': [ch_score],\n",
        "        'Global Score': [global_score]\n",
        "    })\n",
        "    \n",
        "    tabla_results_NS_df = pd.concat([tabla_results_NS_df, new_row], ignore_index=True)\n",
        "    \n",
        "    return tabla_results_NS_df\n",
        "\n",
        "def calcular_puntuacion_global_NS(silhouette_avg, db_score, ch_score):\n",
        "    # Normalizando los valores para que estén en el rango de 0 a 100\n",
        "    normalized_silhouette = (silhouette_avg + 1) * 50  # Ajustando el rango del Silhouette Score de -1 a 1 a 0 a 100\n",
        "    normalized_davies_bouldin = (1 - db_score) * 50  # Ajustando el rango del Davies-Bouldin Index de 0 a 1 a 0 a 100\n",
        "\n",
        "    # Calculando el puntaje global promediando los puntajes normalizados\n",
        "    global_score = (normalized_silhouette + normalized_davies_bouldin + ch_score) / 3\n",
        "\n",
        "    return global_score\n",
        "\n",
        "def show_save_results_no_supervised(X, labels, set_name, model_name):\n",
        "    global tabla_results_NS_df\n",
        "    silhouette_avg = silhouette_score(X, labels)\n",
        "    db_score = davies_bouldin_score(X, labels)\n",
        "    ch_score = calinski_harabasz_score(X, labels)\n",
        "    global_score = calcular_global_score(silhouette_avg, db_score, ch_score)\n",
        "\n",
        "    # Imprimir todas las métricas\n",
        "    print(f\"Metrics for {set_name} set ({model_name}):\")\n",
        "    print(f\" - Silhouette Score: {silhouette_avg:.4f}\")\n",
        "    print(f\" - Davies-Bouldin Index: {db_score:.4f}\")\n",
        "    print(f\" - Calinski-Harabasz Index: {ch_score:.4f}\")\n",
        "    print(f\" - Global Score: {global_score:.4f}\")\n",
        "    print(f\"\")\n",
        "\n",
        "    new_row = pd.DataFrame({\n",
        "            'Model': [model_name],\n",
        "            'Set': [set_name],\n",
        "            'Silhouette Score': [silhouette_avg],\n",
        "            'Davies-Bouldin Index': [db_score],\n",
        "            'Calinski-Harabasz Index': [ch_score],\n",
        "            'Global Score': [global_score]\n",
        "        })\n",
        "        \n",
        "    tabla_results_NS_df = pd.concat([tabla_results_NS_df, new_row], ignore_index=True)\n",
        "        \n",
        "    return tabla_results_NS_df\n",
        "\n",
        "def calcular_global_score(silhouette_score, davies_bouldin_index, calinski_harabasz_index):\n",
        "    # Normalizar los valores para que estén en el rango de 0 a 100\n",
        "    normalized_silhouette = (silhouette_score + 1) * 50  # Ajustando el rango del Silhouette Score de -1 a 1 a 0 a 100\n",
        "    normalized_davies_bouldin = (1 - davies_bouldin_index) * 100  # Ajustando el rango del Davies-Bouldin Index de 0 a 1 a 0 a 100\n",
        "\n",
        "    # Calculando el puntaje global promediando los puntajes normalizados\n",
        "    global_score = (normalized_silhouette + normalized_davies_bouldin + calinski_harabasz_index) / 3\n",
        "\n",
        "    return global_score\n",
        "\n",
        "def encontrar_numero_optimo_clusters(X_train, X_val, model, max_clusters=10, plot_grafica = 'NO'):\n",
        "    wcss = []\n",
        "    silhouette_scores = []\n",
        "    davies_bouldin_scores = []\n",
        "    calinski_harabasz_scores = []\n",
        "    global_scores = []\n",
        "\n",
        "    for i in range(2, max_clusters+1):\n",
        "        model.n_clusters = i\n",
        "        model.fit(X_train)\n",
        "\n",
        "        # Predecir las etiquetas para los datos de validación\n",
        "        labels = model.predict(X_val)\n",
        "\n",
        "        # Calcular las métricas\n",
        "        silhouette = silhouette_score(X_val, labels)\n",
        "        davies_bouldin = davies_bouldin_score(X_val, labels)\n",
        "        calinski_harabasz = calinski_harabasz_score(X_val, labels)\n",
        "        \n",
        "        global_score = calcular_global_score(silhouette, davies_bouldin, calinski_harabasz)\n",
        "        \n",
        "        wcss.append(model.inertia_)\n",
        "        silhouette_scores.append(silhouette)\n",
        "        davies_bouldin_scores.append(davies_bouldin)\n",
        "        calinski_harabasz_scores.append(calinski_harabasz)\n",
        "        global_scores.append(global_score)\n",
        "\n",
        "    if plot_grafica == 'SI':\n",
        "        # Plot para el método del codo\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(range(2, max_clusters+1), wcss, marker='o')\n",
        "        plt.title('Método del Codo')\n",
        "        plt.xlabel('Número de Clústeres')\n",
        "        plt.ylabel('WCSS')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot para las métricas de validación\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(range(2, max_clusters+1), silhouette_scores, marker='o', label='Silhouette Score')\n",
        "        plt.plot(range(2, max_clusters+1), davies_bouldin_scores, marker='o', label='Davies-Bouldin Index')\n",
        "        plt.plot(range(2, max_clusters+1), calinski_harabasz_scores, marker='o', label='Calinski-Harabasz Index')\n",
        "        plt.plot(range(2, max_clusters+1), global_scores, marker='o', label='Global Score')\n",
        "        plt.title('Métricas de Validación para Diferentes Números de Clústeres')\n",
        "        plt.xlabel('Número de Clústeres')\n",
        "        plt.ylabel('Valor de la Métrica')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    # Encontrar el número óptimo de clústeres basado en el puntaje global\n",
        "    numero_optimo_clusters = range(2, max_clusters+1)[np.argmax(global_scores)]\n",
        "    return numero_optimo_clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Procesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga de la URL de donde se encuentran los datos \n",
        "# (I) Introducir valor de nombreArchivo y variar la ruta en local donde se guardan los datos\n",
        "nombreArchivo = 'Tables_S1_to_S11' #nombre del archivo del dataset\n",
        "url_datos = f'D:/MÁSTER DATA SCIENCE/KSCHOOL/1.TFM/{nombreArchivo}.xlsx'\n",
        "\n",
        "# Se procede a hacer una carga de los datos. \n",
        "df_original= pd.read_excel(url_datos, sheet_name='Table S6')\n",
        "\n",
        "# Crear una copia del DataFrame original para realizar los cambios\n",
        "df6 = df_original.copy()\n",
        "\n",
        "# Recorrer las columnas del DataFrame para eliminar las cadenas de texto : ['*', '**']\n",
        "for columna in df6.columns:\n",
        "    if df6[columna].dtype == 'object':\n",
        "        # Aplicar la sustitución para cada secuencia en secuencias_a_buscar\n",
        "        for secuencia in ['*', '**']:\n",
        "            df6[columna] = df6[columna].apply(lambda x: x.replace(secuencia, '') if isinstance(x, str) and secuencia in x else x)\n",
        "\n",
        "df_prep = df6.apply(convert_to_numeric)\n",
        "\n",
        "# Relleno de nulos de la variable \"AJCC Stage\"\n",
        "df_prep[\"AJCC Stage\"] = df_prep[\"AJCC Stage\"].fillna(\"0\")\n",
        "\n",
        "# Calcular la media solo para las columnas numéricas\n",
        "numeric_columns = df_prep.select_dtypes(include=['number'])\n",
        "mean_values = numeric_columns.mean()\n",
        "\n",
        "# Rellenar los valores nulos con la media correspondiente\n",
        "df = df_prep.copy()  # Copiar el DataFrame preprocesado para evitar modificarlo\n",
        "for col in mean_values.index:\n",
        "    df[col].fillna(mean_values[col], inplace=True)\n",
        "\n",
        "\n",
        "# Binarización \"Tumor Type\" 0 -> NO CANCER; 1 -> SI CANCER + 'CancerSEEK Test Result'\n",
        "df['Tumor type'] = df['Tumor type'].apply(lambda x: 0 if str(x).strip().lower() == \"normal\" else 1).astype(int)\n",
        "#df['CancerSEEK Test Result'] = df['CancerSEEK Test Result'].apply(lambda x: 0 if str(x).strip().lower() == \"negative\" else 1).astype(int)\n",
        "\n",
        "# Conservar solo las columnas 'CA19-9 (U/ml)', 'CA-125 (U/ml)','HGF (pg/ml)','OPN (pg/ml)', 'Omega score', 'Prolactin (pg/ml)', 'CEA (pg/ml)', 'Myeloperoxidase (ng/ml)', 'TIMP-1 (pg/ml)'\n",
        "columnas_a_conservar = ['Tumor type','CA19-9 (U/ml)', 'CA-125 (U/ml)','HGF (pg/ml)','OPN (pg/ml)', 'Omega score', 'Prolactin (pg/ml)', 'CEA (pg/ml)', 'Myeloperoxidase (ng/ml)', 'TIMP-1 (pg/ml)']\n",
        "\n",
        "## --- CHECKPOINT ----- Datos limpios\n",
        "\n",
        "# Columna objetivo\n",
        "Y_column = df['Tumor type'].copy()\n",
        "# Reducción del dataFrame\n",
        "df_reduced = df[columnas_a_conservar].copy()\n",
        "#Copia del dataFrame entero\n",
        "df_full = df.copy()\n",
        "\n",
        "''' INICIO - Verificacion del information gain'''\n",
        "# Information Gain inicial\n",
        "df_discretizado = discretizar_df_arboles_1(df_reduced.drop(columns=['Tumor type']))\n",
        "df_reduced_discretizado_escalated = escalado_dataFrame(df_discretizado)\n",
        "calcular_ganancia_informacion(df_reduced_discretizado_escalated, Y_column, imprimir = \"NO\")\n",
        "\n",
        "# Information Gain usando arboles de decision (acorde a : \"..the cancer antigen markers are no longer the top predictive features. Instead, we observe the opposite trend for the purity and accuracy measurements..\")\n",
        "df_discretizado_full = discretizar_df_arboles(df_full, imprimir =\"NO\") # Columnas de este segundo enfoque guardadas en columnas_segundo_enfoque\n",
        "df_discretizado_reduced = discretizar_df_arboles(df_reduced,imprimir =\"NO\")\n",
        "\n",
        "''' FIN - Verificacion del information gain'''\n",
        "\n",
        "# Crear un DataFrame para almacenar los resultados en una tabla para luego poder compararlos - APRENDIZAJE SUPERVISADO\n",
        "tabla_results_df = pd.DataFrame(columns=['Model', 'Set', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Adjusted Rand Index', 'Mean Squared Error', 'R-squared','AUC-ROC', 'TN', 'FP', 'FN', 'TP', 'Global Score'])\n",
        "\n",
        "# Crear un DataFrame para almacenar los resultados en una tabla para luego poder compararlos - APRENDIZAJE NO SUPERVISADO\n",
        "tabla_results_NS_df = pd.DataFrame(columns=['Model', 'Set', 'Silhouette Score', 'Davies-Bouldin Index', 'Calinski-Harabasz Index', 'Global Score'])\n",
        "\n",
        "## --- CHECKPOINT ----- Datos discretizados + Information gain\n",
        "# TODO Una vez hechos los modelos, habrá que estudiar cómo influye el usar esta serie de variables en la predicción\n",
        "columnas_segundo_enfoque = ['Tumor type','OPN (pg/ml)','IL-6 (pg/ml)','IL-8 (pg/ml)','HGF (pg/ml)','Prolactin (pg/ml)','Omega score','GDF15 (ng/ml)','CYFRA 21-1 (pg/ml)','Myeloperoxidase (ng/ml)','sEGFR (pg/ml)']\n",
        "df_reduced_segundo_enfoque = df[columnas_segundo_enfoque].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "''' Valores para X\n",
        "1. df : entero, limpio, sin normalizar ni discretizar\n",
        "2. df_reduced : reducido, limpio, sin normalizar ni discretizar\n",
        "3. df_reduced_discretizado_escalated --> Acorde a la Tabla 1\n",
        "4. df_discretizado_full.drop(columns=['Tumor type']) --> Acorde a la Figura S3 (usando todas las variables) discretizado con arbol de decisión\n",
        "5. df_discretizado_reduced.drop(columns=['Tumor type']) --> Acorde a la Figura S3; discretizado con arbol de decisión\n",
        "'''\n",
        "\n",
        "y = df_reduced['Tumor type']\n",
        "X = df_reduced.drop(columns='Tumor type')\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, train_size=0.6, val_size=0.2, test_size=0.2, random_state=42)\n",
        "\n",
        "numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list()\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "                   [('scale', StandardScaler(), numeric_cols),\n",
        "                    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)],\n",
        "                remainder = 'passthrough',\n",
        "                verbose_feature_names_out = False\n",
        "               ).set_output(transform=\"pandas\")\n",
        "\n",
        "# TODO DUDA - Añadir un proceso de discretización? Puede mejorar el resultado. Ver cómo influye usar la función 'discretizar_df_arboles' ó la función sklearn.preprocessing.KBinsDiscretizer\n",
        "X_train_prep = preprocessor.fit_transform(X_train)\n",
        "X_val_prep = preprocessor.transform(X_val)\n",
        "X_test_prep  = preprocessor.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aprendizaje supervisado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión lineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.43555844 -0.40375505 -0.41374517 -0.41624435 -0.41977734]\n",
            "Média métricas de validación cruzada: -0.41781607165280094\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.7952\n",
            " - Precision: 0.8251\n",
            " - Recall: 0.7952\n",
            " - F1-Score: 0.7950\n",
            " - Adjusted Rand Index: 0.3478\n",
            " - Mean Squared Error: 0.2048\n",
            " - R-squared: 0.1673\n",
            " - Área bajo la curva : 0.810\n",
            " - Confusion Matrix: \n",
            "[[438  37]\n",
            " [186 428]]\n",
            " - Global Score : 74.81\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8159\n",
            " - Precision: 0.8271\n",
            " - Recall: 0.8159\n",
            " - F1-Score: 0.8155\n",
            " - Adjusted Rand Index: 0.3976\n",
            " - Mean Squared Error: 0.1841\n",
            " - R-squared: 0.2610\n",
            " - Área bajo la curva : 0.820\n",
            " - Confusion Matrix: \n",
            "[[153  18]\n",
            " [ 49 144]]\n",
            " - Global Score : 76.76\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7940\n",
            " - Precision: 0.8303\n",
            " - Recall: 0.7940\n",
            " - F1-Score: 0.7917\n",
            " - Adjusted Rand Index: 0.3437\n",
            " - Mean Squared Error: 0.2060\n",
            " - R-squared: 0.1694\n",
            " - Área bajo la curva : 0.806\n",
            " - Confusion Matrix: \n",
            "[[157   9]\n",
            " [ 66 132]]\n",
            " - Global Score : 74.65\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\FloCr\\AppData\\Local\\Temp\\ipykernel_30552\\3849515159.py:270: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  tabla_results_df = pd.concat([tabla_results_df, new_row], ignore_index=True)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model         Set  Accuracy  Precision    Recall  F1-Score  \\\n",
              "0  Regresion Lineal    Training  0.795225   0.825122  0.795225  0.794968   \n",
              "1  Regresion Lineal  Validation  0.815934   0.827130  0.815934  0.815546   \n",
              "2  Regresion Lineal        Test  0.793956   0.830307  0.793956  0.791730   \n",
              "\n",
              "   Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC   TN  FP   FN  \\\n",
              "0             0.347828            0.204775   0.167334  0.809587  438  37  186   \n",
              "1             0.397597            0.184066   0.261037  0.820425  153  18   49   \n",
              "2             0.343680            0.206044   0.169405  0.806225  157   9   66   \n",
              "\n",
              "    TP  Global Score  \n",
              "0  428         74.81  \n",
              "1  144         76.76  \n",
              "2  132         74.65  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Crear una instancia del modelo de regresión lineal\n",
        "model_LR = LinearRegression()\n",
        "model_name = \"Regresion Lineal\"\n",
        "\n",
        "# Definir un umbral\n",
        "umbral = 0.5\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_LR, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_LR.fit(X_train_prep, y_train)\n",
        "\n",
        "#print(\"Fase de ENTRENAMIENTO estadísticas :\")\n",
        "#mostrar_score(X_train_prep, y_train, model_LR)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, np.where(model_LR.predict(X_train_prep) > umbral, 1, 0), \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "# Predecir en el conjunto de validación\n",
        "y_pred = model_LR.predict(X_val_prep)\n",
        "\n",
        "# Binarizar las predicciones\n",
        "y_pred_bin = np.where(y_pred > umbral, 1, 0)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_pred_bin, \"Validation\", model_name, print_roc = \"NO\")\n",
        "\n",
        "# Predicciones\n",
        "y_test_pred = model_LR.predict(X_test_prep)\n",
        "\n",
        "y_test_pred_bin = np.where(y_test_pred > umbral, 1, 0)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred_bin, \"Test\", model_name, print_roc = \"NO\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regresión Logística\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.40068748 -0.38313051 -0.35838578 -0.47409982 -0.42933879]\n",
            "Média métricas de validación cruzada: -0.4091284780006096\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8356\n",
            " - Precision: 0.8437\n",
            " - Recall: 0.8356\n",
            " - F1-Score: 0.8363\n",
            " - Adjusted Rand Index: 0.4501\n",
            " - Mean Squared Error: 0.1644\n",
            " - R-squared: 0.3316\n",
            " - Área bajo la curva : 0.841\n",
            " - Confusion Matrix: \n",
            "[[420  55]\n",
            " [124 490]]\n",
            " - Global Score : 79.14\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8269\n",
            " - Precision: 0.8287\n",
            " - Recall: 0.8269\n",
            " - F1-Score: 0.8271\n",
            " - Adjusted Rand Index: 0.4259\n",
            " - Mean Squared Error: 0.1731\n",
            " - R-squared: 0.3052\n",
            " - Área bajo la curva : 0.828\n",
            " - Confusion Matrix: \n",
            "[[145  26]\n",
            " [ 37 156]]\n",
            " - Global Score : 77.87\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8269\n",
            " - Precision: 0.8391\n",
            " - Recall: 0.8269\n",
            " - F1-Score: 0.8270\n",
            " - Adjusted Rand Index: 0.4259\n",
            " - Mean Squared Error: 0.1731\n",
            " - R-squared: 0.3023\n",
            " - Área bajo la curva : 0.833\n",
            " - Confusion Matrix: \n",
            "[[150  16]\n",
            " [ 47 151]]\n",
            " - Global Score : 78.15\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model         Set  Accuracy  Precision    Recall  F1-Score  \\\n",
              "0     Regresion Lineal    Training  0.795225   0.825122  0.795225  0.794968   \n",
              "1     Regresion Lineal  Validation  0.815934   0.827130  0.815934  0.815546   \n",
              "2     Regresion Lineal        Test  0.793956   0.830307  0.793956  0.791730   \n",
              "3  Regresión Logística    Training  0.835629   0.843677  0.835629  0.836301   \n",
              "4  Regresión Logística  Validation  0.826923   0.828750  0.826923  0.827081   \n",
              "5  Regresión Logística        Test  0.826923   0.839082  0.826923  0.826964   \n",
              "\n",
              "   Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC   TN  FP   FN  \\\n",
              "0             0.347828            0.204775   0.167334  0.809587  438  37  186   \n",
              "1             0.397597            0.184066   0.261037  0.820425  153  18   49   \n",
              "2             0.343680            0.206044   0.169405  0.806225  157   9   66   \n",
              "3             0.450090            0.164371   0.331627  0.841128  420  55  124   \n",
              "4             0.425939            0.173077   0.305154  0.828122  145  26   37   \n",
              "5             0.425926            0.173077   0.302300  0.833120  150  16   47   \n",
              "\n",
              "    TP  Global Score  \n",
              "0  428         74.81  \n",
              "1  144         76.76  \n",
              "2  132         74.65  \n",
              "3  490         79.14  \n",
              "4  156         77.87  \n",
              "5  151         78.15  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model_name = \"Regresión Logística\"\n",
        "print(model_name)\n",
        "# Crear una instancia del modelo de regresión logística\n",
        "model_LogR = LogisticRegression()\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_LogR, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_LogR.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model_LogR.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "y_val_pred = model_LogR.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "\n",
        "y_test_pred = model_LogR.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arbol de decisión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Árbol de Decisión\n",
            "Mejores hiperparámetros: {'criterion': 'entropy', 'max_depth': None, 'max_leaf_nodes': 32, 'min_samples_leaf': 2, 'min_samples_split': 10, 'splitter': 'best'}\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.38907088 -0.39492189 -0.34534918 -0.40068748 -0.33942212]\n",
            "Média métricas de validación cruzada: -0.3738903096983308\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.9431\n",
            " - Precision: 0.9434\n",
            " - Recall: 0.9431\n",
            " - F1-Score: 0.9431\n",
            " - Adjusted Rand Index: 0.7850\n",
            " - Mean Squared Error: 0.0569\n",
            " - R-squared: 0.7685\n",
            " - Área bajo la curva : 0.944\n",
            " - Confusion Matrix: \n",
            "[[450  25]\n",
            " [ 37 577]]\n",
            " - Global Score : 92.45\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8544\n",
            " - Precision: 0.8561\n",
            " - Recall: 0.8544\n",
            " - F1-Score: 0.8538\n",
            " - Adjusted Rand Index: 0.5010\n",
            " - Mean Squared Error: 0.1456\n",
            " - R-squared: 0.4154\n",
            " - Área bajo la curva : 0.851\n",
            " - Confusion Matrix: \n",
            "[[137  34]\n",
            " [ 19 174]]\n",
            " - Global Score : 81.09\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8544\n",
            " - Precision: 0.8547\n",
            " - Recall: 0.8544\n",
            " - F1-Score: 0.8545\n",
            " - Adjusted Rand Index: 0.5010\n",
            " - Mean Squared Error: 0.1456\n",
            " - R-squared: 0.4130\n",
            " - Área bajo la curva : 0.854\n",
            " - Confusion Matrix: \n",
            "[[141  25]\n",
            " [ 28 170]]\n",
            " - Global Score : 81.16\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model         Set  Accuracy  Precision    Recall  F1-Score  \\\n",
              "0     Regresion Lineal    Training  0.795225   0.825122  0.795225  0.794968   \n",
              "1     Regresion Lineal  Validation  0.815934   0.827130  0.815934  0.815546   \n",
              "2     Regresion Lineal        Test  0.793956   0.830307  0.793956  0.791730   \n",
              "3  Regresión Logística    Training  0.835629   0.843677  0.835629  0.836301   \n",
              "4  Regresión Logística  Validation  0.826923   0.828750  0.826923  0.827081   \n",
              "5  Regresión Logística        Test  0.826923   0.839082  0.826923  0.826964   \n",
              "6    Árbol de Decisión    Training  0.943067   0.943447  0.943067  0.943141   \n",
              "7    Árbol de Decisión  Validation  0.854396   0.856113  0.854396  0.853779   \n",
              "8    Árbol de Decisión        Test  0.854396   0.854704  0.854396  0.854492   \n",
              "\n",
              "   Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC   TN  FP   FN  \\\n",
              "0             0.347828            0.204775   0.167334  0.809587  438  37  186   \n",
              "1             0.397597            0.184066   0.261037  0.820425  153  18   49   \n",
              "2             0.343680            0.206044   0.169405  0.806225  157   9   66   \n",
              "3             0.450090            0.164371   0.331627  0.841128  420  55  124   \n",
              "4             0.425939            0.173077   0.305154  0.828122  145  26   37   \n",
              "5             0.425926            0.173077   0.302300  0.833120  150  16   47   \n",
              "6             0.785002            0.056933   0.768496  0.943554  450  25   37   \n",
              "7             0.501006            0.145604   0.415447  0.851362  137  34   19   \n",
              "8             0.501008            0.145604   0.413046  0.853992  141  25   28   \n",
              "\n",
              "    TP  Global Score  \n",
              "0  428         74.81  \n",
              "1  144         76.76  \n",
              "2  132         74.65  \n",
              "3  490         79.14  \n",
              "4  156         77.87  \n",
              "5  151         78.15  \n",
              "6  577         92.45  \n",
              "7  174         81.09  \n",
              "8  170         81.16  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "model_name = \"Árbol de Decisión\"\n",
        "print(model_name)\n",
        "model_DT_leaf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Definir los parámetros a ajustar\n",
        "params = {'max_leaf_nodes': range(2, 50)}  # Probando diferentes valores para max_leaf_nodes\n",
        "\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'max_depth': [None,5, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_leaf_nodes': range(2, 50)\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en rejilla con validación cruzada\n",
        "grid_search = GridSearchCV(model_DT_leaf, param_grid, n_jobs=-1, cv=5)\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
        "\n",
        "model_DT = grid_search.best_estimator_\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_DT.fit(X_train_prep, y_train)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_DT, X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model_DT.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "y_val_pred = model_DT.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "\n",
        "y_test_pred = model_DT.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest\n",
            "Mejores parámetros encontrados: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 50, 'bootstrap': False}\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.35192785 -0.27091418 -0.29522189 -0.31037119 -0.30358837]\n",
            "Média métricas de validación cruzada: -0.30640469635736467\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[475   0]\n",
            " [  0 614]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8874\n",
            " - Precision: 0.8895\n",
            " - Recall: 0.8874\n",
            " - F1-Score: 0.8869\n",
            " - Adjusted Rand Index: 0.5991\n",
            " - Mean Squared Error: 0.1126\n",
            " - R-squared: 0.5478\n",
            " - Área bajo la curva : 0.884\n",
            " - Confusion Matrix: \n",
            "[[143  28]\n",
            " [ 13 180]]\n",
            " - Global Score : 85.21\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9011\n",
            " - Precision: 0.9011\n",
            " - Recall: 0.9011\n",
            " - F1-Score: 0.9010\n",
            " - Adjusted Rand Index: 0.6425\n",
            " - Mean Squared Error: 0.0989\n",
            " - R-squared: 0.6013\n",
            " - Área bajo la curva : 0.899\n",
            " - Confusion Matrix: \n",
            "[[146  20]\n",
            " [ 16 182]]\n",
            " - Global Score : 86.96\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Model         Set  Accuracy  Precision    Recall  F1-Score  \\\n",
              "0      Regresion Lineal    Training  0.795225   0.825122  0.795225  0.794968   \n",
              "1      Regresion Lineal  Validation  0.815934   0.827130  0.815934  0.815546   \n",
              "2      Regresion Lineal        Test  0.793956   0.830307  0.793956  0.791730   \n",
              "3   Regresión Logística    Training  0.835629   0.843677  0.835629  0.836301   \n",
              "4   Regresión Logística  Validation  0.826923   0.828750  0.826923  0.827081   \n",
              "5   Regresión Logística        Test  0.826923   0.839082  0.826923  0.826964   \n",
              "6     Árbol de Decisión    Training  0.943067   0.943447  0.943067  0.943141   \n",
              "7     Árbol de Decisión  Validation  0.854396   0.856113  0.854396  0.853779   \n",
              "8     Árbol de Decisión        Test  0.854396   0.854704  0.854396  0.854492   \n",
              "9         Random Forest    Training  1.000000   1.000000  1.000000  1.000000   \n",
              "10        Random Forest  Validation  0.887363   0.889476  0.887363  0.886886   \n",
              "11        Random Forest        Test  0.901099   0.901102  0.901099  0.900990   \n",
              "\n",
              "    Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC   TN  FP  \\\n",
              "0              0.347828            0.204775   0.167334  0.809587  438  37   \n",
              "1              0.397597            0.184066   0.261037  0.820425  153  18   \n",
              "2              0.343680            0.206044   0.169405  0.806225  157   9   \n",
              "3              0.450090            0.164371   0.331627  0.841128  420  55   \n",
              "4              0.425939            0.173077   0.305154  0.828122  145  26   \n",
              "5              0.425926            0.173077   0.302300  0.833120  150  16   \n",
              "6              0.785002            0.056933   0.768496  0.943554  450  25   \n",
              "7              0.501006            0.145604   0.415447  0.851362  137  34   \n",
              "8              0.501008            0.145604   0.413046  0.853992  141  25   \n",
              "9              1.000000            0.000000   1.000000  1.000000  475   0   \n",
              "10             0.599091            0.112637   0.547799  0.884450  143  28   \n",
              "11             0.642523            0.098901   0.601314  0.899355  146  20   \n",
              "\n",
              "     FN   TP  Global Score  \n",
              "0   186  428         74.81  \n",
              "1    49  144         76.76  \n",
              "2    66  132         74.65  \n",
              "3   124  490         79.14  \n",
              "4    37  156         77.87  \n",
              "5    47  151         78.15  \n",
              "6    37  577         92.45  \n",
              "7    19  174         81.09  \n",
              "8    28  170         81.16  \n",
              "9     0  614        100.00  \n",
              "10   13  180         85.21  \n",
              "11   16  182         86.96  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_name = \"Random Forest\"\n",
        "print(model_name)\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "rf_params = {\n",
        "    \"n_estimators\": [100, 200, 300, 400, 500],  \n",
        "    \"max_depth\": [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],  \n",
        "    \"max_features\": [None, \"sqrt\", \"log2\"], \n",
        "    \"min_samples_split\": [2, 5, 10], \n",
        "    \"min_samples_leaf\": [1, 2, 4], \n",
        "    \"bootstrap\": [True, False]  \n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=RF,\n",
        "                                   param_distributions=rf_params,\n",
        "                                   n_iter=100,  \n",
        "                                   cv=5,  \n",
        "                                   verbose=0, \n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1) \n",
        "\n",
        "random_search.fit(X_train_prep, y_train)\n",
        "\n",
        "print(\"Mejores parámetros encontrados:\", random_search.best_params_)\n",
        "\n",
        "model_RF = random_search.best_estimator_\n",
        "\n",
        "model_RF.fit(X_train_prep, y_train)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_RF, X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model_RF.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "y_val_pred = model_RF.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "\n",
        "y_test_pred = model_RF.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN\n",
            "Best parameters for KNN:  {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'uniform'}\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8522\n",
            " - Precision: 0.8564\n",
            " - Recall: 0.8522\n",
            " - F1-Score: 0.8527\n",
            " - Adjusted Rand Index: 0.4956\n",
            " - Mean Squared Error: 0.1478\n",
            " - R-squared: 0.3988\n",
            " - Área bajo la curva : 0.855\n",
            " - Confusion Matrix: \n",
            "[[418  57]\n",
            " [104 510]]\n",
            " - Global Score : 81.04\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.7857\n",
            " - Precision: 0.7873\n",
            " - Recall: 0.7857\n",
            " - F1-Score: 0.7859\n",
            " - Adjusted Rand Index: 0.3247\n",
            " - Mean Squared Error: 0.2143\n",
            " - R-squared: 0.1397\n",
            " - Área bajo la curva : 0.787\n",
            " - Confusion Matrix: \n",
            "[[137  34]\n",
            " [ 44 149]]\n",
            " - Global Score : 72.93\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7940\n",
            " - Precision: 0.8000\n",
            " - Recall: 0.7940\n",
            " - F1-Score: 0.7943\n",
            " - Adjusted Rand Index: 0.3438\n",
            " - Mean Squared Error: 0.2060\n",
            " - R-squared: 0.1694\n",
            " - Área bajo la curva : 0.797\n",
            " - Confusion Matrix: \n",
            "[[139  27]\n",
            " [ 48 150]]\n",
            " - Global Score : 74.04\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Model         Set  Accuracy  Precision    Recall  F1-Score  \\\n",
              "0      Regresion Lineal    Training  0.795225   0.825122  0.795225  0.794968   \n",
              "1      Regresion Lineal  Validation  0.815934   0.827130  0.815934  0.815546   \n",
              "2      Regresion Lineal        Test  0.793956   0.830307  0.793956  0.791730   \n",
              "3   Regresión Logística    Training  0.835629   0.843677  0.835629  0.836301   \n",
              "4   Regresión Logística  Validation  0.826923   0.828750  0.826923  0.827081   \n",
              "5   Regresión Logística        Test  0.826923   0.839082  0.826923  0.826964   \n",
              "6     Árbol de Decisión    Training  0.943067   0.943447  0.943067  0.943141   \n",
              "7     Árbol de Decisión  Validation  0.854396   0.856113  0.854396  0.853779   \n",
              "8     Árbol de Decisión        Test  0.854396   0.854704  0.854396  0.854492   \n",
              "9         Random Forest    Training  1.000000   1.000000  1.000000  1.000000   \n",
              "10        Random Forest  Validation  0.887363   0.889476  0.887363  0.886886   \n",
              "11        Random Forest        Test  0.901099   0.901102  0.901099  0.900990   \n",
              "12                  KNN    Training  0.852158   0.856418  0.852158  0.852701   \n",
              "13                  KNN  Validation  0.785714   0.787289  0.785714  0.785909   \n",
              "14                  KNN        Test  0.793956   0.799964  0.793956  0.794316   \n",
              "\n",
              "    Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC   TN  FP  \\\n",
              "0              0.347828            0.204775   0.167334  0.809587  438  37   \n",
              "1              0.397597            0.184066   0.261037  0.820425  153  18   \n",
              "2              0.343680            0.206044   0.169405  0.806225  157   9   \n",
              "3              0.450090            0.164371   0.331627  0.841128  420  55   \n",
              "4              0.425939            0.173077   0.305154  0.828122  145  26   \n",
              "5              0.425926            0.173077   0.302300  0.833120  150  16   \n",
              "6              0.785002            0.056933   0.768496  0.943554  450  25   \n",
              "7              0.501006            0.145604   0.415447  0.851362  137  34   \n",
              "8              0.501008            0.145604   0.413046  0.853992  141  25   \n",
              "9              1.000000            0.000000   1.000000  1.000000  475   0   \n",
              "10             0.599091            0.112637   0.547799  0.884450  143  28   \n",
              "11             0.642523            0.098901   0.601314  0.899355  146  20   \n",
              "12             0.495592            0.147842   0.398838  0.855309  418  57   \n",
              "13             0.324677            0.214286   0.139715  0.786595  137  34   \n",
              "14             0.343845            0.206044   0.169405  0.797463  139  27   \n",
              "\n",
              "     FN   TP  Global Score  \n",
              "0   186  428         74.81  \n",
              "1    49  144         76.76  \n",
              "2    66  132         74.65  \n",
              "3   124  490         79.14  \n",
              "4    37  156         77.87  \n",
              "5    47  151         78.15  \n",
              "6    37  577         92.45  \n",
              "7    19  174         81.09  \n",
              "8    28  170         81.16  \n",
              "9     0  614        100.00  \n",
              "10   13  180         85.21  \n",
              "11   16  182         86.96  \n",
              "12  104  510         81.04  \n",
              "13   44  149         72.93  \n",
              "14   48  150         74.04  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "model_name = \"KNN\"\n",
        "print(model_name)\n",
        "# Parámetros de búsqueda para KNN\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring='accuracy')\n",
        "grid_search_knn.fit(X_train_prep, y_train)\n",
        "\n",
        "print(\"Best parameters for KNN: \", grid_search_knn.best_params_)\n",
        "\n",
        "y_train_pred = grid_search_knn.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "y_val_pred = grid_search_knn.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "\n",
        "y_test_pred = grid_search_knn.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVN\n",
            "      CA19-9 (U/ml)  CA-125 (U/ml)  HGF (pg/ml)  OPN (pg/ml)  Omega score  \\\n",
            "1333      -0.092602      -0.107994    -0.309247     0.050290    -0.143176   \n",
            "1784      -0.092321      16.603266     0.504689     3.463351     0.336480   \n",
            "1465      -0.092602      -0.106787    -0.057182    -0.739401    -0.163334   \n",
            "1252      -0.092602      -0.107994    -0.309247    -0.413678    -0.177269   \n",
            "53        -0.092251      -0.107994    -0.312427    -0.139405    -0.158614   \n",
            "...             ...            ...          ...          ...          ...   \n",
            "797       -0.091747      -0.108175    -0.287776     0.012175    -0.155475   \n",
            "1669      -0.093555      -0.109863    -0.319221    -0.389669    -0.167576   \n",
            "766       -0.086317      -0.074770     0.218594     1.107403    -0.122682   \n",
            "75        -0.092251      -0.107994    -0.312427    -0.024325    -0.058391   \n",
            "1541      -0.053513      -0.096282    -0.280971    -0.422183    -0.132929   \n",
            "\n",
            "      Prolactin (pg/ml)  CEA (pg/ml)  Myeloperoxidase (ng/ml)  TIMP-1 (pg/ml)  \n",
            "1333          -0.362952    -0.130982                -0.250333       -1.017092  \n",
            "1784           0.506493    -0.116108                 0.069232        1.860676  \n",
            "1465          -0.385378    -0.129505                -0.321950       -0.224166  \n",
            "1252          -0.325200    -0.110392                -0.361861       -0.472180  \n",
            "53            -0.333856    -0.156455                -0.283847       -0.709295  \n",
            "...                 ...          ...                      ...             ...  \n",
            "797           -0.168136    -0.104257                -0.190397       -0.569358  \n",
            "1669          -0.438204    -0.111057                -0.371178       -0.205878  \n",
            "766           -0.286075    -0.142170                -0.170511        0.129632  \n",
            "75            -0.440049    -0.082846                -0.342670       -0.468926  \n",
            "1541          -0.448495    -0.139981                -0.341140        0.000447  \n",
            "\n",
            "[1089 rows x 9 columns]\n",
            "Mejores parámetros:  {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8466\n",
            " - Precision: 0.8501\n",
            " - Recall: 0.8466\n",
            " - F1-Score: 0.8472\n",
            " - Adjusted Rand Index: 0.4802\n",
            " - Mean Squared Error: 0.1534\n",
            " - R-squared: 0.3764\n",
            " - Área bajo la curva : 0.849\n",
            " - Confusion Matrix: \n",
            "[[412  63]\n",
            " [104 510]]\n",
            " - Global Score : 80.33\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8214\n",
            " - Precision: 0.8213\n",
            " - Recall: 0.8214\n",
            " - F1-Score: 0.8213\n",
            " - Adjusted Rand Index: 0.4116\n",
            " - Mean Squared Error: 0.1786\n",
            " - R-squared: 0.2831\n",
            " - Área bajo la curva : 0.820\n",
            " - Confusion Matrix: \n",
            "[[137  34]\n",
            " [ 31 162]]\n",
            " - Global Score : 77.11\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8214\n",
            " - Precision: 0.8266\n",
            " - Recall: 0.8214\n",
            " - F1-Score: 0.8218\n",
            " - Adjusted Rand Index: 0.4117\n",
            " - Mean Squared Error: 0.1786\n",
            " - R-squared: 0.2802\n",
            " - Área bajo la curva : 0.825\n",
            " - Confusion Matrix: \n",
            "[[143  23]\n",
            " [ 42 156]]\n",
            " - Global Score : 77.31\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Model         Set  Accuracy  Precision    Recall  F1-Score  \\\n",
              "0      Regresion Lineal    Training  0.795225   0.825122  0.795225  0.794968   \n",
              "1      Regresion Lineal  Validation  0.815934   0.827130  0.815934  0.815546   \n",
              "2      Regresion Lineal        Test  0.793956   0.830307  0.793956  0.791730   \n",
              "3   Regresión Logística    Training  0.835629   0.843677  0.835629  0.836301   \n",
              "4   Regresión Logística  Validation  0.826923   0.828750  0.826923  0.827081   \n",
              "5   Regresión Logística        Test  0.826923   0.839082  0.826923  0.826964   \n",
              "6     Árbol de Decisión    Training  0.943067   0.943447  0.943067  0.943141   \n",
              "7     Árbol de Decisión  Validation  0.854396   0.856113  0.854396  0.853779   \n",
              "8     Árbol de Decisión        Test  0.854396   0.854704  0.854396  0.854492   \n",
              "9         Random Forest    Training  1.000000   1.000000  1.000000  1.000000   \n",
              "10        Random Forest  Validation  0.887363   0.889476  0.887363  0.886886   \n",
              "11        Random Forest        Test  0.901099   0.901102  0.901099  0.900990   \n",
              "12                  KNN    Training  0.852158   0.856418  0.852158  0.852701   \n",
              "13                  KNN  Validation  0.785714   0.787289  0.785714  0.785909   \n",
              "14                  KNN        Test  0.793956   0.799964  0.793956  0.794316   \n",
              "15                  SVN    Training  0.846648   0.850097  0.846648  0.847172   \n",
              "16                  SVN  Validation  0.821429   0.821337  0.821429  0.821327   \n",
              "17                  SVN        Test  0.821429   0.826572  0.821429  0.821762   \n",
              "\n",
              "    Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC   TN  FP  \\\n",
              "0              0.347828            0.204775   0.167334  0.809587  438  37   \n",
              "1              0.397597            0.184066   0.261037  0.820425  153  18   \n",
              "2              0.343680            0.206044   0.169405  0.806225  157   9   \n",
              "3              0.450090            0.164371   0.331627  0.841128  420  55   \n",
              "4              0.425939            0.173077   0.305154  0.828122  145  26   \n",
              "5              0.425926            0.173077   0.302300  0.833120  150  16   \n",
              "6              0.785002            0.056933   0.768496  0.943554  450  25   \n",
              "7              0.501006            0.145604   0.415447  0.851362  137  34   \n",
              "8              0.501008            0.145604   0.413046  0.853992  141  25   \n",
              "9              1.000000            0.000000   1.000000  1.000000  475   0   \n",
              "10             0.599091            0.112637   0.547799  0.884450  143  28   \n",
              "11             0.642523            0.098901   0.601314  0.899355  146  20   \n",
              "12             0.495592            0.147842   0.398838  0.855309  418  57   \n",
              "13             0.324677            0.214286   0.139715  0.786595  137  34   \n",
              "14             0.343845            0.206044   0.169405  0.797463  139  27   \n",
              "15             0.480168            0.153352   0.376434  0.848994  412  63   \n",
              "16             0.411647            0.178571   0.283095  0.820274  137  34   \n",
              "17             0.411656            0.178571   0.280151  0.824662  143  23   \n",
              "\n",
              "     FN   TP  Global Score  \n",
              "0   186  428         74.81  \n",
              "1    49  144         76.76  \n",
              "2    66  132         74.65  \n",
              "3   124  490         79.14  \n",
              "4    37  156         77.87  \n",
              "5    47  151         78.15  \n",
              "6    37  577         92.45  \n",
              "7    19  174         81.09  \n",
              "8    28  170         81.16  \n",
              "9     0  614        100.00  \n",
              "10   13  180         85.21  \n",
              "11   16  182         86.96  \n",
              "12  104  510         81.04  \n",
              "13   44  149         72.93  \n",
              "14   48  150         74.04  \n",
              "15  104  510         80.33  \n",
              "16   31  162         77.11  \n",
              "17   42  156         77.31  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model_name = \"SVN\"\n",
        "print(model_name)\n",
        "# Define la cuadrícula de parámetros para buscar\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Crea un objeto GridSearchCV\n",
        "grid_search_svm = GridSearchCV(SVC(), param_grid, refit=True)\n",
        "print(X_train_prep)\n",
        "# Ajusta el objeto GridSearchCV a los datos\n",
        "grid_search_svm.fit(X_train_prep, y_train)\n",
        "\n",
        "# Imprime los mejores parámetros\n",
        "print(\"Mejores parámetros: \", grid_search_svm.best_params_)\n",
        "\n",
        "y_train_pred = grid_search_svm.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "y_val_pred = grid_search_svm.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "\n",
        "y_test_pred = grid_search_svm.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussiano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes - Gaussian\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.47891314 -0.48367856 -0.48367856 -0.49307126 -0.52583048]\n",
            "Média métricas de validación cruzada: -0.49303440260524434\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.7594\n",
            " - Precision: 0.8315\n",
            " - Recall: 0.7594\n",
            " - F1-Score: 0.7543\n",
            " - Adjusted Rand Index: 0.2674\n",
            " - Mean Squared Error: 0.2406\n",
            " - R-squared: 0.0217\n",
            " - Área bajo la curva : 0.784\n",
            " - Confusion Matrix: \n",
            "[[465  10]\n",
            " [252 362]]\n",
            " - Global Score : 71.38\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.7665\n",
            " - Precision: 0.8093\n",
            " - Recall: 0.7665\n",
            " - F1-Score: 0.7615\n",
            " - Adjusted Rand Index: 0.2820\n",
            " - Mean Squared Error: 0.2335\n",
            " - R-squared: 0.0625\n",
            " - Área bajo la curva : 0.776\n",
            " - Confusion Matrix: \n",
            "[[161  10]\n",
            " [ 75 118]]\n",
            " - Global Score : 71.4\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7582\n",
            " - Precision: 0.8154\n",
            " - Recall: 0.7582\n",
            " - F1-Score: 0.7525\n",
            " - Adjusted Rand Index: 0.2644\n",
            " - Mean Squared Error: 0.2418\n",
            " - R-squared: 0.0254\n",
            " - Área bajo la curva : 0.774\n",
            " - Confusion Matrix: \n",
            "[[159   7]\n",
            " [ 81 117]]\n",
            " - Global Score : 70.79\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Model         Set  Accuracy  Precision    Recall  \\\n",
              "0         Regresion Lineal    Training  0.795225   0.825122  0.795225   \n",
              "1         Regresion Lineal  Validation  0.815934   0.827130  0.815934   \n",
              "2         Regresion Lineal        Test  0.793956   0.830307  0.793956   \n",
              "3      Regresión Logística    Training  0.835629   0.843677  0.835629   \n",
              "4      Regresión Logística  Validation  0.826923   0.828750  0.826923   \n",
              "5      Regresión Logística        Test  0.826923   0.839082  0.826923   \n",
              "6        Árbol de Decisión    Training  0.943067   0.943447  0.943067   \n",
              "7        Árbol de Decisión  Validation  0.854396   0.856113  0.854396   \n",
              "8        Árbol de Decisión        Test  0.854396   0.854704  0.854396   \n",
              "9            Random Forest    Training  1.000000   1.000000  1.000000   \n",
              "10           Random Forest  Validation  0.887363   0.889476  0.887363   \n",
              "11           Random Forest        Test  0.901099   0.901102  0.901099   \n",
              "12                     KNN    Training  0.852158   0.856418  0.852158   \n",
              "13                     KNN  Validation  0.785714   0.787289  0.785714   \n",
              "14                     KNN        Test  0.793956   0.799964  0.793956   \n",
              "15                     SVN    Training  0.846648   0.850097  0.846648   \n",
              "16                     SVN  Validation  0.821429   0.821337  0.821429   \n",
              "17                     SVN        Test  0.821429   0.826572  0.821429   \n",
              "18  Naive Bayes - Gaussian    Training  0.759412   0.831542  0.759412   \n",
              "19  Naive Bayes - Gaussian  Validation  0.766484   0.809282  0.766484   \n",
              "20  Naive Bayes - Gaussian        Test  0.758242   0.815378  0.758242   \n",
              "\n",
              "    F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC  \\\n",
              "0   0.794968             0.347828            0.204775   0.167334  0.809587   \n",
              "1   0.815546             0.397597            0.184066   0.261037  0.820425   \n",
              "2   0.791730             0.343680            0.206044   0.169405  0.806225   \n",
              "3   0.836301             0.450090            0.164371   0.331627  0.841128   \n",
              "4   0.827081             0.425939            0.173077   0.305154  0.828122   \n",
              "5   0.826964             0.425926            0.173077   0.302300  0.833120   \n",
              "6   0.943141             0.785002            0.056933   0.768496  0.943554   \n",
              "7   0.853779             0.501006            0.145604   0.415447  0.851362   \n",
              "8   0.854492             0.501008            0.145604   0.413046  0.853992   \n",
              "9   1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "10  0.886886             0.599091            0.112637   0.547799  0.884450   \n",
              "11  0.900990             0.642523            0.098901   0.601314  0.899355   \n",
              "12  0.852701             0.495592            0.147842   0.398838  0.855309   \n",
              "13  0.785909             0.324677            0.214286   0.139715  0.786595   \n",
              "14  0.794316             0.343845            0.206044   0.169405  0.797463   \n",
              "15  0.847172             0.480168            0.153352   0.376434  0.848994   \n",
              "16  0.821327             0.411647            0.178571   0.283095  0.820274   \n",
              "17  0.821762             0.411656            0.178571   0.280151  0.824662   \n",
              "18  0.754310             0.267385            0.240588   0.021711  0.784262   \n",
              "19  0.761488             0.282026            0.233516   0.062509  0.776460   \n",
              "20  0.752494             0.264371            0.241758   0.025435  0.774370   \n",
              "\n",
              "     TN  FP   FN   TP  Global Score  \n",
              "0   438  37  186  428         74.81  \n",
              "1   153  18   49  144         76.76  \n",
              "2   157   9   66  132         74.65  \n",
              "3   420  55  124  490         79.14  \n",
              "4   145  26   37  156         77.87  \n",
              "5   150  16   47  151         78.15  \n",
              "6   450  25   37  577         92.45  \n",
              "7   137  34   19  174         81.09  \n",
              "8   141  25   28  170         81.16  \n",
              "9   475   0    0  614        100.00  \n",
              "10  143  28   13  180         85.21  \n",
              "11  146  20   16  182         86.96  \n",
              "12  418  57  104  510         81.04  \n",
              "13  137  34   44  149         72.93  \n",
              "14  139  27   48  150         74.04  \n",
              "15  412  63  104  510         80.33  \n",
              "16  137  34   31  162         77.11  \n",
              "17  143  23   42  156         77.31  \n",
              "18  465  10  252  362         71.38  \n",
              "19  161  10   75  118         71.40  \n",
              "20  159   7   81  117         70.79  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "model_NB = GaussianNB(var_smoothing=1e-9)\n",
        "model_name = \"Naive Bayes - Gaussian\"\n",
        "print(model_name)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_NB, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_NB.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model_NB.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "y_pred = model_NB.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "\n",
        "y_test_pred = model_NB.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bernouilli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes - Bernoulli\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.43893115 -0.4175068  -0.39492189 -0.46432352 -0.50344436]\n",
            "Média métricas de validación cruzada: -0.4438255429621882\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8145\n",
            " - Precision: 0.8203\n",
            " - Recall: 0.8145\n",
            " - F1-Score: 0.8152\n",
            " - Adjusted Rand Index: 0.3951\n",
            " - Mean Squared Error: 0.1855\n",
            " - R-squared: 0.2457\n",
            " - Área bajo la curva : 0.818\n",
            " - Confusion Matrix: \n",
            "[[402  73]\n",
            " [129 485]]\n",
            " - Global Score : 76.49\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.7912\n",
            " - Precision: 0.7911\n",
            " - Recall: 0.7912\n",
            " - F1-Score: 0.7911\n",
            " - Adjusted Rand Index: 0.3374\n",
            " - Mean Squared Error: 0.2088\n",
            " - R-squared: 0.1618\n",
            " - Área bajo la curva : 0.790\n",
            " - Confusion Matrix: \n",
            "[[132  39]\n",
            " [ 37 156]]\n",
            " - Global Score : 73.49\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8269\n",
            " - Precision: 0.8330\n",
            " - Recall: 0.8269\n",
            " - F1-Score: 0.8272\n",
            " - Adjusted Rand Index: 0.4259\n",
            " - Mean Squared Error: 0.1731\n",
            " - R-squared: 0.3023\n",
            " - Área bajo la curva : 0.831\n",
            " - Confusion Matrix: \n",
            "[[145  21]\n",
            " [ 42 156]]\n",
            " - Global Score : 78.0\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Model         Set  Accuracy  Precision    Recall  \\\n",
              "0          Regresion Lineal    Training  0.795225   0.825122  0.795225   \n",
              "1          Regresion Lineal  Validation  0.815934   0.827130  0.815934   \n",
              "2          Regresion Lineal        Test  0.793956   0.830307  0.793956   \n",
              "3       Regresión Logística    Training  0.835629   0.843677  0.835629   \n",
              "4       Regresión Logística  Validation  0.826923   0.828750  0.826923   \n",
              "5       Regresión Logística        Test  0.826923   0.839082  0.826923   \n",
              "6         Árbol de Decisión    Training  0.943067   0.943447  0.943067   \n",
              "7         Árbol de Decisión  Validation  0.854396   0.856113  0.854396   \n",
              "8         Árbol de Decisión        Test  0.854396   0.854704  0.854396   \n",
              "9             Random Forest    Training  1.000000   1.000000  1.000000   \n",
              "10            Random Forest  Validation  0.887363   0.889476  0.887363   \n",
              "11            Random Forest        Test  0.901099   0.901102  0.901099   \n",
              "12                      KNN    Training  0.852158   0.856418  0.852158   \n",
              "13                      KNN  Validation  0.785714   0.787289  0.785714   \n",
              "14                      KNN        Test  0.793956   0.799964  0.793956   \n",
              "15                      SVN    Training  0.846648   0.850097  0.846648   \n",
              "16                      SVN  Validation  0.821429   0.821337  0.821429   \n",
              "17                      SVN        Test  0.821429   0.826572  0.821429   \n",
              "18   Naive Bayes - Gaussian    Training  0.759412   0.831542  0.759412   \n",
              "19   Naive Bayes - Gaussian  Validation  0.766484   0.809282  0.766484   \n",
              "20   Naive Bayes - Gaussian        Test  0.758242   0.815378  0.758242   \n",
              "21  Naive Bayes - Bernoulli    Training  0.814509   0.820274  0.814509   \n",
              "22  Naive Bayes - Bernoulli  Validation  0.791209   0.791105  0.791209   \n",
              "23  Naive Bayes - Bernoulli        Test  0.826923   0.833036  0.826923   \n",
              "\n",
              "    F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC  \\\n",
              "0   0.794968             0.347828            0.204775   0.167334  0.809587   \n",
              "1   0.815546             0.397597            0.184066   0.261037  0.820425   \n",
              "2   0.791730             0.343680            0.206044   0.169405  0.806225   \n",
              "3   0.836301             0.450090            0.164371   0.331627  0.841128   \n",
              "4   0.827081             0.425939            0.173077   0.305154  0.828122   \n",
              "5   0.826964             0.425926            0.173077   0.302300  0.833120   \n",
              "6   0.943141             0.785002            0.056933   0.768496  0.943554   \n",
              "7   0.853779             0.501006            0.145604   0.415447  0.851362   \n",
              "8   0.854492             0.501008            0.145604   0.413046  0.853992   \n",
              "9   1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "10  0.886886             0.599091            0.112637   0.547799  0.884450   \n",
              "11  0.900990             0.642523            0.098901   0.601314  0.899355   \n",
              "12  0.852701             0.495592            0.147842   0.398838  0.855309   \n",
              "13  0.785909             0.324677            0.214286   0.139715  0.786595   \n",
              "14  0.794316             0.343845            0.206044   0.169405  0.797463   \n",
              "15  0.847172             0.480168            0.153352   0.376434  0.848994   \n",
              "16  0.821327             0.411647            0.178571   0.283095  0.820274   \n",
              "17  0.821762             0.411656            0.178571   0.280151  0.824662   \n",
              "18  0.754310             0.267385            0.240588   0.021711  0.784262   \n",
              "19  0.761488             0.282026            0.233516   0.062509  0.776460   \n",
              "20  0.752494             0.264371            0.241758   0.025435  0.774370   \n",
              "21  0.815240             0.395110            0.185491   0.245747  0.818109   \n",
              "22  0.791133             0.337388            0.208791   0.161773  0.790110   \n",
              "23  0.827225             0.425943            0.173077   0.302300  0.830686   \n",
              "\n",
              "     TN  FP   FN   TP  Global Score  \n",
              "0   438  37  186  428         74.81  \n",
              "1   153  18   49  144         76.76  \n",
              "2   157   9   66  132         74.65  \n",
              "3   420  55  124  490         79.14  \n",
              "4   145  26   37  156         77.87  \n",
              "5   150  16   47  151         78.15  \n",
              "6   450  25   37  577         92.45  \n",
              "7   137  34   19  174         81.09  \n",
              "8   141  25   28  170         81.16  \n",
              "9   475   0    0  614        100.00  \n",
              "10  143  28   13  180         85.21  \n",
              "11  146  20   16  182         86.96  \n",
              "12  418  57  104  510         81.04  \n",
              "13  137  34   44  149         72.93  \n",
              "14  139  27   48  150         74.04  \n",
              "15  412  63  104  510         80.33  \n",
              "16  137  34   31  162         77.11  \n",
              "17  143  23   42  156         77.31  \n",
              "18  465  10  252  362         71.38  \n",
              "19  161  10   75  118         71.40  \n",
              "20  159   7   81  117         70.79  \n",
              "21  402  73  129  485         76.49  \n",
              "22  132  39   37  156         73.49  \n",
              "23  145  21   42  156         78.00  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "model_NB = BernoulliNB()\n",
        "model_name = \"Naive Bayes - Bernoulli\"\n",
        "print(model_name)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_NB, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_NB.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model_NB.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "y_pred = model_NB.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "\n",
        "y_test_pred = model_NB.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.35838578 -0.27091418 -0.31037119 -0.30289127 -0.27153769]\n",
            "Média métricas de validación cruzada: -0.30282002265725105\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[475   0]\n",
            " [  0 614]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8901\n",
            " - Precision: 0.8906\n",
            " - Recall: 0.8901\n",
            " - F1-Score: 0.8899\n",
            " - Adjusted Rand Index: 0.6077\n",
            " - Mean Squared Error: 0.1099\n",
            " - R-squared: 0.5588\n",
            " - Área bajo la curva : 0.888\n",
            " - Confusion Matrix: \n",
            "[[147  24]\n",
            " [ 16 177]]\n",
            " - Global Score : 85.58\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9286\n",
            " - Precision: 0.9287\n",
            " - Recall: 0.9286\n",
            " - F1-Score: 0.9286\n",
            " - Adjusted Rand Index: 0.7340\n",
            " - Mean Squared Error: 0.0714\n",
            " - R-squared: 0.7121\n",
            " - Área bajo la curva : 0.929\n",
            " - Confusion Matrix: \n",
            "[[154  12]\n",
            " [ 14 184]]\n",
            " - Global Score : 90.55\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.890577</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.889909</td>\n",
              "      <td>0.607662</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.558828</td>\n",
              "      <td>0.888374</td>\n",
              "      <td>147</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>177</td>\n",
              "      <td>85.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928693</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928604</td>\n",
              "      <td>0.733959</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.712060</td>\n",
              "      <td>0.928502</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>184</td>\n",
              "      <td>90.55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Model         Set  Accuracy  Precision    Recall  \\\n",
              "0          Regresion Lineal    Training  0.795225   0.825122  0.795225   \n",
              "1          Regresion Lineal  Validation  0.815934   0.827130  0.815934   \n",
              "2          Regresion Lineal        Test  0.793956   0.830307  0.793956   \n",
              "3       Regresión Logística    Training  0.835629   0.843677  0.835629   \n",
              "4       Regresión Logística  Validation  0.826923   0.828750  0.826923   \n",
              "5       Regresión Logística        Test  0.826923   0.839082  0.826923   \n",
              "6         Árbol de Decisión    Training  0.943067   0.943447  0.943067   \n",
              "7         Árbol de Decisión  Validation  0.854396   0.856113  0.854396   \n",
              "8         Árbol de Decisión        Test  0.854396   0.854704  0.854396   \n",
              "9             Random Forest    Training  1.000000   1.000000  1.000000   \n",
              "10            Random Forest  Validation  0.887363   0.889476  0.887363   \n",
              "11            Random Forest        Test  0.901099   0.901102  0.901099   \n",
              "12                      KNN    Training  0.852158   0.856418  0.852158   \n",
              "13                      KNN  Validation  0.785714   0.787289  0.785714   \n",
              "14                      KNN        Test  0.793956   0.799964  0.793956   \n",
              "15                      SVN    Training  0.846648   0.850097  0.846648   \n",
              "16                      SVN  Validation  0.821429   0.821337  0.821429   \n",
              "17                      SVN        Test  0.821429   0.826572  0.821429   \n",
              "18   Naive Bayes - Gaussian    Training  0.759412   0.831542  0.759412   \n",
              "19   Naive Bayes - Gaussian  Validation  0.766484   0.809282  0.766484   \n",
              "20   Naive Bayes - Gaussian        Test  0.758242   0.815378  0.758242   \n",
              "21  Naive Bayes - Bernoulli    Training  0.814509   0.820274  0.814509   \n",
              "22  Naive Bayes - Bernoulli  Validation  0.791209   0.791105  0.791209   \n",
              "23  Naive Bayes - Bernoulli        Test  0.826923   0.833036  0.826923   \n",
              "24               AdaBoost 1    Training  1.000000   1.000000  1.000000   \n",
              "25               AdaBoost 1  Validation  0.890110   0.890577  0.890110   \n",
              "26               AdaBoost 1        Test  0.928571   0.928693  0.928571   \n",
              "\n",
              "    F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC  \\\n",
              "0   0.794968             0.347828            0.204775   0.167334  0.809587   \n",
              "1   0.815546             0.397597            0.184066   0.261037  0.820425   \n",
              "2   0.791730             0.343680            0.206044   0.169405  0.806225   \n",
              "3   0.836301             0.450090            0.164371   0.331627  0.841128   \n",
              "4   0.827081             0.425939            0.173077   0.305154  0.828122   \n",
              "5   0.826964             0.425926            0.173077   0.302300  0.833120   \n",
              "6   0.943141             0.785002            0.056933   0.768496  0.943554   \n",
              "7   0.853779             0.501006            0.145604   0.415447  0.851362   \n",
              "8   0.854492             0.501008            0.145604   0.413046  0.853992   \n",
              "9   1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "10  0.886886             0.599091            0.112637   0.547799  0.884450   \n",
              "11  0.900990             0.642523            0.098901   0.601314  0.899355   \n",
              "12  0.852701             0.495592            0.147842   0.398838  0.855309   \n",
              "13  0.785909             0.324677            0.214286   0.139715  0.786595   \n",
              "14  0.794316             0.343845            0.206044   0.169405  0.797463   \n",
              "15  0.847172             0.480168            0.153352   0.376434  0.848994   \n",
              "16  0.821327             0.411647            0.178571   0.283095  0.820274   \n",
              "17  0.821762             0.411656            0.178571   0.280151  0.824662   \n",
              "18  0.754310             0.267385            0.240588   0.021711  0.784262   \n",
              "19  0.761488             0.282026            0.233516   0.062509  0.776460   \n",
              "20  0.752494             0.264371            0.241758   0.025435  0.774370   \n",
              "21  0.815240             0.395110            0.185491   0.245747  0.818109   \n",
              "22  0.791133             0.337388            0.208791   0.161773  0.790110   \n",
              "23  0.827225             0.425943            0.173077   0.302300  0.830686   \n",
              "24  1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "25  0.889909             0.607662            0.109890   0.558828  0.888374   \n",
              "26  0.928604             0.733959            0.071429   0.712060  0.928502   \n",
              "\n",
              "     TN  FP   FN   TP  Global Score  \n",
              "0   438  37  186  428         74.81  \n",
              "1   153  18   49  144         76.76  \n",
              "2   157   9   66  132         74.65  \n",
              "3   420  55  124  490         79.14  \n",
              "4   145  26   37  156         77.87  \n",
              "5   150  16   47  151         78.15  \n",
              "6   450  25   37  577         92.45  \n",
              "7   137  34   19  174         81.09  \n",
              "8   141  25   28  170         81.16  \n",
              "9   475   0    0  614        100.00  \n",
              "10  143  28   13  180         85.21  \n",
              "11  146  20   16  182         86.96  \n",
              "12  418  57  104  510         81.04  \n",
              "13  137  34   44  149         72.93  \n",
              "14  139  27   48  150         74.04  \n",
              "15  412  63  104  510         80.33  \n",
              "16  137  34   31  162         77.11  \n",
              "17  143  23   42  156         77.31  \n",
              "18  465  10  252  362         71.38  \n",
              "19  161  10   75  118         71.40  \n",
              "20  159   7   81  117         70.79  \n",
              "21  402  73  129  485         76.49  \n",
              "22  132  39   37  156         73.49  \n",
              "23  145  21   42  156         78.00  \n",
              "24  475   0    0  614        100.00  \n",
              "25  147  24   16  177         85.58  \n",
              "26  154  12   14  184         90.55  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_name = \"AdaBoost 1\"\n",
        "print(model_name)\n",
        "\n",
        "# Inicializar el clasificador débil (stump)\n",
        "base_estimator = DecisionTreeClassifier(max_depth=2) # max_depth=1 menos sobreajuste, resultados ligeramente peores \n",
        "\n",
        "# Inicializar el modelo AdaBoost\n",
        "model = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "y_pred = model.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "\n",
        "y_test_pred = model.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros: {'learning_rate': 0.07, 'n_estimators': 200}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.28734789 -0.21417647 -0.25341701 -0.23461857 -0.24476077]\n",
            "Média métricas de validación cruzada: -0.24686414084382471\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Training set :\n",
            " - Accuracy: 0.9780\n",
            " - Precision: 0.9780\n",
            " - Recall: 0.9780\n",
            " - F1-Score: 0.9780\n",
            " - Adjusted Rand Index: 0.9137\n",
            " - Mean Squared Error: 0.0220\n",
            " - R-squared: 0.9104\n",
            " - Área bajo la curva : 0.977\n",
            " - Confusion Matrix: \n",
            "[[462  13]\n",
            " [ 11 603]]\n",
            " - Global Score : 97.02\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9286\n",
            " - Precision: 0.9292\n",
            " - Recall: 0.9286\n",
            " - F1-Score: 0.9284\n",
            " - Adjusted Rand Index: 0.7340\n",
            " - Mean Squared Error: 0.0714\n",
            " - R-squared: 0.7132\n",
            " - Área bajo la curva : 0.927\n",
            " - Confusion Matrix: \n",
            "[[154  17]\n",
            " [  9 184]]\n",
            " - Global Score : 90.51\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9423\n",
            " - Precision: 0.9423\n",
            " - Recall: 0.9423\n",
            " - F1-Score: 0.9423\n",
            " - Adjusted Rand Index: 0.7819\n",
            " - Mean Squared Error: 0.0577\n",
            " - R-squared: 0.7674\n",
            " - Área bajo la curva : 0.941\n",
            " - Confusion Matrix: \n",
            "[[154  12]\n",
            " [  9 189]]\n",
            " - Global Score : 92.3\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.890577</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.889909</td>\n",
              "      <td>0.607662</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.558828</td>\n",
              "      <td>0.888374</td>\n",
              "      <td>147</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>177</td>\n",
              "      <td>85.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928693</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928604</td>\n",
              "      <td>0.733959</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.712060</td>\n",
              "      <td>0.928502</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>184</td>\n",
              "      <td>90.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977957</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977956</td>\n",
              "      <td>0.913688</td>\n",
              "      <td>0.022039</td>\n",
              "      <td>0.910386</td>\n",
              "      <td>0.977358</td>\n",
              "      <td>462</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>603</td>\n",
              "      <td>97.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.929217</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928441</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>0.926976</td>\n",
              "      <td>154</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>184</td>\n",
              "      <td>90.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942345</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942262</td>\n",
              "      <td>0.781936</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.767433</td>\n",
              "      <td>0.941128</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>189</td>\n",
              "      <td>92.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Model         Set  Accuracy  Precision    Recall  \\\n",
              "0          Regresion Lineal    Training  0.795225   0.825122  0.795225   \n",
              "1          Regresion Lineal  Validation  0.815934   0.827130  0.815934   \n",
              "2          Regresion Lineal        Test  0.793956   0.830307  0.793956   \n",
              "3       Regresión Logística    Training  0.835629   0.843677  0.835629   \n",
              "4       Regresión Logística  Validation  0.826923   0.828750  0.826923   \n",
              "5       Regresión Logística        Test  0.826923   0.839082  0.826923   \n",
              "6         Árbol de Decisión    Training  0.943067   0.943447  0.943067   \n",
              "7         Árbol de Decisión  Validation  0.854396   0.856113  0.854396   \n",
              "8         Árbol de Decisión        Test  0.854396   0.854704  0.854396   \n",
              "9             Random Forest    Training  1.000000   1.000000  1.000000   \n",
              "10            Random Forest  Validation  0.887363   0.889476  0.887363   \n",
              "11            Random Forest        Test  0.901099   0.901102  0.901099   \n",
              "12                      KNN    Training  0.852158   0.856418  0.852158   \n",
              "13                      KNN  Validation  0.785714   0.787289  0.785714   \n",
              "14                      KNN        Test  0.793956   0.799964  0.793956   \n",
              "15                      SVN    Training  0.846648   0.850097  0.846648   \n",
              "16                      SVN  Validation  0.821429   0.821337  0.821429   \n",
              "17                      SVN        Test  0.821429   0.826572  0.821429   \n",
              "18   Naive Bayes - Gaussian    Training  0.759412   0.831542  0.759412   \n",
              "19   Naive Bayes - Gaussian  Validation  0.766484   0.809282  0.766484   \n",
              "20   Naive Bayes - Gaussian        Test  0.758242   0.815378  0.758242   \n",
              "21  Naive Bayes - Bernoulli    Training  0.814509   0.820274  0.814509   \n",
              "22  Naive Bayes - Bernoulli  Validation  0.791209   0.791105  0.791209   \n",
              "23  Naive Bayes - Bernoulli        Test  0.826923   0.833036  0.826923   \n",
              "24               AdaBoost 1    Training  1.000000   1.000000  1.000000   \n",
              "25               AdaBoost 1  Validation  0.890110   0.890577  0.890110   \n",
              "26               AdaBoost 1        Test  0.928571   0.928693  0.928571   \n",
              "27               AdaBoost 2    Training  0.977961   0.977957  0.977961   \n",
              "28               AdaBoost 2  Validation  0.928571   0.929217  0.928571   \n",
              "29               AdaBoost 2        Test  0.942308   0.942345  0.942308   \n",
              "\n",
              "    F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC  \\\n",
              "0   0.794968             0.347828            0.204775   0.167334  0.809587   \n",
              "1   0.815546             0.397597            0.184066   0.261037  0.820425   \n",
              "2   0.791730             0.343680            0.206044   0.169405  0.806225   \n",
              "3   0.836301             0.450090            0.164371   0.331627  0.841128   \n",
              "4   0.827081             0.425939            0.173077   0.305154  0.828122   \n",
              "5   0.826964             0.425926            0.173077   0.302300  0.833120   \n",
              "6   0.943141             0.785002            0.056933   0.768496  0.943554   \n",
              "7   0.853779             0.501006            0.145604   0.415447  0.851362   \n",
              "8   0.854492             0.501008            0.145604   0.413046  0.853992   \n",
              "9   1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "10  0.886886             0.599091            0.112637   0.547799  0.884450   \n",
              "11  0.900990             0.642523            0.098901   0.601314  0.899355   \n",
              "12  0.852701             0.495592            0.147842   0.398838  0.855309   \n",
              "13  0.785909             0.324677            0.214286   0.139715  0.786595   \n",
              "14  0.794316             0.343845            0.206044   0.169405  0.797463   \n",
              "15  0.847172             0.480168            0.153352   0.376434  0.848994   \n",
              "16  0.821327             0.411647            0.178571   0.283095  0.820274   \n",
              "17  0.821762             0.411656            0.178571   0.280151  0.824662   \n",
              "18  0.754310             0.267385            0.240588   0.021711  0.784262   \n",
              "19  0.761488             0.282026            0.233516   0.062509  0.776460   \n",
              "20  0.752494             0.264371            0.241758   0.025435  0.774370   \n",
              "21  0.815240             0.395110            0.185491   0.245747  0.818109   \n",
              "22  0.791133             0.337388            0.208791   0.161773  0.790110   \n",
              "23  0.827225             0.425943            0.173077   0.302300  0.830686   \n",
              "24  1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "25  0.889909             0.607662            0.109890   0.558828  0.888374   \n",
              "26  0.928604             0.733959            0.071429   0.712060  0.928502   \n",
              "27  0.977956             0.913688            0.022039   0.910386  0.977358   \n",
              "28  0.928441             0.733961            0.071429   0.713238  0.926976   \n",
              "29  0.942262             0.781936            0.057692   0.767433  0.941128   \n",
              "\n",
              "     TN  FP   FN   TP  Global Score  \n",
              "0   438  37  186  428         74.81  \n",
              "1   153  18   49  144         76.76  \n",
              "2   157   9   66  132         74.65  \n",
              "3   420  55  124  490         79.14  \n",
              "4   145  26   37  156         77.87  \n",
              "5   150  16   47  151         78.15  \n",
              "6   450  25   37  577         92.45  \n",
              "7   137  34   19  174         81.09  \n",
              "8   141  25   28  170         81.16  \n",
              "9   475   0    0  614        100.00  \n",
              "10  143  28   13  180         85.21  \n",
              "11  146  20   16  182         86.96  \n",
              "12  418  57  104  510         81.04  \n",
              "13  137  34   44  149         72.93  \n",
              "14  139  27   48  150         74.04  \n",
              "15  412  63  104  510         80.33  \n",
              "16  137  34   31  162         77.11  \n",
              "17  143  23   42  156         77.31  \n",
              "18  465  10  252  362         71.38  \n",
              "19  161  10   75  118         71.40  \n",
              "20  159   7   81  117         70.79  \n",
              "21  402  73  129  485         76.49  \n",
              "22  132  39   37  156         73.49  \n",
              "23  145  21   42  156         78.00  \n",
              "24  475   0    0  614        100.00  \n",
              "25  147  24   16  177         85.58  \n",
              "26  154  12   14  184         90.55  \n",
              "27  462  13   11  603         97.02  \n",
              "28  154  17    9  184         90.51  \n",
              "29  154  12    9  189         92.30  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import  GridSearchCV\n",
        "\n",
        "model_name = \"AdaBoost 2\"\n",
        "print(model_name)\n",
        "\n",
        "# Inicializar el clasificador débil (stump)\n",
        "base_estimator = DecisionTreeClassifier(max_depth=2)\n",
        "\n",
        "# Configurar la búsqueda de hiperparámetros\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 250, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.07, 0.10, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Inicializar el modelo AdaBoost\n",
        "ada = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=ada, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Entrenar el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Mejores hiperparámetros: {best_params}\")\n",
        "\n",
        "# Mejor modelo\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(best_model, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "best_model.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = best_model.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "\n",
        "y_pred = best_model.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "\n",
        "y_test_pred = best_model.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      CA19-9 (U/ml)  CA-125 (U/ml)  HGF (pg/ml)  OPN (pg/ml)  Omega score  \\\n",
            "1333      -0.092602      -0.107994    -0.309247     0.050290    -0.143176   \n",
            "1784      -0.092321      16.603266     0.504689     3.463351     0.336480   \n",
            "1465      -0.092602      -0.106787    -0.057182    -0.739401    -0.163334   \n",
            "1252      -0.092602      -0.107994    -0.309247    -0.413678    -0.177269   \n",
            "53        -0.092251      -0.107994    -0.312427    -0.139405    -0.158614   \n",
            "...             ...            ...          ...          ...          ...   \n",
            "797       -0.091747      -0.108175    -0.287776     0.012175    -0.155475   \n",
            "1669      -0.093555      -0.109863    -0.319221    -0.389669    -0.167576   \n",
            "766       -0.086317      -0.074770     0.218594     1.107403    -0.122682   \n",
            "75        -0.092251      -0.107994    -0.312427    -0.024325    -0.058391   \n",
            "1541      -0.053513      -0.096282    -0.280971    -0.422183    -0.132929   \n",
            "\n",
            "      Prolactin (pg/ml)  CEA (pg/ml)  Myeloperoxidase (ng/ml)  TIMP-1 (pg/ml)  \n",
            "1333          -0.362952    -0.130982                -0.250333       -1.017092  \n",
            "1784           0.506493    -0.116108                 0.069232        1.860676  \n",
            "1465          -0.385378    -0.129505                -0.321950       -0.224166  \n",
            "1252          -0.325200    -0.110392                -0.361861       -0.472180  \n",
            "53            -0.333856    -0.156455                -0.283847       -0.709295  \n",
            "...                 ...          ...                      ...             ...  \n",
            "797           -0.168136    -0.104257                -0.190397       -0.569358  \n",
            "1669          -0.438204    -0.111057                -0.371178       -0.205878  \n",
            "766           -0.286075    -0.142170                -0.170511        0.129632  \n",
            "75            -0.440049    -0.082846                -0.342670       -0.468926  \n",
            "1541          -0.448495    -0.139981                -0.341140        0.000447  \n",
            "\n",
            "[1089 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "print(X_train_prep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting\n",
            "Mejores hiperparámetros: {'learning_rate': 0.5, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.25341701 -0.24419875 -0.21417647 -0.23461857 -0.25400025]\n",
            "Média métricas de validación cruzada: -0.24008220989590182\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[475   0]\n",
            " [  0 614]]\n",
            " - Global Score : 100.0\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHFCAYAAAAe+pb9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABw8klEQVR4nO3dd1gUV9sG8HvpTUBRERRBVGJHBQsoGhsqiT2xgBp7S6KiibEkGjXGvJbYsWBXUBMV3+hrQ2PHEhBsYOyiAqIQAREpu+f7w89JNqCyuDDA3r/r2ivsmZmde3eC+3DmzBmFEEKAiIiISAfpyR2AiIiISC4shIiIiEhnsRAiIiIincVCiIiIiHQWCyEiIiLSWSyEiIiISGexECIiIiKdxUKIiIiIdBYLISIiItJZLISIKF8uX76MwYMHo1q1ajAxMYGFhQUaN26MefPmITk5We54BaJQKNQelpaW8PT0xLZt2964zblz5/Dpp5/Czs4ORkZGqFSpEj755BOcPXv2jduUxs+OqLRgIURE7xQYGAg3Nzf88ccf+Prrr3Hw4EGEhITg008/xapVqzB06FC5IxbY6yImLCwMq1atQmpqKnx9fREcHJxr3WXLlqFFixZ4+PAh5s2bhyNHjmDBggV49OgRWrZsieXLl+fapjR/dkSlgiAieouwsDChr68vOnXqJF6+fJlreWZmpvjvf/+rlX29ePFCqFQqrbxWfgAQn3/+uVrbvXv3BADRqlUrtfbTp08LPT098fHHH4vs7Gy1ZdnZ2eLjjz8Wenp64vTp01J7UX52RFQw7BEiorf68ccfoVAosGbNGhgbG+dabmRkhK5du0rPFQoFvv/++1zrOTk5YdCgQdLzjRs3QqFQ4PDhwxgyZAgqVKgAMzMz7NixAwqFAkePHs31GitXroRCocDly5cBAOHh4ejbty+cnJxgamoKJycn9OvXD/fv3y/w+3V0dESFChXw+PFjtfa5c+dCoVBg5cqVMDAwUFtmYGCAgIAAKBQK/PTTT1K7pp8dERU9FkJE9EZKpRK///473Nzc4ODgUCj7GDJkCAwNDbFlyxbs3LkTPXr0QMWKFbFhw4Zc627cuBGNGzdGgwYNAAD37t3DBx98gMWLF+PQoUP4z3/+g/j4eDRp0gRPnz4tUJ6UlBQkJyfDxcVFalMqlTh27Bjc3d1RpUqVPLdzcHCAm5sbfv/9dyiVyiL57Ijo/Rm8exUi0lVPnz7FixcvUK1atULbR7t27bB69Wq1tv79+2PlypVISUmBlZUVACAmJgYXLlzAsmXLpPU++eQTfPLJJ9JzpVKJjz/+GLa2tggODsbYsWPfuX8hBHJyciCEwL179/DVV1/BzMwMM2bMkNbJ7+dQrVo1XLhwAUlJSRBCFPpnR0Tvjz1CRCSrXr165WobMmQIMjIysGPHDqltw4YNMDY2hq+vr9T2/PlzfPPNN6hRowYMDAxgYGAACwsLpKenIyYmJl/7DwgIgKGhIYyMjODi4oIDBw5g27ZtcHNz0/i9CCEAvDo9SEQlAwshInqj8uXLw8zMDHfv3i20fdjZ2eVqq1u3Lpo0aSKdHlMqldi6dSu6deuGcuXKSev5+vpi+fLlGDZsGA4dOoQLFy7gjz/+QIUKFZCRkZGv/ffu3Rt//PEHwsLCsHr1apQpUwZ9+/bFzZs3pXXy+zncu3cPZmZmKFeuXJF8dkT0/lgIEdEb6evro127doiIiMDDhw/ztY2xsTEyMzNztSclJeW5/pt6TwYPHoxz584hJiYGBw8eRHx8PAYPHiwtT0lJwb59+zBp0iRMnjwZ7dq1Q5MmTVC/fn2N5uapUKEC3N3d4eHhgREjRmDPnj1IT0+Hv7+/tI6+vj7atGmD8PDwN34ODx8+REREBNq2bQt9ff0CfXZEVPRYCBHRW02ZMgVCCAwfPhxZWVm5lmdnZ2Pv3r3ScycnJ+mqrtd+//13PH/+XKP99uvXDyYmJti4cSM2btyIypUrw9vbW1quUCgghMh1NdbatWuhVCo12tc/eXl5YeDAgfjf//6nNkni689hzJgxuV5fqVRi9OjREEJgypQpubbJ72dHREWPg6WJ6K08PDywcuVKjBkzBm5ubhg9ejTq1q2L7OxsREZGYs2aNahXrx66dOkCABgwYAC+++47TJ8+Ha1bt0Z0dDSWL18uDXrOL2tra/To0QMbN27Es2fP8NVXX0FP7++/3SwtLdGqVSvMnz8f5cuXh5OTE06cOIF169bB2tr6vd7z7NmzsWPHDnz33Xc4cuQIAKBFixZYvHgxxo8fj5YtW+KLL75A1apVERsbixUrVuD8+fNYvHgxPD09C/zZEZEMZJvBiIhKlKioKPHZZ5+JqlWrCiMjI2Fubi4aNWokpk+fLhITE6X1MjMzxaRJk4SDg4MwNTUVrVu3FlFRUcLR0VF89tln0nobNmwQAMQff/zxxn0ePnxYABAAxI0bN3Itf/jwoejVq5coW7asKFOmjOjUqZO4evVqrn29CfKYUPG1r7/+WgAQJ06cUGs/e/as+OSTT4Stra0wMDAQFStWFD179hRhYWFv3E9+PzsiKnoKIf7/MgciIiIiHcMxQkRERKSzWAgRERGRzmIhRERERDqLhRARERHpLBZCREREpLNYCBEREZHO0rkJFVUqFeLi4lCmTBneGJGIiKiEEEIgLS0N9vb2apOrvi+dK4Ti4uLg4OAgdwwiIiIqgAcPHqBKlSpaez2dK4TKlCkD4NUHaWlpKXMaIiIiyo/U1FQ4ODhI3+PaonOF0OvTYZaWliyEiIiIShhtD2vhYGkiIiLSWSyEiIiISGexECIiIiKdxUKIiIiIdBYLISIiItJZLISIiIhIZ7EQIiIiIp3FQoiIiIh0FgshIiIi0lkshIiIiEhnyVoInTx5El26dIG9vT0UCgX27Nnzzm1OnDgBNzc3mJiYwNnZGatWrSr8oERERFQqyVoIpaenw9XVFcuXL8/X+nfv3oWPjw+8vLwQGRmJqVOnYuzYsdi1a1chJyUiIqLSSNabrnbu3BmdO3fO9/qrVq1C1apVsXjxYgBA7dq1ER4ejgULFqBXr16FlJKIiIhKqxI1Rujs2bPw9vZWa+vYsSPCw8ORnZ0tUyoiIiIqbFevJhbK68raI6SphIQE2NraqrXZ2toiJycHT58+hZ2dXa5tMjMzkZmZKT1PTU199cP6WoBpiaoDiYiIdE5KhiG+2N4CW887FMrrl6hCCAAUCoXacyFEnu2vzZ07FzNnzsy9ID0eUGo9HhEREWnJmbsO6B/cDff+KgvgZaHso0QVQpUqVUJCQoJaW2JiIgwMDGBjY5PnNlOmTMGECROk56mpqXBwcAAUCsDCvlDzEhERUcFkZuuhb3AfPPzLAgBQxjgbaZnv2KgASlQh5OHhgb1796q1HT58GO7u7jA0NMxzG2NjYxgbG+deYFYJGPmwMGISERHRezIGsM7lNjp23IoWLRywcmV7NGiwUOv7kXWQzPPnzxEVFYWoqCgAry6Pj4qKQmxsLIBXvTkDBw6U1h81ahTu37+PCRMmICYmBuvXr8e6devw1VdfyRGfiIiItEQIgYwM9QufvL2r49Ch/jh+fBAcHa0LZb+yFkLh4eFo1KgRGjVqBACYMGECGjVqhOnTpwMA4uPjpaIIAKpVq4b9+/fj+PHjaNiwIWbPno2lS5fy0nkiIqISLDk5A3367ETv3julsb+veXtXh4FB4ZUrCvHvPZZyqampsLKyQsoiO1iOj5M7DhERkU47duwuBgwIwaNHaQCAgAAfjB7dJNd60vd3SgosLS21tn9eP05ERERFLitLiUmTQtGu3WapCCpb1gSVKlkUaY4SNViaiIiISr7r15/C13cXIiP/vhK8bdtq2LSpO6pU0V5vT36wECIiIqIiIYTA6tURmDDhEDIycgAAhoZ6mDu3Hfz9PaCnl/ecgIWJhRAREREVuszMHHz66a/Yu/eG1Fa7dnkEBfVEo0a57wxRVDhGiIiIiAqdsbEBypT5e16/MWPcER4+QtYiCGCPEBERERWRFSt8cPNmEqZPb42PP3aROw4AFkJERERUCC5ffoy4uDR06lRDarO2NsH588PeeH9QOfDUGBEREWmNSiWwaNFZNGkSCF/fXXj4MFVteXEqggAWQkRERKQlr3qAtmLChMPIylLir79e4scfT8kd6614aoyIiIje25491zFs2G9ISsqQ2iZO9MCcOW1lTPVuLISIiIiowNLTs+DvfwiBgRelNjs7C2ze3APt2zvLmCx/WAgRERFRgYSHx8HPbzdu3EiS2nr0qIXAwC6wsTGTMVn+sRAiIiIijb18mYOuXbchPv45AMDMzBBLl3bCkCGNit2A6LfhYGkiIiLSmImJAQICPgIANGlij6iokRg6tHGJKoIA9ggRERFRPmVlKWFkpC897969FkJC+uCjj2rC0FD/LVsWX+wRIiIiordKSXmJAQNC0L//bggh1JZ1716rxBZBAHuEiIiI6C3OnIlF//4huHfvGQDgo48u4bPPGsqaSZvYI0RERES5ZGcrMX36MbRqtVEqgiwtjWFiUrr6UErXuyEiIqL3dutWMvr3343z5x9JbS1aOGDr1p5wcrKWL1ghYCFEREREAAAhBDZujMKXXx5Aeno2AEBfX4Hvv/8Qkye3hIFB6TuRxEKIiIiI8PJlDgYMCMHOndFSW/XqZREU1BPNmlWRMVnhYiFEREREMDbWR3a2Uno+dGgjLF7cCRYWRjKmKnylr4+LiIiINKZQKLB2bVfUrVsBO3d+irVru5b6IghgjxAREZFOun79KR4/fo7WrZ2ktvLlzXD58mjo6ZWs2aHfB3uEiIiIdIgQAqtWhaNx49Xo3XsnHj9+rrZcl4oggIUQERGRzkhMTEe3btsxevT/kJGRg8TEdMyefVLuWLLiqTEiIiIdcODATQwe/F88fpwutX3+eRPMm9dBxlTyYyFERERUimVkZOObb45g2bILUlvFiuZYv74rPvrIRcZkxQMLISIiolLq0qUE+PntxrVrT6Q2H5+aWL++K2xtLWRMVnywECIiIiqFMjKy4e29FYmJr06FmZgYYMGCDhgzpgkUCt0aEP02HCxNRERUCpmaGmLRoo4AAFdXW0REjMDnnzdlEfQv7BEiIiIqJZRKFfT1/+7j8PWtDyEEPvmkDoyN+ZWfF/YIERERlXDp6VkYMWIvhg3bm2uZn18DFkFvwU+GiIioBAsPj4Of327cuJEEAPDxqYFPP60rc6qSgz1CREREJZBSqcLcuafg4bFOKoLMzAyRmal8x5b0T+wRIiIiKmFiY1MwYEAITp68L7W5u9sjKKgnXFxsZExW8rAQIiIiKkG2b7+KUaP2ISUlEwCgUABTp3phxozWMDTUlzldycNCiIiIqATIyMjGyJH7sGXLZamtalUrbN3aA15ejjImK9lYCBEREZUAxsYGavcJ8/WtjxUrfGBtbSJjqpKPg6WJiIhKAD09BTZu7Ibq1cti69YeCArqySJIC9gjREREVAzdupWMpKQXaNasitRmZ1cG169/AQMD9mNoCz9JIiKiYkQIgQ0bItGw4Sr06vULkpMz1JazCNIufppERETFRHJyBnr33okhQ35Deno2Hj1Kw8yZx+WOVarx1BgREVExcOzYXQwYEIJHj9KktqFDG2HOnHYypir9WAgRERHJKCtLiW+//R0LFoRBiFdtZcuaIDCwC3r1qiNvOB3AQoiIiEgm168/ha/vLkRGJkhtbdtWw6ZN3VGliqWMyXQHCyEiIiIZvHiRjVatNuDJkxcAAENDPcyd2w7+/h7Q01PInE53cLA0ERGRDMzMDDFnTlsAQO3a5XHhwnBMnOjJIqiIsUeIiIioiAghoFD8XegMG9YYQgD9+zeAmZmhjMl0FwshIiKiQpaRkY1vvjkCIQSWLfOR2hUKBUaMcJMxGbEQIiIiKkSXLiXAz283rl17AgDo1KkGPvrIReZU9BrHCBERERUClUpg0aKzaNp0rVQEmZgYSIOjqXhgjxAREZGWxcWlYdCgPQgNvSO1ubraIji4F+rUqSBjMvo3FkJERERaFBISg+HD9yIp6e97hE2c6IE5c9rC2Jhfu8UNjwgREZEWvHyZg7FjDyAw8KLUZm9fBps2dUf79s4yJqO3YSFERESkBYaGerh+/an0vEePWggM7AIbGzMZU9G7cLA0ERGRFujr62HLlh6oXLkM1q7tgl27erMIKgHYI0RERFQA9+8/w19/vUTDhpWkNkdHa9y+PZZjgUoQ9ggRERFpaNu2K3B1XYWePXcgNTVTbRmLoJKFhRAREVE+paS8xIABIfD13Y2UlEzcvfsMM2celzsWvQfZC6GAgABUq1YNJiYmcHNzw6lTp966flBQEFxdXWFmZgY7OzsMHjwYSUlJRZSWiIh01ZkzsWjYcDW2br0stfn61sf06a1lTEXvS9ZCaMeOHRg/fjymTZuGyMhIeHl5oXPnzoiNjc1z/dOnT2PgwIEYOnQorl27hl9//RV//PEHhg0bVsTJiYhIV2RnKzF9+jG0arUR9+49AwBYWhpj69YeCArqCSsrE3kD0nvRuBC6ePEirly5Ij3/73//i+7du2Pq1KnIysrS6LV+/vlnDB06FMOGDUPt2rWxePFiODg4YOXKlXmuf+7cOTg5OWHs2LGoVq0aWrZsiZEjRyI8PFzTt0FERPROt28nw8trA2bPPgmVSgAAWrasikuXRsHPr4HM6UgbNC6ERo4ciRs3bgAA7ty5g759+8LMzAy//vorJk2alO/XycrKQkREBLy9vdXavb29ERYWluc2np6eePjwIfbv3w8hBB4/foydO3fio48+euN+MjMzkZqaqvYgIiJ6l/T0LDRvvg7nzz8CAOjrK/DDD21w/PhncHKyljccaY3GhdCNGzfQsGFDAMCvv/6KVq1aITg4GBs3bsSuXbvy/TpPnz6FUqmEra2tWrutrS0SEhLy3MbT0xNBQUHo06cPjIyMUKlSJVhbW2PZsmVv3M/cuXNhZWUlPRwcHPKdkYiIdJe5uRG+/dYLAFC9elmEhQ3FtGmtoK8v+/Ba0iKNj6YQAiqVCgBw5MgR+Pj4AAAcHBzw9OnTt22aJ4VCkev1/932WnR0NMaOHYvp06cjIiICBw8exN27dzFq1Kg3vv6UKVOQkpIiPR48eKBxRiIi0g1CCLXnX37ZDD//7I2oqFFo2rSyTKmoMGk82YG7uzt++OEHtG/fHidOnJDG89y9ezdX787blC9fHvr6+rl6fxITE9/4OnPnzkWLFi3w9ddfAwAaNGgAc3NzeHl54YcffoCdnV2ubYyNjWFsbJzvXEREpHuyspT49tvfoaenwE8/tZfa9fQU8Pf3kDEZFTaNe4QWL16Mixcv4osvvsC0adNQo0YNAMDOnTvh6emZ79cxMjKCm5sbQkND1dpDQ0Pf+DovXryAnp56ZH19fQC5q3giIqL8iIl5gubN12L+/DDMm3cGx47dlTsSFSGNe4QaNGigdtXYa/Pnz5eKkvyaMGECBgwYAHd3d3h4eGDNmjWIjY2VTnVNmTIFjx49wubNmwEAXbp0wfDhw7Fy5Up07NgR8fHxGD9+PJo2bQp7e3tN3woREekwIQRWrQrHxImHkZGRAwAwMNDD7dt/oU2bajKno6JS4HnAIyIiEBMTA4VCgdq1a6Nx48Yav0afPn2QlJSEWbNmIT4+HvXq1cP+/fvh6OgIAIiPj1ebU2jQoEFIS0vD8uXLMXHiRFhbW6Nt27b4z3/+U9C3QUREOigxMR1Dh/6GfftuSG21a5dHcHAvtXuHUemnEBqeU0pMTESfPn1w4sQJWFtbQwiBlJQUtGnTBtu3b0eFChUKK6tWpKamwsrKCimL7GA5Pk7uOEREVMQOHLiJQYP+i8TEdKltzBh3zJ/vDTMzQxmT0dtI398pKbC0tNTa62o8RujLL79EWloarl27huTkZPz111+4evUqUlNTMXbsWK0FIyIi0qaXL3MwduwB+PgES0VQhQpm2Lu3H1as+IhFkI7S+NTYwYMHceTIEdSuXVtqq1OnDlasWJFrckQiIqLiQl9fgXPnHkrPfXxqYv36rrC1tZAxFclN4x4hlUoFQ8PcVbOhoaE0vxAREVFxY2ioj6Cgnihf3gzLl3fGvn39WASR5oVQ27ZtMW7cOMTF/T2+5tGjR/D390e7du20Go6IiKig4uLSEBPzRK2tZk0b3Ls3Dp9/3vSNk/eSbtG4EFq+fDnS0tLg5OSE6tWro0aNGqhWrRrS0tLeeqsLIiKiohISEoMGDVaiV69f8OJFttoyc3MjmVJRcaTxGCEHBwdcvHgRoaGhuH79OoQQqFOnDtq3b//ujYmIiApRenoW/P0PITDwIgAgKSkDs2adUJstmuifNC6E7t27BycnJ3To0AEdOnQojExEREQaCw+Pg5/fbty4kSS19ehRC19/nf+7HpDu0fjUmLOzM1q2bInVq1cjOTm5MDIRERHlm1Kpwty5p+DhsU4qgszMDLF2bRfs2tUbNjZmMiek4kzjQig8PBweHh744YcfYG9vj27duuHXX39FZmZmYeQjIiJ6o9jYFLRtuxlTp/6OnJxXVy43aWKPqKiRGDq0MQdE0ztpXAg1btwY8+fPR2xsLA4cOICKFSti5MiRqFixIoYMGVIYGYmIiHJJS8uEu/sanDx5HwCgUADTpnnhzJkhqFnTRuZ0VFJoXAi9plAo0KZNGwQGBuLIkSNwdnbGpk2btJmNiIjojcqUMcb48c0BAFWrWuHEiUH44Ye2MDTU7AbgpNsKfNPVBw8eYNu2bQgODsaVK1fg4eGB5cuXazMbERHRW33zTQuoVAJffNEU1tYmcsehEkjjQmjNmjUICgrCmTNn8MEHH8DPzw979uyBk5NTIcQjIiICcnJUmD37BAwM9PDdd62ldn19PXz7bSsZk1FJp3EhNHv2bPTt2xdLlixBw4YNCyESERHR327fToaf326cP/8IenoKtG/vDA8PB7ljUSmhcSEUGxvLUfhERFTohBDYtOkSvvzyAJ4/zwLwakD0pUuPWQiR1uSrELp8+TLq1asHPT09XLly5a3rNmjQQCvBiIhIdyUnZ2DkyH3YuTNaaqtevSyCgnqiWbMqMiaj0iZfhVDDhg2RkJCAihUromHDhlAoFBBCSMtfP1coFFAqlYUWloiISr9jx+5iwIAQPHqUJrUNHdoIixd3goUF7xNG2pWvQuju3buoUKGC9DMREZG2ZWUp8d13v2P+/DC8/lu7bFkTBAZ2Qa9edeQNR6VWvgohR0dH6ef79+/D09MTBgbqm+bk5CAsLExtXSIiovxSqQQOHLglFUFt21bDpk3dUaWKpbzBqFTTeELFNm3a5HmPsZSUFLRp00YroYiISPeYmBggOLgXLC2NsWBBB4SGDmARRIVO46vGXo8F+rekpCSYm5trJRQREZV+iYnpSEvLRPXq5aS2evUq4v798ZwckYpMvguhnj17Ang1MHrQoEEwNjaWlimVSly+fBmenp7aT0hERKXOgQM3MWjQf2FvXwbnzg2FsfHfX0csgqgo5bsQsrKyAvCqR6hMmTIwNTWVlhkZGaF58+YYPny49hMSEVGpkZGRjW++OYJlyy4AeNUrNGfOKcyaxaEVJI98F0IbNmwAADg5OeGrr77iaTAiItLIpUsJ8PPbjWvXnkhtPj418fnnTWRMRbpO4zFCM2bMKIwcRERUSqlUAkuWnMPkyUeRlfVqrjkTEwMsWNABY8Y04d0KSFb5KoQaN26Mo0ePomzZsmjUqNFb/6e9ePGi1sIREVHJFheXhs8+24MjR+5Iba6utggO7oU6dSrImIzolXwVQt26dZMGR3fv3r0w8xARUSmRkvISDRuuwpMnL6S2iRM9MGdOW7XB0URyUoh/3itDB6SmpsLKygopi+xgOT5O7jhERKXat9/+jjlzTsHevgw2beqO9u2d5Y5EJZT0/Z2SAktL7c0vpXFJ/uDBAygUClSp8uqmdxcuXEBwcDDq1KmDESNGaC0YERGVfDNmtIZKJTBxogdsbMzkjkOUi8YzS/v6+uLYsWMAgISEBLRv3x4XLlzA1KlTMWvWLK0HJCKi4k+pVGHu3FNYtOisWruhoT5+/LEdiyAqtjQuhK5evYqmTZsCAH755RfUr18fYWFhCA4OxsaNG7Wdj4iIirnY2BS0bbsZU6f+jm++OYLIyHi5IxHlm8aFUHZ2tjRw+siRI+jatSsAoFatWoiP5//8RES6ZPv2q2jQYCVOnrwPAMjJUSEs7IHMqYjyT+NCqG7duli1ahVOnTqF0NBQdOrUCQAQFxcHGxsbrQckIqLiJzU1EwMHhqBfv11ISckEAFStaoUTJwbh88+bypyOKP80LoT+85//YPXq1fjwww/Rr18/uLq6AgB+++036ZQZERGVXmfOxMLVdRW2bLkstfn61selS6Pg5eUoYzIizWl81diHH36Ip0+fIjU1FWXLlpXaR4wYATMzDoYjIiqtsrOVmD37JObMOQWV6tXMK5aWxggI8IGfXwOZ0xEVTIFmtNLX10dOTg5Onz4NhUIBFxcXODk5aTkaEREVJ1lZSuzYcU0qglq2rIotW3rAycla3mBE70HjU2Pp6ekYMmQI7Ozs0KpVK3h5ecHe3h5Dhw7Fixcv3v0CRERUIpmbGyEoqCdMTQ3www9tcPz4ZyyCqMTTuBCaMGECTpw4gb179+LZs2d49uwZ/vvf/+LEiROYOHFiYWQkIiIZJCdn4MGDFLU2d3d73Ls3HtOmtYK+vsZfIUTFjsanxnbt2oWdO3fiww8/lNp8fHxgamqK3r17Y+XKldrMR0REMjh27C4GDAiBg4MVTp0aDAODv4ueihXNZUxGpF0al/MvXryAra1trvaKFSvy1BgRUQmXlaXEpEmhaNduMx49SsO5cw/xn/+cljsWUaHRuBDy8PDAjBkz8PLlS6ktIyMDM2fOhIeHh1bDERFR0YmJeYLmzddi/vwwvL4dd9u21fDZZw1lzUVUmDQ+NbZ48WJ06tQJVapUgaurKxQKBaKiomBiYoJDhw4VRkYiIipEQgisXh2BCRMOISMjBwBgaKiHH39shwkTPKCnp5A5IVHh0bgQql+/Pm7duoWgoCDExMRACIG+ffvCz88PpqamhZGRiIgKSWJiOoYN+w17996Q2mrXLo+goJ5o1MhOxmRERUOjQuj8+fP47bffkJ2djXbt2mHYsGGFlYuIiArZs2cv4eq6CgkJz6W2MWPcMX++N8zMDGVMRlR08j1GKCQkBC1atMCSJUuwZs0a+Pj4YPHixYUYjYiICpO1tQn69q0LAKhQwQx79/bDihUfsQginZLvQujHH3/EoEGDpLmDZs6ciR9++KEwsxERUSGbO7c9xo5tiitXRuPjj13kjkNU5BRCvL424O0sLS0RHh4OF5dXvyiZmZkwNzdHQkICypcvX6ghtSk1NRVWVlZIWWQHy/FxcschIioSKpXAkiXnYG5uhBEj3OSOQ6Qx6fs7JQWWlpZae918jxF6/vw5rK2tpefGxsYwNTVFampqiSqEiIh0TVxcGgYN2oPQ0DswMTGAl1dV1K5dQe5YRMWCRoOlDx06BCsrK+m5SqXC0aNHcfXqVamta9eu2ktHRETvJSQkBsOH70VSUgYA4OXLHISG3mEhRPT/8n1qTE/v3cOJFAoFlErle4cqTDw1RkS6ID09C/7+hxAYeFFqs7cvg02buqN9e2cZkxEVjOynxlQqldZ2SkREhSc8PA5+frtx40aS1NajRy0EBnaBjY2ZjMmIih+NJ1QkIqLiSalUYd68M5g+/Thycl798WpmZoilSzthyJBGUCg4QzTRv7EQIiIqJdLTs7F6dYRUBDVpYo+goJ6oWdNG5mRExZfGN10lIqLiydLSGFu29IChoR6mTfPCmTNDWAQRvQN7hIiISqjU1Ey8eJGNSpUspDYvL0fcvj0WDg5Wb9mSiF5jjxARUQl05kwsXF1Xwdd3F1Qq9Yt/WQQR5Z/GhdCDBw/w8OFD6fmFCxcwfvx4rFmzRqvBiIgot+xsJaZPP4ZWrTbi3r1nOHbsHhYtOit3LKISS+NCyNfXF8eOHQMAJCQkoEOHDrhw4QKmTp2KWbNmaT0gERG9cutWMry8NmD27JNSL1DLllXRq1cdmZMRlVwaF0JXr15F06ZNAQC//PIL6tWrh7CwMAQHB2Pjxo3azkdEpPOEENiwIRING67C+fOPAAD6+gr88EMbHD/+GZycrOUNSFSCaVwIZWdnw9jYGABw5MgR6ZYatWrVQnx8vMYBAgICUK1aNZiYmMDNzQ2nTp166/qZmZmYNm0aHB0dYWxsjOrVq2P9+vUa75eIqCRITs5A7947MWTIb0hPzwYAVK9eFmFhQzFtWivo63OoJ9H70Piqsbp162LVqlX46KOPEBoaitmzZwMA4uLiYGOj2WWaO3bswPjx4xEQEIAWLVpg9erV6Ny5M6Kjo1G1atU8t+nduzceP36MdevWoUaNGkhMTEROTo6mb4OIqNj7668MuLquwsOHqVLb0KGNsHhxJ1hYGMmYjKj0yPe9xl47fvw4evTogdTUVHz22WdSb8zUqVNx/fp17N69O9+v1axZMzRu3BgrV66U2mrXro3u3btj7ty5udY/ePAg+vbtizt37qBcuXKaxJbwXmNEVJKMHLkXa9ZcRNmyJggM7MLxQKSzZL/X2Gsffvghnj59itTUVJQtW1ZqHzFiBMzM8n8Pm6ysLERERGDy5Mlq7d7e3ggLC8tzm99++w3u7u6YN28etmzZAnNzc3Tt2hWzZ8+Gqamppm+FiKjY+/nnjlAqBb7//kNUqaK9f/yJ6JUCTaior6+PnJwcnD59GgqFAi4uLnByctLoNZ4+fQqlUglbW1u1dltbWyQkJOS5zZ07d3D69GmYmJggJCQET58+xZgxY5CcnPzGcUKZmZnIzMyUnqempua5HhGRnIQQWL06AhYWRujfv4HUbm5uhLVru8qYjKh003iUXXp6OoYMGQI7Ozu0atUKXl5esLe3x9ChQ/HixQuNA/z7JoBCiDfeGFClUkGhUCAoKAhNmzaFj48Pfv75Z2zcuBEZGRl5bjN37lxYWVlJDwcHB40zEhEVpsTEdHTrth2jR/8Po0f/D7dvJ8sdiUhnaFwITZgwASdOnMDevXvx7NkzPHv2DP/9739x4sQJTJw4Md+vU758eejr6+fq/UlMTMzVS/SanZ0dKleuDCurv2dNrV27NoQQapM8/tOUKVOQkpIiPR48eJDvjEREhe3AgZto0GAl9u69AQB4/jwL+/bdkDkVke7QuBDatWsX1q1bh86dO8PS0hKWlpbw8fFBYGAgdu7cme/XMTIygpubG0JDQ9XaQ0ND4enpmec2LVq0QFxcHJ4/fy613bhxA3p6eqhSpUqe2xgbG0s5Xz+IiOSWkZGNsWMPwMcnGI8fpwMAKlQww969/TBuXHOZ0xHpDo0LoRcvXuTZY1OxYkWNT41NmDABa9euxfr16xETEwN/f3/ExsZi1KhRAF715gwcOFBa39fXFzY2Nhg8eDCio6Nx8uRJfP311xgyZAgHSxNRiXH58mM0aRKIZcsuSG0+PjVx5cpofPyxi4zJiHSPxoOlPTw8MGPGDGzevBkmJiYAgIyMDMycORMeHh4avVafPn2QlJSEWbNmIT4+HvXq1cP+/fvh6OgIAIiPj0dsbKy0voWFBUJDQ/Hll1/C3d0dNjY26N27N3744QdN3wYRUZFTqQSWLDmHyZOPIitLCQAwMTHAggUdMGZMkzeOjySiwqPxPEJXrlxB586d8fLlS7i6ukKhUCAqKgomJiY4dOgQ6tatW1hZtYLzCBGRXP76KwN16wYgPv7V6f0GDWwRHNwTdetWlDkZUfFXbOYRql+/Pm7evImtW7fi+vXrEEKgb9++8PPz4+kpIqK3KFvWFJs2dUenTkHw92+OOXPawti4QLOYEJGWaPQbmJ2djQ8++AD79u3D8OHDCysTEVGpkJ6ehZcvc2Bj8/dksx06VMeff36BGjUKNjs+EWmXRoOlDQ0NkZmZyfPYRETvEB4eh8aN12DgwD349wgEFkFExYfGV419+eWX+M9//sMbnRIR5UGpVGHu3FPw8FiHGzeSsH//TaxcGS53LCJ6A41PTp8/fx5Hjx7F4cOHUb9+fZibm6st1+Smq0REpUlsbAoGDAjByZP3pbYmTezRoYOzjKmI6G00LoSsra3Rq1evwshCRFRibd9+FaNG7UNKyqt7G+rpKTBlSkvMmNEahob6MqcjojfRuBDasGFDYeQgIiqRUlMz8cUX+7Fly2WprWpVK2zd2gNeXo4yJiOi/CjQdZs5OTk4fvw4bt++DV9fX5QpUwZxcXGwtLSEhYWFtjMSERVLSUkv0KRJIO7efSa1+frWx4oVPrC2NpEvGBHlm8aF0P3799GpUyfExsYiMzMTHTp0QJkyZTBv3jy8fPkSq1atKoycRETFjo2NGVq0qIq7d5/B0tIYAQE+8PNrIHcsItKAxoXQuHHj4O7ujkuXLsHGxkZq79GjB4YNG6bVcERExd3y5Z2hVKrw44/t4ORkLXccItKQxoXQ6dOncebMGRgZGam1Ozo64tGjR1oLRkRUnAghsGnTJVhaGqNnz9pSu5WVCYKDeQEJUUmlcSGkUqmgVCpztT98+BBlypTRSigiouIkOTkDI0fuw86d0bC2NkGTJvZwcLCSOxYRaYHGEyp26NABixcvlp4rFAo8f/4cM2bMgI+PjzazERHJ7tixu2jQYCV27owGADx79lL6mYhKPo17hBYtWoQ2bdqgTp06ePnyJXx9fXHz5k2UL18e27ZtK4yMRERFLitLiW+//R0LFoTh9R0yypY1QWBgF/TqVUfecESkNRoXQvb29oiKisK2bdtw8eJFqFQqDB06lHefJ6JS4/r1p/D13YXIyASprW3bati0qTuqVLGUMRkRaZtC/PtugKVcamoqrKyskLLIDpbj4+SOQ0TFiBACq1dHYMKEQ8jIeHU/RUNDPcyd2w7+/h7Q0+MNp4nkIn1/p6TA0lJ7f5Dkq0fot99+y/cLdu3atcBhiIjklJycge++OyYVQbVrl0dwcC80bFhJ5mREVFjyVQh1795d7blCocC/O5IUild/KeV1RRkRUUlgY2OGtWu7oHv3HRgzxh3z53vDzMxQ7lhEVIjyddWYSqWSHocPH0bDhg1x4MABPHv2DCkpKThw4AAaN26MgwcPFnZeIiKtycjIRkrKS7W2bt1q4fLlUVix4iMWQUQ6QOPB0uPHj8eqVavQsmVLqa1jx44wMzPDiBEjEBMTo9WARESF4fLlx/D13YXatSvgl18+kXq1AaB+fVsZkxFRUdJ4HqHbt2/Dyir3RGJWVla4d++eNjIRERUalUpg0aKzaNIkENeuPcHOndHYtOmS3LGISCYaF0JNmjTB+PHjER8fL7UlJCRg4sSJaNq0qVbDERFpU1xcGjp12ooJEw4jK+vVeEZXV1s0bVpZ5mREJBeNC6H169cjMTERjo6OqFGjBmrUqIGqVasiPj4e69atK4yMRETvLSQkBg0arERo6B2pbeJED5w/Pwx16lSQMRkRyUnjMUI1atTA5cuXERoaiuvXr0MIgTp16qB9+/Zq59iJiIqD9PQs+PsfQmDgRanN3r4MNm3qjvbtnWVMRkTFgcaFEPDqUnlvb294e3trOw8RkdY8eZKOli034MaNJKmtR49aCAzsAhsbMxmTEVFxUaBCKD09HSdOnEBsbCyysrLUlo0dO1YrwYiI3lf58maoW7cCbtxIgpmZIZYu7YQhQxqx95qIJBoXQpGRkfDx8cGLFy+Qnp6OcuXK4enTpzAzM0PFihVZCBFRsaFQKBAY2AVKpcCCBR1Qs6aN3JGIqJjReLC0v78/unTpguTkZJiamuLcuXO4f/8+3NzcsGDBgsLISESUL9u3X8WBAzfV2mxszPDf//ZlEUREedK4EIqKisLEiROhr68PfX19ZGZmwsHBAfPmzcPUqVMLIyMR0VulpmZi4MAQ9Ou3C599tgePHz+XOxIRlRAaF0KGhobS+XVbW1vExsYCeDWh4uufiYiKypkzsXB1XYUtWy4DAJ48eYGgoCsypyKikkLjMUKNGjVCeHg4XFxc0KZNG0yfPh1Pnz7Fli1bUL9+/cLISESUS3a2ErNnn8ScOaegUr26CbSlpTECAnzg59dA5nREVFJo3CP0448/ws7ODgAwe/Zs2NjYYPTo0UhMTMSaNWu0HpCI6N9u3UqGl9cGzJ59UiqCWrasikuXRrEIIiKNaNwj5O7uLv1coUIF7N+/X6uBiIjeRAiBjRuj8OWXB5Ceng0A0NdXYObMDzF5ckvo62v8tx0R6bgCzSNERCSHJ09ewN//kFQEVa9eFkFBPdGsWRWZkxFRSZWvQqhRo/xPQHbx4sV3r0REVAAVK5pj1aqP0a/fLgwd2giLF3eChYWR3LGIqATLVyHUvXt36eeXL18iICAAderUgYeHBwDg3LlzuHbtGsaMGVMoIYlIN2VlKZGdrYS5+d/FTt++9eDsXJZ3jCcirchXITRjxgzp52HDhmHs2LGYPXt2rnUePHig3XREpLOuX38KP7/dqF+/IjZu7K62jEUQEWmLxiMLf/31VwwcODBXe//+/bFr1y6thCIi3SWEwKpV4WjceDUuXozHpk2X8Msv1+SORUSllMaFkKmpKU6fPp2r/fTp0zAxMdFKKCLSTU+epKNbt+0YPfp/yMjIAQDUrl0eNWuWkzkZEZVWGl81Nn78eIwePRoRERFo3rw5gFdjhNavX4/p06drPSAR6YaDB29h0KA9ePw4XWobM8Yd8+d7w8zMUMZkRFSaaVwITZ48Gc7OzliyZAmCg4MBALVr18bGjRvRu3dvrQckotItIyMbkycfwdKlF6S2ChXMsH59N3z8sYuMyYhIF2hUCOXk5GDOnDkYMmQIix4iem+Jielo124zrl5NlNp8fGpi/fqusLW1kDEZEekKjcYIGRgYYP78+VAqlYWVh4h0SPnyZqhcuQwAwMTEAMuXd8a+ff1YBBFRkdF4sHT79u1x/PjxQohCRLpGT0+BDRu6oX17Z0REjMDnnzfN9+StRETaoPEYoc6dO2PKlCm4evUq3NzcYG5urra8a9euWgtHRKXLnj3XYW1tgg8/dJLa7OzKIDR0gHyhiEinKYQQQpMN9PTe3ImkUCiK/Wmz1NRUWFlZIWWRHSzHx8kdh0gnpKdnwd//EAIDL6Jy5TK4fHk0ypUzlTsWEZUg0vd3SgosLS219roanxpTqVRvfBT3IoiIil54eBwaN16DwMBX9yF89CgNGzdGyRuKiOj/aVwI/dPLly+1lYOIShmlUoW5c0/Bw2MdbtxIAgCYmRli7dou8PdvLnM6IqJXNC6ElEolZs+ejcqVK8PCwgJ37twBAHz33XdYt26d1gMSUckTG5uCtm03Y+rU35GTowIAuLvbIzJyJIYObcwB0URUbGhcCM2ZMwcbN27EvHnzYGT09x2h69evj7Vr12o1HBGVPNu3X0WDBitx8uR9AIBCAUyb5oWwsCFwcbGROR0RkTqNC6HNmzdjzZo18PPzg76+vtTeoEEDXL9+XavhiKhkSUh4jmHDfkNKSiYAoGpVK5w4MQg//NAWhob679iaiKjoaVwIPXr0CDVq1MjVrlKpkJ2drZVQRFQyVapkgSVLOgEA+vWrh0uXRsHLy1HmVEREb6bxPEJ169bFqVOn4Oio/o/br7/+ikaNGmktGBEVf9nZSiiVAiYmf/9TMmRIIzg7l0WbNtVkTEZElD8aF0IzZszAgAED8OjRI6hUKuzevRt//vknNm/ejH379hVGRiIqhm7dSkb//rvh5maHFSs+ktoVCgWLICIqMfJ9auzJkycAgC5dumDHjh3Yv38/FAoFpk+fjpiYGOzduxcdOnQotKBEVDwIIbBhQyQaNlyF8+cfISAgHPv23ZA7FhFRgeS7R6hy5cro2rUrhg4dik6dOqFjx46FmYuIiqHk5AyMHLkPO3dGS23Vq5dFxYrmb9mKiKj4yneP0KZNm5CamoouXbrAwcEB3333nTSHEBGVfseO3UWDBivViqChQxshKmoUmjatLGMyIqKCy3ch1K9fPxw+fBh3797F8OHDERQUhJo1a6JNmzYICgriLNNEpVRWlhKTJoWiXbvNePQoDQBQtqwJdu78FGvXdoWFhdE7XoGIqPjS+PJ5BwcHzJgxA3fu3MHhw4dRuXJljBgxAnZ2dhgzZkxhZCQimSQmpqN587WYPz8Mr2/P3K5dNVy5Mhq9etWRNxwRkRa8173G2rVrh61bt2Lz5s3Q09PD6tWrtZWLiIoBGxtTlCljDAAwNNTDggUdcPjwAFSurL07PxMRyanAhdC9e/cwY8YMODk5oU+fPmjcuDGCgoI0fp2AgABUq1YNJiYmcHNzw6lTp/K13ZkzZ2BgYICGDRtqvE8iyh99fT1s2dIDnp4OuHBhOCZO9ISeHu8TRkSlh0bzCL18+RK//vorNmzYgJMnT6Jy5coYNGgQBg8eDCcnJ413vmPHDowfPx4BAQFo0aIFVq9ejc6dOyM6OhpVq1Z943YpKSkYOHAg2rVrh8ePH2u8XyLK24EDN1G2rCmaN68itVWtaoXTpwfzRqlEVCophHh95v/tRowYgV9++QUvX75Et27dMGTIEHh7e7/XP47NmjVD48aNsXLlSqmtdu3a6N69O+bOnfvG7fr27YuaNWtCX18fe/bsQVRUVL73mZqaCisrK6QssoPl+LgCZycqTTIysvHNN0ewbNkFVKtmjaioUbC0NJY7FhGRRPr+TkmBpaX2Ts/n+9TYuXPnMHPmTMTFxWHHjh3o2LHjexVBWVlZiIiIgLe3t1q7t7c3wsLC3rjdhg0bcPv2bcyYMSNf+8nMzERqaqrag4j+dulSApo0CcSyZRcAAHfvPsO6dRdlTkVEVDTyfWrs8uXLWt3x06dPoVQqYWtrq9Zua2uLhISEPLe5efMmJk+ejFOnTsHAIH/R586di5kzZ753XqLSRqUSWLLkHCZPPoqsLCUAwMTEAAsXemP0aHeZ0xERFY33umpMG/7dqySEyLOnSalUwtfXFzNnzoSLi0u+X3/KlClISUmRHg8ePHjvzEQlXVxcGjp12ooJEw5LRZCrqy0iIkZgzJgmHA9ERDpD45uuakv58uWhr6+fq/cnMTExVy8RAKSlpSE8PByRkZH44osvAAAqlQpCCBgYGODw4cNo27Ztru2MjY1hbMyxDkSvhYTEYPjwvUhKypDaJk70wJw5bWFsLNs/CUREspDtXz0jIyO4ubkhNDQUPXr0kNpDQ0PRrVu3XOtbWlriypUram0BAQH4/fffsXPnTlSrxrtdE71LXFwa+vXbhczMV71A9vZlsGlTd7Rv7yxzMiIiecj659+ECRMwYMAAuLu7w8PDA2vWrEFsbCxGjRoF4NVprUePHkkTNtarV09t+4oVK8LExCRXOxHlzd6+DObP74CxYw+iR49aCAzsAhsbM7ljERHJpkCF0KlTp7B69Wrcvn0bO3fuROXKlbFlyxZUq1YNLVu2zPfr9OnTB0lJSZg1axbi4+NRr1497N+/H46OjgCA+Ph4xMbGFiQiEQFQKlVQqQQMDfWlti++aApn57Lw8anJsUBEpPPyPY/Qa7t27cKAAQPg5+eHLVu2IDo6Gs7OzggICMC+ffuwf//+wsqqFZxHiHRFbGwKBgwIQbNmlTFvXge54xARvRfZ5xF67YcffsCqVasQGBgIQ0NDqd3T0xMXL3LuEaLiYPv2q2jQYCVOnryP+fPDcPToHbkjEREVSxqfGvvzzz/RqlWrXO2WlpZ49uyZNjIRUQGlpmbiiy/2Y8uWv+f9qlrVCiYmvBqMiCgvGv/raGdnh1u3buW6t9jp06fh7MwrT4jkcuZMLPr3D8G9e8+kNl/f+lixwgfW1ibyBSMiKsY0PjU2cuRIjBs3DufPn4dCoUBcXByCgoLw1VdfYcyYMYWRkYjeIjtbienTj6FVq41SEWRpaYytW3sgKKgniyAiorfQuEdo0qRJSElJQZs2bfDy5Uu0atUKxsbG+Oqrr6SJDomoaCQmpqNr1204f/6R1NayZVVs2dIDTk7W8gUjIiohCjRwYM6cOZg2bRqio6OhUqlQp04dWFhYaDsbEb1D2bImeH3dp76+AjNnfojJk1tCX1/2u+cQEZUIBR5BaWZmBnd33piRSE6GhvoICuqJTz/9FatXf4ymTSvLHYmIqETJVyHUs2fPfL/g7t27CxyGiN7u2LG7KFvWFA0bVpLaatQoh4sXR3ByRCKiAshX/7mVlZX0sLS0xNGjRxEeHi4tj4iIwNGjR2FlZVVoQYl0WVaWEpMmhaJdu83o128XXrzIVlvOIoiIqGDy1SO0YcMG6edvvvkGvXv3xqpVq6Cv/2rafqVSiTFjxmh1pkcieuX69afw9d2FyMgE6XlgYATGjWsuczIiopJP41tsVKhQAadPn8YHH3yg1v7nn3/C09MTSUlJWg2obbzFBpUUQgisXh2BCRMOISMjBwBgaKiHuXPbwd/fA3p67AUiIt1RWLfY0HiwdE5ODmJiYnIVQjExMVCpVFoLRqTLEhPTMWzYb9i794bUVrt2eQQH91IbH0RERO9H40Jo8ODBGDJkCG7duoXmzV91zZ87dw4//fQTBg8erPWARLrmwIGbGDz4v3j8OF1qGzPGHfPne8PMzPAtWxIRkaY0LoQWLFiASpUqYdGiRYiPjwfw6rYbkyZNwsSJE7UekEiXPHyYim7dtiM7+1XvaoUKZli/vhs+/thF5mRERKWTxmOE/ik1NRUAStQgaY4RouLup59OY8qUo+jcuQY2bOgGW1tOVkpEVGzGCP1TSSqAiIojlUpACKE2E/TXX3uievWy+OSTOrwsnoiokHEefiKZxMWloVOnrZg9+6Rau76+Hj79tC6LICKiIvBePUJEVDAhITEYPnwvkpIycPToXXh7V4enp4PcsYiIdA4LIaIilJ6eBX//QwgMvCi12dqaIztbKWMqIiLdxUKIqIiEh8fBz283btz4e9LRHj1qITCwC2xszGRMRkSkuwpUCKWnp+PEiROIjY1FVlaW2rKxY8dqJRhRaaFUqjBv3hlMn34cOTmvLos3MzPE0qWdMGRII44FIiKSkcaFUGRkJHx8fPDixQukp6ejXLlyePr0KczMzFCxYkUWQkT/kJiYjk8//RUnT96X2po0sUdQUE/UrGkjYzIiIgIKcNWYv78/unTpguTkZJiamuLcuXO4f/8+3NzcsGDBgsLISFRiWVoa49mzlwAAhQKYNs0LZ84MYRFERFRMaFwIRUVFYeLEidDX14e+vj4yMzPh4OCAefPmYerUqYWRkajEMjExQHBwT3zwgQ1OnBiEH35oC0NDfbljERHR/9O4EDI0NJTGNNja2iI2NhYAYGVlJf1MpKvOnIlFdPQTtba6dSvi2rUx8PJylCkVERG9icZjhBo1aoTw8HC4uLigTZs2mD59Op4+fYotW7agfv36hZGRqNjLzlZi9uyTmDPnFOrXr4jz54fB2PjvX69/zhxNRETFh8b/Ov/444+ws7MDAMyePRs2NjYYPXo0EhMTsWbNGq0HJCrubt9OhpfXBsyefRIqlcClS4+xZk2E3LGIiCgfNO4Rcnd3l36uUKEC9u/fr9VARCWFEAKbNl3Cl18ewPPnr6aR0NdXYObMDzFmTBN5wxERUb5oXAhlZGRACAEzs1cTwN2/fx8hISGoU6cOvL29tR6QqDhKTs7AyJH7sHNntNRWvXpZBAf3QtOmlWVMRkREmtD41Fi3bt2wefNmAMCzZ8/QtGlTLFy4EN26dcPKlSu1HpCouPn997to0GClWhE0dGgjREWNYhFERFTCaFwIXbx4EV5eXgCAnTt3olKlSrh//z42b96MpUuXaj0gUXESG5uCjh234tGjNABA2bIm2LnzU6xd2xUWFkYypyMiIk1pXAi9ePECZcqUAQAcPnwYPXv2hJ6eHpo3b4779++/Y2uikq1qVStMmdISANC2bTVcvjwavXrVkTkVEREVlMaFUI0aNbBnzx48ePAAhw4dksYFJSYmwtLSUusBieQkhIBKJdTavvuuFTZu7IbQ0AGoUoX/zxMRlWQaF0LTp0/HV199BScnJzRr1gweHh4AXvUONWrUSOsBieSSmJiObt22Y+HCMLV2Q0N9fPZZQ+jp8WapREQlnUIIId69mrqEhATEx8fD1dUVenqvaqkLFy7A0tIStWrV0npIbUpNTYWVlRVSFtnBcnyc3HGomDpw4CYGD/4vHj9Oh6GhHs6dG4bGje3kjkVEpLOk7++UFK2egdL48nkAqFSpEipVqqTW1rRpU60EIpJTRkY2vvnmCJYtuyC1WVub4K+/MmRMRUREhaVAhdAff/yBX3/9FbGxscjKylJbtnv3bq0EIypqly4lwM9vN65d+/teYZ0718CGDd1ga2shYzIiIiosGo8R2r59O1q0aIHo6GiEhIQgOzsb0dHR+P3332FlZVUYGYkKlUolsGjRWTRtulYqgkxMDLBsWWf873++LIKIiEoxjXuEfvzxRyxatAiff/45ypQpgyVLlqBatWoYOXKkdA8yopLiyZN0+PruxpEjd6S2Bg1sERzcE3XrVpQxGRERFQWNe4Ru376Njz76CABgbGyM9PR0KBQK+Pv786arVOKYmRkiNjZFej5xogcuXBjGIoiISEdoXAiVK1cOaWmvZtWtXLkyrl69CuDV7TZevHih3XREhczc3AjBwT3h5GSN0NABWLDAG8bGBRo6R0REJVC+C6EhQ4YgLS0NXl5eCA0NBQD07t0b48aNw/Dhw9GvXz+0a9eu0IISaUN4eBxu305Wa3Nzs8eNG1+gfXtnmVIREZFc8j2PkL6+PuLj42FgYICXL1/C3t4eKpUKCxYswOnTp1GjRg189913KFu2bGFnfi+cR0g3KZUqzJt3BtOnH4ebmx1OnRoMQ0N9uWMREVE+FdY8QvkuhPT09JCQkICKFUv22AkWQronNjYFAwaE4OTJv++FFxDgg9Gjm8iYioiINFEsJlRUKHhLASpZtm+/ilGj9iElJRMAoFAAU6d6YdiwxjInIyKi4kCjQsjFxeWdxVBycvJblxMVhdTUTHzxxX5s2XJZaqta1Qpbt/aAl5ejjMmIiKg40agQmjlzJidNpGIvLOwB+vffjbt3n0ltvr71sWKFD6ytTeQLRkRExY5GhVDfvn1L/BghKt3u3XuG1q03IidHBQCwtDRGQIAP/PwayJyMiIiKo3xfPs/xQVQSODlZ48svX90AuEULB1y6NIpFEBERvVG+e4TyeXEZUZF6/f/lPwv1H39shxo1ymHECDcYGGg8ZygREemQfH9LqFQqnhajYiU5OQO9e+9EQMAfau0mJgYYM6YJiyAiInon3kuASqRjx+5iwIAQPHqUhn37buDDD514fzAiItIY/2SmEiUrS4lJk0LRrt1mPHr06p53pqYG0s9ERESaYI8QlRgxMU/g57cbkZEJUlvbttWwaVN3VKmivVlGiYhId7AQomJPCIFVq8IxceJhZGTkAAAMDfUwd247+Pt7QE+PVzQSEVHBsBCiYi0p6QUGDfov9u27IbXVrl0eQUE90aiRnYzJiIioNOAYISrWDAz0cOXKY+n5mDHuCA8fwSKIiIi0goUQFWtWVibYurUn7OwssHdvP6xY8RHMzAzljkVERKUET41RsXLpUgLKlTOFg8Pf97Rr2bIq7twZBxMT/u9KRETaJXuPUEBAAKpVqwYTExO4ubnh1KlTb1x39+7d6NChAypUqABLS0t4eHjg0KFDRZiWCotKJbBo0Vk0bboWAwaEQKlUqS1nEURERIVB1kJox44dGD9+PKZNm4bIyEh4eXmhc+fOiI2NzXP9kydPokOHDti/fz8iIiLQpk0bdOnSBZGRkUWcnLQpLi4NnTptxYQJh5GVpcSJE/exfj2PKRERFT6FkPEmYs2aNUPjxo2xcuVKqa127dro3r075s6dm6/XqFu3Lvr06YPp06fna/3U1FRYWVkhZZEdLMfHFSg3aU9ISAyGD9+LpKQMqW3iRA/MmdMWxsbsBSIiolek7++UFFhaam/uONm+abKyshAREYHJkyertXt7eyMsLCxfr6FSqZCWloZy5cq9cZ3MzExkZmZKz1NTUwsWmLQqPT0L/v6HEBh4UWqzty+DTZu6o317ZxmTERGRLpHt1NjTp0+hVCpha2ur1m5ra4uEhIQ3bKVu4cKFSE9PR+/evd+4zty5c2FlZSU9HBwc3is3vb/w8Dg0brxGrQjq2bM2Ll8exSKIiIiKlOyDpRUK9VmBhRC52vKybds2fP/999ixYwcqVnzzzTanTJmClJQU6fHgwYP3zkwFd+fOX/DwWIcbN5IAAObmhli3rit27vwUNjZmMqcjIiJdI1shVL58eejr6+fq/UlMTMzVS/RvO3bswNChQ/HLL7+gffv2b13X2NgYlpaWag+Sj7NzWQwd2ggA0KSJPSIjR2LIkEb5Kn6JiIi0TbZCyMjICG5ubggNDVVrDw0Nhaen5xu327ZtGwYNGoTg4GB89NFHhR2TCsHChd5YsKADzpwZgpo1beSOQ0REOkzWU2MTJkzA2rVrsX79esTExMDf3x+xsbEYNWoUgFentQYOHCitv23bNgwcOBALFy5E8+bNkZCQgISEBKSkpMj1FugtUlMzMXBgCDZsUL8U3tzcCBMnesLQUF+mZERERK/Ien1ynz59kJSUhFmzZiE+Ph716tXD/v374ejoCACIj49Xm1No9erVyMnJweeff47PP/9cav/ss8+wcePGoo5PbxEW9gD9++/G3bvPEBJyHV5ejqhR481X9xEREclB1nmE5MB5hApXTo4Ks2efwA8/nIJK9ep/LUtLY+zY8Qk6daohczoiIiqpSt08QlT63L6dDD+/3Th//pHU1rJlVWzZ0gNOTtbyBSMiInoDFkL03oQQ2LTpEr788gCeP88CAOjrKzBz5oeYPLkl9PVln6WBiIgoTyyE6L389VcGRozYh507o6W26tXLIji4F5o2rSxjMiIiondjIUTvRaUSCAv7e5LKoUMbYfHiTrCwMJIxFRERUf7wnAW9FxsbM2za1B02NqbYufNTrF3blUUQERGVGOwRIo3ExDxBuXKmsLW1kNrat3fG3bvjUKaMsYzJiIiINMceIcoXIQRWrQqHm9saDB78X/x71gUWQUREVBKxEKJ3SkxMR7du2zF69P+QkZGDAwduYdOmS3LHIiIiem88NUZvdfDgLQwatAePH6dLbWPGuKN377oypiIiItIOFkKUp4yMbEyefARLl16Q2ipUMMP69d3w8ccuMiYjIiLSHhZClMuVK4/h67sbV68mSm0+PjWxfn1XtUHSREREJR0LIVJz61Yy3N0DkZWlBACYmBhgwYIOGDOmCRQKhczpiIiItIuDpUlNjRrl0KfPq/E/rq62iIgYgc8/b8oiiIiISiX2CFEuy5f7oGbNcpg0qQWMjfm/CBERlV7sEdJh6elZGDFiL3bsuKrWbmlpjO++a80iiIiISj1+0+mo8PA4+Pntxo0bSfj112h4ejrAwcFK7lhERERFij1COkapVGHu3FPw8FiHGzeSAABZWUpcvvxY5mRERERFjz1COiQ2NgUDBoTg5Mn7UluTJvYICuqJmjVtZExGREQkDxZCOmL79qsYNWofUlIyAQAKBTB1qhdmzGgNQ0N9mdMRERHJg4VQKZeamokvvtiPLVsuS21Vq1ph69Ye8PJylDEZERGR/FgIlXIvXmTjwIFb0vN+/eohIOAjWFubyJiKiIioeOBg6VKuUiULrFvXFZaWxti6tQeCg3uxCCIiIvp/7BEqZW7dSkbZsiawsTGT2rp2/QB3745DuXKmMiYjIiIqftgjVEoIIbBhQyQaNlyFkSP3QQihtpxFEBERUW4shEqB5OQM9O69E0OG/Ib09Gzs2hWDbduuvntDIiIiHcdTYyXcsWN3MWBACB49SpPahg5thK5dP5AxFRERUcnAQqiEyspS4ttvf8eCBWF4fRasbFkTBAZ2Qa9edeQNR0REVEKwECqBrl9/Cl/fXYiMTJDa2rathk2buqNKFUsZkxEREZUsLIRKmD//fIrGjVcjIyMHAGBoqIe5c9vB398DenoKmdMRERGVLBwsXcK4uNigc+eaAIDatcvjwoXhmDjRk0UQERFRAbBHqIRRKBRYs+ZjuLiUw3fftYaZmaHckYiIiEosFkLFWEZGNr755gg6dHBGly5/XwVmY2OGuXPby5iMSHcIIZCTkwOlUil3FKJSz9DQEPr6RXsjcBZCxdSlSwnw89uNa9eeYNu2q7hyZTQqVbKQOxaRTsnKykJ8fDxevHghdxQinaBQKFClShVYWBTd9x0LoWJGpRJYsuQcJk8+iqysV3+BPn+ehfDwOHz8sYvM6Yh0h0qlwt27d6Gvrw97e3sYGRlBoeBYPKLCIoTAkydP8PDhQ9SsWbPIeoZYCBUjcXFpGDRoD0JD70htrq62CA7uhTp1KsiYjEj3ZGVlQaVSwcHBAWZmZu/egIjeW4UKFXDv3j1kZ2ezENI1ISExGD58L5KSMqS2iRM9MGdOWxgb8zARyUVPjxfXEhUVOXpd+Q0rs+fPs+DvfxBr10ZKbfb2ZbBpU3e0b+8sYzIiIqLSj4WQzP76KwO//hotPe/RoxYCA7vAxoZd8URERIWNfb4yc3CwwurVH8Pc3BBr13bBrl29WQQRERUDrVq1QnBwsNwxSo3MzExUrVoVERERckdRw0KoiMXGpiA1NVOtrU+ferh1ayyGDm3Mq1KISCsSEhLw5ZdfwtnZGcbGxnBwcECXLl1w9OhRuaPl6d69e1AoFNLDysoKzZs3x969e3Otm5GRgRkzZuCDDz6AsbExypcvj08++QTXrl3LtW5qaiqmTZuGWrVqwcTEBJUqVUL79u2xe/duiNd3rM7Dvn37kJCQgL59++Za9uOPP0JfXx8//fRTrmXff/89GjZsmKv92bNnUCgUOH78uFr7rl278OGHH8LKygoWFhZo0KABZs2aheTk5Ddme19r1qzBhx9+CEtLSygUCjx79ixf2wUEBKBatWowMTGBm5sbTp06pbZcCIHvv/8e9vb2MDU1xYcffqh2TIyNjfHVV1/hm2++0ebbeW8shIrQ9u1X0aDBSnz55YFcyzhHEBFpy7179+Dm5obff/8d8+bNw5UrV3Dw4EG0adMGn3/+eYFf9/XkkoXpyJEjiI+Px/nz59G0aVP06tULV69elZZnZmaiffv2WL9+PWbPno0bN25g//79UCqVaNasGc6dOyet++zZM3h6emLz5s2YMmUKLl68iJMnT6JPnz6YNGkSUlJS3phj6dKlGDx4cJ6D5Tds2IBJkyZh/fr17/Vep02bhj59+qBJkyY4cOAArl69ioULF+LSpUvYsmXLe73227x48QKdOnXC1KlT873Njh07MH78eEybNg2RkZHw8vJC586dERsbK60zb948/Pzzz1i+fDn++OMPVKpUCR06dEBaWpq0jp+fH06dOoWYmBitvqf3InRMSkqKACBSFtkV4T5figEDdgvge+mxc+e1Its/EWkuIyNDREdHi4yMDLmjaKxz586icuXK4vnz57mW/fXXX0IIIe7evSsAiMjISLVlAMSxY8eEEEIcO3ZMABAHDx4Ubm5uwtDQUKxatUoAEDExMWqvu3DhQuHo6ChUKpXIyckRQ4YMEU5OTsLExES4uLiIxYsXvzVzXnlSU1MFALF06VKp7aeffhIKhUJERUWpba9UKoW7u7uoU6eOUKlUQgghRo8eLczNzcWjR49y7S8tLU1kZ2fnmeXJkydCoVCIq1ev5lp2/PhxUblyZZGVlSXs7e3FiRMn1JbPmDFDuLq65tru35/t+fPnBYA3fi6vj1Nhen1887Ovpk2bilGjRqm11apVS0yePFkIIYRKpRKVKlUSP/30k7T85cuXwsrKSqxatUptuw8//FB89913ee7nbb930vd3Sso782qCg6UL2ZkzsejfPwT37j2T2vr1q4d27XhFGFGJtNUdSE8o+v2aVwL6h79zteTkZBw8eBBz5syBubl5ruXW1tYa73rSpElYsGABnJ2dYW1tjcDAQAQFBWH27NnSOsHBwfD19YVCoYBKpUKVKlXwyy+/oHz58ggLC8OIESNgZ2eH3r1752uf2dnZCAwMBPDqtgv/3E+HDh3g6uqqtr6enh78/f3h5+eHS5cuoUGDBti+fTv8/Pxgb2+f6/XfNnPx6dOnYWZmhtq1a+datm7dOvTr1w+Ghobo168f1q1bh1atWuXrPf1TUFAQLCwsMGbMmDyXv+041a1bF/fv33/jckdHxzxPExZUVlYWIiIiMHnyZLV2b29vhIWFAQDu3r2LhIQEeHt7S8uNjY3RunVrhIWFYeTIkVJ706ZNc51WkxMLoUKSna3E7NknMWfOKahUr85DW1oaIyDAB35+DWROR0QFlp4APH8kd4o3unXrFoQQqFWrltZec9asWejQoYP03M/PD8uXL5cKoRs3biAiIgKbN28G8KpwmTlzprR+tWrVEBYWhl9++eWdhZCnpyf09PSQkZEBlUoFJycntW1u3LiBNm3a5Lnt68Llxo0bsLe3x19//VWgz+HevXuwtbXNdVosNTUVu3btkr78+/fvjxYtWmDZsmWwtLTUaB83b96Es7OzWpGXX/v370d2dvYblxfkNd/m6dOnUCqVsLW1VWu3tbVFQsKrPwpe/zevdf5dtFWuXBn37t3Tasb3wUKoENy6lYz+/Xfj/Pm//7Fs0cIBW7f2hJOTtXzBiOj9mVcq1vsV/z8AWJsXXri7u6s979u3L77++mucO3cOzZs3R1BQEBo2bIg6depI66xatQpr167F/fv3kZGRgaysrDwHEf/bjh07UKtWLdy4cQPjx4/HqlWrUK5cuXzl/Od7f5/PISMjAyYmJrnag4OD4ezsLPVGNWzYEM7Ozti+fTtGjBih0T6EEAU+Ro6OjgXa7n39O29e7yE/65iamhar+/exENKymJgnaNIkEOnpr6p1fX0Fvv/+Q0ye3BIGBhybTlTi5eP0lJxq1qwJhUKBmJgYdO/e/Y3rve7tEP+4cupNvQz/PsVmZ2eHNm3aIDg4GM2bN8e2bdvUTn388ssv8Pf3x8KFC+Hh4YEyZcpg/vz5OH/+/DvzOzg4oGbNmqhZsyYsLCzQq1cvREdHo2LFigAAFxcXREdH57nt9evXpc+gQoUKKFu2bIEG5ZYvXx5//fVXrvb169fj2rVrMDD4+6tTpVJh3bp1UiFkaWmZ5yDs11dmWVlZSe/j9OnTyM7O1rgHp6hPjZUvXx76+vpSr89riYmJUg9QpUqvCvWEhATY2dnluc5rycnJqFCh+Nw2it/MWlarVnl4eb2q1qtXL4szZ4bg229bsQgioiJRrlw5dOzYEStWrEB6enqu5a+/kF9/EcXHx0vLoqKi8r0fPz8/7NixA2fPnsXt27fVLjM/deoUPD09MWbMGDRq1Ag1atTA7du3NX4vrVu3Rr169TBnzhyprW/fvjhy5AguXbqktq5KpcKiRYtQp04duLq6Qk9PD3369EFQUBDi4uJyvXZ6evobr4Br1KgREhIS1IqhK1euIDw8HMePH0dUVJT0OHnyJP744w/pyrZatWrh4cOHuYqGP/74A3p6eqhRowYAwNfXF8+fP0dAQECeGd52Sfv+/fvVMvz7sX///jduWxBGRkZwc3NDaGioWntoaCg8PT0BvDr9WalSJbV1srKycOLECWmd165evYpGjRppNeN70erQ6xKgKK4ai49PE+PGHRBpaZmFtg8iKlwl+aqxO3fuiEqVKok6deqInTt3ihs3bojo6GixZMkSUatWLWm95s2bCy8vL3Ht2jVx4sQJ0bRp0zyvGsvrqqKUlBRhYmIiXF1dRbt27dSWLV68WFhaWoqDBw+KP//8U3z77bfC0tIyz6upXsvrqjEhhPjtt9+EsbGxePjwoRDi1XFp1qyZcHBwEL/88ou4f/++uHDhgujevbswNzcXZ8+elbZNTk4WtWrVElWqVBGbNm0S165dEzdu3BDr1q0TNWrUeOPVUjk5OaJixYpi7969Utu4ceNEs2bN8lzf09NTjB8/XgghRHZ2tqhfv75o3bq1OH36tLhz547Ys2ePqFq1qhgzZozadpMmTRL6+vri66+/FmFhYeLevXviyJEj4pNPPnnnVXbvIz4+XkRGRorAwEABQJw8eVJERkaKpKQkaZ22bduKZcuWSc+3b98uDA0Nxbp160R0dLQYP368MDc3F/fu3ZPW+emnn4SVlZXYvXu3uHLliujXr5+ws7MTqampavt3dHQUmzdvzjObHFeNsRB6D5mZOWLSpMMiNPS2FpIRUXFSkgshIYSIi4sTn3/+uXB0dBRGRkaicuXKomvXrlKRI4QQ0dHRonnz5sLU1FQ0bNhQHD58ON+FkBBCfPrppwKAWL9+vVr7y5cvxaBBg4SVlZWwtrYWo0ePFpMnTy5QIaRSqcQHH3wgRo8eLbWlp6eLb7/9VtSoUUMYGhqKcuXKiV69eokrV67ket1nz56JyZMni5o1awojIyNha2sr2rdvL0JCQqTL7PMyefJk0bdvXyGEEJmZmcLGxkbMmzcvz3UXLlwoypcvLzIzX/3xGx8fLwYPHiwcHR2FqampqFWrlpg1a5Z4+fJlrm137NghWrVqJcqUKSPMzc1FgwYNxKxZswr18vkZM2YIALkeGzZskNZxdHQUM2bMUNtuxYoV0v9PjRs3zjV1gEqlEjNmzBCVKlUSxsbGolWrVrmOSVhYmLC2thYvXrzIM5schZBCiLdMrVkKpaamwsrKCimL7GA5Pnd3aX5dv/4Uvr67EBmZAHv7Mrh8eRRvjUFUirx8+RJ3796VZtIl3fL48WPUrVsXERERsg1OLo0+/fRTNGrU6I2TOb7t9076/k5J0fgqvbfhwBUNCSGwalU4GjdejcjIV+eAnzxJR1jYA5mTERGRttja2mLdunVqMyfT+8nMzISrqyv8/f3ljqKGV41pIDExHcOG/Ya9e29IbbVrl0dwcC80bCjTJbVERFQounXrJneEUsXY2Bjffvut3DFyYSGUTwcP3sKgQXvw+PHfV2GMGeOO+fO9YWam3cmriIiIqGiwEHqHjIxsTJ58BEuXXpDaKlQww/r13fDxxy4yJiMiIqL3xULoHeLi0rBuXaT03MenJtav7wpbW94tnkgX6Nj1JESykuP3jYOl36F69XJYurQzTEwMsHx5Z+zb149FEJEOeD3bb3G6FQBRaZeVlQUA0NfXL7J9skfoX+Li0mBtbaI27mfw4IZo164aHB2t5QtGREVKX18f1tbWSExMBACYmZlp9f5dRKROpVLhyZMnMDMzU7uNSWFjIfQPISExGD58Lz79tA5WrvxYalcoFCyCiHTQ6/snvS6GiKhw6enpoWrVqkX6RwcLIQDPn2fB3/8g1q59NRZo1aoIfPSRCwdDE+k4hUIBOzs7VKxY8Y03JCUi7TEyMpJuCFxUZC+EAgICMH/+fMTHx6Nu3bpYvHgxvLy83rj+iRMnMGHCBFy7dg329vaYNGkSRo0aVeD9//HHI/j57cbNm8lSW48eteDhUaXAr0lEpYu+vn6RjlkgoqIj62DpHTt2YPz48Zg2bRoiIyPh5eWFzp07v3Emz7t378LHxwdeXl6IjIzE1KlTMXbsWOzatUvjfStVCsydewqenuulIsjMzBBr13bBrl29ebsMIiIiHSDrvcaaNWuGxo0bY+XKlVJb7dq10b17d8ydOzfX+t988w1+++03xMTESG2jRo3CpUuXcPbs2Xzt8/W9SjydByPszt/3j2nSxB5BQT1Rs6bNe7wjIiIiKgyl7l5jWVlZiIiIgLe3t1q7t7c3wsLC8tzm7Nmzudbv2LEjwsPDNT5/H3bHFgCgp6fAtGleOHNmCIsgIiIiHSPbGKGnT59CqVTC1tZWrd3W1hYJCQl5bpOQkJDn+jk5OXj69Cns7OxybZOZmYnMzEzpeUpKyuslqFLFCoGBH8PTsyoyMtKRkfF+74mIiIgKR2pqKgDtT7oo+2Dpf18iJ4R462Vzea2fV/trc+fOxcyZM/NYsggPHwKdO0/RLDARERHJJikpCVZWVlp7PdkKofLly0NfXz9X709iYmKuXp/XKlWqlOf6BgYGsLHJ+7TWlClTMGHCBOn5s2fP4OjoiNjYWK1+kFQwqampcHBwwIMHD7R6zpc0x2NRfPBYFB88FsVHSkoKqlatinLlymn1dWUrhIyMjODm5obQ0FD06NFDag8NDUW3bt3y3MbDwwN79+5Vazt8+DDc3d2l6fD/zdjYGMbGxrnarays+D91MWJpacnjUUzwWBQfPBbFB49F8aHteYZkvXx+woQJWLt2LdavX4+YmBj4+/sjNjZWmhdoypQpGDhwoLT+qFGjcP/+fUyYMAExMTFYv3491q1bh6+++kqut0BEREQlmKxjhPr06YOkpCTMmjUL8fHxqFevHvbv3w9Hx1eXtcfHx6vNKVStWjXs378f/v7+WLFiBezt7bF06VL06tVLrrdAREREJZjsg6XHjBmDMWPG5Lls48aNudpat26NixcvFnh/xsbGmDFjRp6ny6jo8XgUHzwWxQePRfHBY1F8FNaxkHVCRSIiIiI5yTpGiIiIiEhOLISIiIhIZ7EQIiIiIp3FQoiIiIh0VqkshAICAlCtWjWYmJjAzc0Np06deuv6J06cgJubG0xMTODs7IxVq1YVUdLST5NjsXv3bnTo0AEVKlSApaUlPDw8cOjQoSJMW/pp+rvx2pkzZ2BgYICGDRsWbkAdoumxyMzMxLRp0+Do6AhjY2NUr14d69evL6K0pZumxyIoKAiurq4wMzODnZ0dBg8ejKSkpCJKW3qdPHkSXbp0gb29PRQKBfbs2fPObbTy/S1Kme3btwtDQ0MRGBgooqOjxbhx44S5ubm4f/9+nuvfuXNHmJmZiXHjxono6GgRGBgoDA0Nxc6dO4s4eemj6bEYN26c+M9//iMuXLggbty4IaZMmSIMDQ3FxYsXizh56aTp8Xjt2bNnwtnZWXh7ewtXV9eiCVvKFeRYdO3aVTRr1kyEhoaKu3fvivPnz4szZ84UYerSSdNjcerUKaGnpyeWLFki7ty5I06dOiXq1q0runfvXsTJS5/9+/eLadOmiV27dgkAIiQk5K3ra+v7u9QVQk2bNhWjRo1Sa6tVq5aYPHlynutPmjRJ1KpVS61t5MiRonnz5oWWUVdoeizyUqdOHTFz5kxtR9NJBT0effr0Ed9++62YMWMGCyEt0fRYHDhwQFhZWYmkpKSiiKdTND0W8+fPF87OzmptS5cuFVWqVCm0jLooP4WQtr6/S9WpsaysLERERMDb21ut3dvbG2FhYXluc/bs2Vzrd+zYEeHh4cjOzi60rKVdQY7Fv6lUKqSlpWn9Bnu6qKDHY8OGDbh9+zZmzJhR2BF1RkGOxW+//QZ3d3fMmzcPlStXhouLC7766itkZGQUReRSqyDHwtPTEw8fPsT+/fshhMDjx4+xc+dOfPTRR0URmf5BW9/fss8srU1Pnz6FUqnMdfd6W1vbXHetfy0hISHP9XNycvD06VPY2dkVWt7SrCDH4t8WLlyI9PR09O7duzAi6pSCHI+bN29i8uTJOHXqFAwMStU/FbIqyLG4c+cOTp8+DRMTE4SEhODp06cYM2YMkpOTOU7oPRTkWHh6eiIoKAh9+vTBy5cvkZOTg65du2LZsmVFEZn+QVvf36WqR+g1hUKh9lwIkavtXevn1U6a0/RYvLZt2zZ8//332LFjBypWrFhY8XROfo+HUqmEr68vZs6cCRcXl6KKp1M0+d1QqVRQKBQICgpC06ZN4ePjg59//hkbN25kr5AWaHIsoqOjMXbsWEyfPh0RERE4ePAg7t69K90snIqWNr6/S9WfeeXLl4e+vn6uSj4xMTFX1fhapUqV8lzfwMAANjY2hZa1tCvIsXhtx44dGDp0KH799Ve0b9++MGPqDE2PR1paGsLDwxEZGYkvvvgCwKsvYyEEDAwMcPjwYbRt27ZIspc2BfndsLOzQ+XKlWFlZSW11a5dG0IIPHz4EDVr1izUzKVVQY7F3Llz0aJFC3z99dcAgAYNGsDc3BxeXl744YcfeBahCGnr+7tU9QgZGRnBzc0NoaGhau2hoaHw9PTMcxsPD49c6x8+fBju7u4wNDQstKylXUGOBfCqJ2jQoEEIDg7mOXct0vR4WFpa4sqVK4iKipIeo0aNwgcffICoqCg0a9asqKKXOgX53WjRogXi4uLw/Plzqe3GjRvQ09NDlSpVCjVvaVaQY/HixQvo6al/derr6wP4uzeCiobWvr81GlpdAry+FHLdunUiOjpajB8/Xpibm4t79+4JIYSYPHmyGDBggLT+68vv/P39RXR0tFi3bh0vn9cSTY9FcHCwMDAwECtWrBDx8fHS49mzZ3K9hVJF0+Pxb7xqTHs0PRZpaWmiSpUq4pNPPhHXrl0TJ06cEDVr1hTDhg2T6y2UGpoeiw0bNggDAwMREBAgbt++LU6fPi3c3d1F06ZN5XoLpUZaWpqIjIwUkZGRAoD4+eefRWRkpDSVQWF9f5e6QkgIIVasWCEcHR2FkZGRaNy4sThx4oS07LPPPhOtW7dWW//48eOiUaNGwsjISDg5OYmVK1cWceLSS5Nj0bp1awEg1+Ozzz4r+uCllKa/G//EQki7ND0WMTExon379sLU1FRUqVJFTJgwQbx48aKIU5dOmh6LpUuXijp16ghTU1NhZ2cn/Pz8xMOHD4s4delz7Nixt34HFNb3t0II9uURERGRbipVY4SIiIiINMFCiIiIiHQWCyEiIiLSWSyEiIiISGexECIiIiKdxUKIiIiIdBYLISIiItJZLISIKN8GDRqE7t27y7b/77//Hg0bNpRt/4XJyckJixcvfus6pfn9E8mFhRBRMaFQKN76GDRokNwRteLDDz/M8/3l5OTIHe2tNm7cqJbXzs4OvXv3xt27d7Xy+n/88QdGjBghPVcoFNizZ4/aOl999RWOHj2qlf0R0Sul6u7zRCVZfHy89POOHTswffp0/Pnnn1KbqampHLEKxfDhwzFr1iy1NgOD4v/PkaWlJf78808IIXD9+nWMHDkSXbt2RVRUlHTjzYKqUKHCO9exsLCAhYXFe+2HiNSxR4iomKhUqZL0sLKygkKhkJ4bGhpi1KhRqFKlCszMzFC/fn1s27ZNbfudO3eifv36MDU1hY2NDdq3b4/09HQAr3obOnTogPLly8PKygqtW7fGxYsX35pHqVRiwoQJsLa2ho2NDSZNmpTr7tpCCMybNw/Ozs4wNTWFq6srdu7c+c73amZmpvZ+K1WqBAD45ptv4OLiAjMzMzg7O+O7775Ddnb2G1/n+PHjaNq0KczNzWFtbY0WLVrg/v370vKVK1eievXqMDIywgcffIAtW7aobf/999+jatWqMDY2hr29PcaOHfvW3K+PiZ2dHdq0aYMZM2bg6tWruHXr1nvv75+nxpycnAAAPXr0gEKhkJ7/89TYoUOHYGJigmfPnqntY+zYsWjdurX0fNeuXahbty6MjY3h5OSEhQsXqq0fEBCAmjVrwsTEBLa2tvjkk0/e+hkQlTYshIhKgJcvX8LNzQ379u3D1atXMWLECAwYMADnz58H8Ko3qV+/fhgyZAhiYmJw/Phx9OzZUypc0tLS8Nlnn+HUqVM4d+4catasCR8fH6Slpb1xnwsXLsT69euxbt06nD59GsnJyQgJCVFb59tvv8WGDRuwcuVKXLt2Df7+/ujfvz9OnDhRoPdZpkwZbNy4EdHR0ViyZAkCAwOxaNGiPNfNyclB9+7d0bp1a1y+fBlnz57FiBEjoFAoAAAhISEYN24cJk6ciKtXr2LkyJEYPHgwjh07BuBV4bho0SKsXr0aN2/exJ49e1C/fn2N8r7upcvOztbq/v744w8AwIYNGxAfHy89/6f27dvD2toau3btktqUSiV++eUX+Pn5AQAiIiLQu3dv9O3bF1euXMH333+P7777Dhs3bgQAhIeHY+zYsZg1axb+/PNPHDx4EK1atdLoMyAq8d7rVrFEVCg2bNggrKys3rqOj4+PmDhxohBCiIiICAFA3Lt3L1+vn5OTI8qUKSP27t37xnXs7OzETz/9JD3Pzs4WVapUEd26dRNCCPH8+XNhYmIiwsLC1LYbOnSo6Nev3xtft3Xr1sLQ0FCYm5tLjwkTJuS57rx584Sbm5v0fMaMGcLV1VUIIURSUpIAII4fP57ntp6enmL48OFqbZ9++qnw8fERQgixcOFC4eLiIrKyst6Y9Z/+fUwePHggmjdvLqpUqSIyMzPfe3+Ojo5i0aJF0nMAIiQkRG2df75/IYQYO3asaNu2rfT80KFDwsjISCQnJwshhPD19RUdOnRQe42vv/5a1KlTRwghxK5du4SlpaVITU3N12dAVBqxR4ioBFAqlZgzZw4aNGgAGxsbWFhY4PDhw4iNjQUAuLq6ol27dqhfvz4+/fRTBAYG4q+//pK2T0xMxKhRo+Di4gIrKytYWVnh+fPn0vb/lpKSgvj4eHh4eEhtBgYGcHd3l55HR0fj5cuX6NChgzR2xcLCAps3b8bt27ff+n78/PwQFRUlPaZMmQLgVa9Jy5YtUalSJVhYWOC77757Y8Zy5cph0KBB6NixI7p06YIlS5aojbOKiYlBixYt1LZp0aIFYmJiAACffvopMjIy4OzsjOHDhyMkJOSdA7ZTUlJgYWEBc3NzODg4ICsrC7t374aRkVGh7O9d/Pz8cPz4ccTFxQEAgoKC4OPjg7Jly771M7h58yaUSiU6dOgAR0dHODs7Y8CAAQgKCsKLFy/eKxNRScNCiKgEWLhwIRYtWoRJkybh999/R1RUFDp27IisrCwAgL6+PkJDQ3HgwAHUqVMHy5YtwwcffCBd0TRo0CBERERg8eLFCAsLQ1RUFGxsbKTtC0KlUgEA/ve//6kVNdHR0e8cJ2RlZYUaNWpIj/Lly+PcuXPo27cvOnfujH379iEyMhLTpk17a8YNGzbg7Nmz8PT0xI4dO+Di4oJz585Jy1+fJntNCCG1OTg44M8//8SKFStgamqKMWPGoFWrVm8dk1SmTBlERUXhypUreP78OSIiItCkSZNC29+7NG3aFNWrV8f27duRkZGBkJAQ9O/fP8/9/7Ptn+/n4sWL2LZtG+zs7DB9+nS4urrmGndEVJqxECIqAU6dOoVu3bqhf//+cHV1hbOzM27evKm2jkKhQIsWLTBz5kxERkbCyMhIGtNz6tQpjB07Fj4+PtLA2adPn75xf1ZWVrCzs1MrKnJychARESE9r1OnDoyNjREbG6tW1NSoUQMODg4av8czZ87A0dER06ZNg7u7O2rWrKk28PlNGjVqhClTpiAsLAz16tVDcHAwAKB27do4ffq02rphYWGoXbu29NzU1BRdu3bF0qVLcfz4cZw9exZXrlx547709PRQo0YNODs7w9zcXG2ZtvdnaGgIpVL5zvfv6+uLoKAg7N27F3p6evjoo4+kZXXq1Mkzk4uLi3SVm4GBAdq3b4958+bh8uXLuHfvHn7//fd37peotCj+16sSEWrUqIFdu3YhLCwMZcuWxc8//4yEhATpS/b8+fM4evQovL29UbFiRZw/fx5PnjyRlteoUQNbtmyBu7s7UlNT8fXXX7/zcvxx48bhp59+Qs2aNVG7dm38/PPPaj0FZcqUwVdffQV/f3+oVCq0bNkSqampCAsLg4WFBT777DON32NsbCy2b9+OJk2a4H//+1+uwdn/dPfuXaxZswZdu3aFvb09/vzzT9y4cQMDBw4EAHz99dfo3bs3GjdujHbt2mHv3r3YvXs3jhw5AuDVvEBKpRLNmjWDmZkZtmzZAlNTUzg6OmqU+zVt78/JyQlHjx5FixYtYGxsLJ3u+jc/Pz/MnDkTc+bMwSeffAITExNp2cSJE9GkSRPMnj0bffr0wdmzZ7F8+XIEBAQAAPbt24c7d+6gVatWKFu2LPbv3w+VSoUPPvigQJ8BUYkk7xAlIsrLvwfmJiUliW7dugkLCwtRsWJF8e2334qBAwdKA5ejo6NFx44dRYUKFYSxsbFwcXERy5Ytk7a/ePGicHd3F8bGxqJmzZri119/zTU499+ys7PFuHHjhKWlpbC2thYTJkxQ26cQQqhUKrFkyRLxwQcfCENDQ1GhQgXRsWNHceLEiTe+buvWrcW4cePyXPb1118LGxsbYWFhIfr06SMWLVqk9jn8c7BwQkKC6N69u7CzsxNGRkbC0dFRTJ8+XSiVSmn9gIAA4ezsLAwNDYWLi4vYvHmztCwkJEQ0a9ZMWFpaCnNzc9G8eXNx5MiRN+bOzwD299nfv4/Hb7/9JmrUqCEMDAyEo6Njrvf/T02aNBEAxO+//55r2c6dO0WdOnWEoaGhqFq1qpg/f7607NSpU6J169aibNmywtTUVDRo0EDs2LHjre+RqLRRCPGviUGIiIiIdATHCBEREZHOYiFEREREOouFEBEREeksFkJERESks1gIERERkc5iIUREREQ6i4UQERER6SwWQkRERKSzWAgRERGRzmIhRERERDqLhRARERHpLBZCREREpLP+DwXXEcHHxH5pAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9451\n",
            " - Precision: 0.9452\n",
            " - Recall: 0.9451\n",
            " - F1-Score: 0.9450\n",
            " - Adjusted Rand Index: 0.7917\n",
            " - Mean Squared Error: 0.0549\n",
            " - R-squared: 0.7794\n",
            " - Área bajo la curva : 0.944\n",
            " - Confusion Matrix: \n",
            "[[159  12]\n",
            " [  8 185]]\n",
            " - Global Score : 92.67\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Test set :\n",
            " - Accuracy: 0.9313\n",
            " - Precision: 0.9313\n",
            " - Recall: 0.9313\n",
            " - F1-Score: 0.9313\n",
            " - Adjusted Rand Index: 0.7434\n",
            " - Mean Squared Error: 0.0687\n",
            " - R-squared: 0.7231\n",
            " - Área bajo la curva : 0.931\n",
            " - Confusion Matrix: \n",
            "[[153  13]\n",
            " [ 12 186]]\n",
            " - Global Score : 90.88\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHFCAYAAAAe+pb9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8FElEQVR4nO3dd1hT59sH8G/YGxQVQRFEpW5UcKCidSutu3WA1r1oq6Kt1dpq1Vr7OuqeuBXUVqWt/ly4B46CuLHWiQqISmXLSJ73D0o0AkowcCD5fq4rV0+ec07OnaSa2/s8QyaEECAiIiLSQXpSB0BEREQkFSZCREREpLOYCBEREZHOYiJEREREOouJEBEREeksJkJERESks5gIERERkc5iIkREREQ6i4kQERER6SwmQkRUIFeuXMGQIUNQtWpVmJiYwMLCAo0aNcLcuXMRHx8vdXiFIpPJVB5WVlZo3rw5tm3blu85586dw6effgp7e3sYGRmhYsWK+OSTT3D27Nl8z9HGz45IWzARIqJ3CggIgLu7O/766y98/fXXOHDgAIKDg/Hpp59i1apVGDZsmNQhFlpOEhMaGopVq1YhMTERPj4+CAoKynXs0qVL0aJFCzx69Ahz587F4cOHMX/+fDx+/BgtW7bEsmXLcp2jzZ8dkVYQRERvERoaKvT19UXnzp3Fy5cvc+1PT08Xf/zxh0aulZqaKhQKhUZeqyAAiM8//1yl7f79+wKAaNWqlUr76dOnhZ6envj4449FZmamyr7MzEzx8ccfCz09PXH69Glle3F+dkRUOKwIEdFb/fTTT5DJZFizZg2MjY1z7TcyMkK3bt2Uz2UyGX744Ydcxzk7O2Pw4MHK5xs3boRMJsOhQ4cwdOhQlC9fHmZmZtixYwdkMhmOHDmS6zVWrlwJmUyGK1euAADCwsLQr18/ODs7w9TUFM7Ozujfvz8ePHhQ6Pfr5OSE8uXL48mTJyrtc+bMgUwmw8qVK2FgYKCyz8DAACtWrIBMJsPPP/+sbFf3syOi4sdEiIjyJZfLcfToUbi7u8PR0bFIrjF06FAYGhpiy5Yt2LlzJ3r27IkKFSpgw4YNuY7duHEjGjVqhPr16wMA7t+/jw8++ACLFi3CwYMH8X//93+IiYlB48aN8ezZs0LFk5CQgPj4eLi6uirb5HI5jh07Bg8PD1SuXDnP8xwdHeHu7o6jR49CLpcXy2dHRO/P4N2HEJGuevbsGVJTU1G1atUiu0a7du2wevVqlbYBAwZg5cqVSEhIgLW1NQAgMjISFy5cwNKlS5XHffLJJ/jkk0+Uz+VyOT7++GPY2dkhKCgIY8eOfef1hRDIysqCEAL379/HV199BTMzM0yfPl15TEE/h6pVq+LChQt4/vw5hBBF/tkR0ftjRYiIJNW7d+9cbUOHDkVaWhp27NihbNuwYQOMjY3h4+OjbEtOTsY333yD6tWrw8DAAAYGBrCwsEBKSgoiIyMLdP0VK1bA0NAQRkZGcHV1xf79+7Ft2za4u7ur/V6EEACybw8SUenARIiI8lWuXDmYmZnh3r17RXYNe3v7XG116tRB48aNlbfH5HI5tm7diu7du6Ns2bLK43x8fLBs2TIMHz4cBw8exIULF/DXX3+hfPnySEtLK9D1+/Tpg7/++guhoaFYvXo1LC0t0a9fP/zzzz/KYwr6Ody/fx9mZmYoW7ZssXx2RPT+mAgRUb709fXRrl07hIeH49GjRwU6x9jYGOnp6bnanz9/nufx+VVPhgwZgnPnziEyMhIHDhxATEwMhgwZotyfkJCAvXv3YtKkSZg8eTLatWuHxo0bo169emrNzVO+fHl4eHjA09MTI0eOxO+//46UlBT4+/srj9HX10ebNm0QFhaW7+fw6NEjhIeHo23bttDX1y/UZ0dExY+JEBG91ZQpUyCEwIgRI5CRkZFrf2ZmJvbs2aN87uzsrBzVlePo0aNITk5W67r9+/eHiYkJNm7ciI0bN6JSpUro2LGjcr9MJoMQItdorLVr10Iul6t1rdd5eXnhs88+w//+9z+VSRJzPgc/P79cry+XyzFmzBgIITBlypRc5xT0syOi4sfO0kT0Vp6enli5ciX8/Pzg7u6OMWPGoE6dOsjMzERERATWrFmDunXromvXrgCAgQMH4vvvv8e0adPQunVr3LhxA8uWLVN2ei4oGxsb9OzZExs3bsSLFy/w1VdfQU/v1b/drKys0KpVK8ybNw/lypWDs7MzTpw4gXXr1sHGxua93vOsWbOwY8cOfP/99zh8+DAAoEWLFli0aBHGjx+Pli1b4osvvkCVKlUQFRWF5cuX4/z581i0aBGaN29e6M+OiCQg2QxGRFSqXLp0SQwaNEhUqVJFGBkZCXNzc9GwYUMxbdo0ERcXpzwuPT1dTJo0STg6OgpTU1PRunVrcenSJeHk5CQGDRqkPG7Dhg0CgPjrr7/yveahQ4cEAAFA3Lp1K9f+R48eid69e4syZcoIS0tL0blzZ3Ht2rVc18oP8phQMcfXX38tAIgTJ06otJ89e1Z88sknws7OThgYGIgKFSqIXr16idDQ0HyvU9DPjoiKn0yI/4Y5EBEREekY9hEiIiIincVEiIiIiHQWEyEiIiLSWUyEiIiISGcxESIiIiKdxUSIiIiIdJbOTaioUCgQHR0NS0tLLoxIRERUSgghkJSUBAcHB5XJVd+XziVC0dHRcHR0lDoMIiIiKoSHDx+icuXKGns9nUuELC0tAWR/kFZWVhJHQ0RERAWRmJgIR0dH5e+4puhcIpRzO8zKyoqJEBERUSmj6W4t7CxNREREOouJEBEREeksJkJERESks5gIERERkc5iIkREREQ6i4kQERER6SwmQkRERKSzmAgRERGRzmIiRERERDqLiRARERHpLEkToZMnT6Jr165wcHCATCbD77///s5zTpw4AXd3d5iYmMDFxQWrVq0q+kCJiIhIK0maCKWkpMDNzQ3Lli0r0PH37t2Dt7c3vLy8EBERgW+//RZjx47Frl27ijhSIiIi0kaSLrrapUsXdOnSpcDHr1q1ClWqVMGiRYsAALVq1UJYWBjmz5+P3r17F1GUREREpK1K1erzZ8+eRceOHVXaOnXqhHXr1iEzMxOGhoYSRUZEREQFJs8EMpOBjOTs/76+nU/btcvJRRJKqUqEYmNjYWdnp9JmZ2eHrKwsPHv2DPb29rnOSU9PR3p6uvJ5YmJikcdJRESkFYQAstIKlKi8uy3l1XN5RoFDSEgzxhfB3th68YMieYulKhECAJlMpvJcCJFne445c+ZgxowZRR4XERGRpBRZuRMOTSQvEJK9pTP3HDEgqBfu/1sGwMsiuUapSoQqVqyI2NhYlba4uDgYGBjA1tY2z3OmTJmCCRMmKJ8nJibC0dGxSOMkIiLKlxBA1st8kpACJjF57Zenv/vakpABhuaAkQVg+N/D6I3/5tGWLszQr9M9PPo3EwBgaWGIpCK4O1aqEiFPT0/s2bNHpe3QoUPw8PDIt3+QsbExjI2NiyM8IiLSNgr5q+Qk3ySkEMmLUEj9zvKmb/RGYmKuVvKSZ5uBKZDPXZu3MQawbtMddOq0FS1aOGLlyvaoX3+Wxt+ypIlQcnIybt++rXx+7949XLp0CWXLlkWVKlUwZcoUPH78GJs3bwYAjB49GsuWLcOECRMwYsQInD17FuvWrcO2bdukegtERFQSCJHd70St2z8p7z42K03qd5a/nCQlryTkzX0FSl7MsxMhiQgh8PJlFkxNXxU2OnashoMHB6Bt26pITdXCztJhYWFo06aN8nnOLaxBgwZh48aNiImJQVRUlHJ/1apVsW/fPvj7+2P58uVwcHDAkiVLOHSeiKg0EQogM7VwfVjelrwosqR+Z3mT6QNGlgVLTAzMC5i8mAEy7VkcIj4+DaNH70VaWhb+/LOfSr/fjh2rFem1ZSKnt7GOSExMhLW1NRISEmBlZSV1OEREJZs8Izv5KHRn29cSmNeTmZLKwLQQt3/ecftI36hQt4Z0xbFj9zBwYDAeP04CAKxY4Y0xYxrnOq6ofr9LVR8hIiLKhxBAVqpqnxVNJC9qDHMuVjK9t9wWKugtoTdvLZkDevpSvzOdkZEhx3ffHcX8+aHIKcmUKWOCihUtijUOJkJERMVNOcz5bVUTdZMXaYc5v5WBST6JR2GSl/9ex8CEVZZS7ObNZ/Dx2YWIiFcjwdu2rYpNm3qgcuXivVvDRIiIKD9vHeacR/KiDcOc80s8CpOw5Nw20uNPDWUTQmD16nBMmHAQaWnZfboMDfUwZ047+Pt7Qk+v+JNb/t9JRNqhQMOc36igaNUw53ySkGIY5kxUEOnpWfj009+wZ88tZVutWuUQGNgLDRvmXhmiuDARIqLiVahhzgVIXkrrMOdCJS/SDnMmKgxjYwNYWr6a18/PzwPz5nWEmZm064QyESKi/L3XMOfXt9/o+6INw5wLPG+Ldg1zJnofy5d7459/nmPatNb4+GNXqcMBwESISHvIM9SbKK4g/VmyUqV+V/kzMHvPvit5VGA4zJlIY65ceYLo6CR07lxd2WZjY4Lz54fnuz6oFJgIERU3lWHOyZpLXhSZUr+zvMn0XlVZ3rvj7X//NTDjMGeiEkqhEFi8+BwmTz4Cc3NDXLkyRmUkWElKggAmQkRvl98wZ5WqyTtGDZXKYc4a6Hibs61vzCoLkY6Ijk7C4MG/IyTkLoDsuYJ++ukUVqz4SOLI8sdEiLRDQYY5v2uRRG0Y5pxf3xUOcyaiIvb77zcxfPifeP781cCFiRM9MXt2Wwmjejf+rUfFT+1hzgVMXkr1MOd39F3hMGciKqFSUjLg738QAQEXlW329hbYvLkn2rd3kTCygmEiRPl7r2HOb+n7ojXDnPPbb66azHCYMxFpqbCwaPj67satW8+VbT171kRAQFfY2ppJGFnBMRHSFkLx2hBlNastb5sxt6QOc9YzyO6AW+CVmt+V0JhzmDMRkRpevsxCt27bEBOTDAAwMzPEkiWdMXRowxLXIfptmAiVJKlPgagjQEaS9g1z1vS0/ayyEBFJysTEACtWfISePXegcWMHBAb2Qo0atlKHpTYmQiVFZhqw2Q1IiZEuhjeHOWsieeEwZyIirZGRIYeR0au/03v0qIng4L746KMaMDQsnX/XMxEqKZ5fUy8Jeusw53xGBHGYMxERFUJCwkt88cV+pKdnYceOT1RuffXoUVPCyN4fE6GSIjHq1bZrH8C191sqMBzmTERExePMmSgMGBCM+/dfAAA++ugyBg1qIGlMmsRf05Ii6bVEqHo34IM+0sVCREQ6LzNTjlmzTmL27FNQKLIngbWyMoaJiXalDtr1bkqz1ytCllWki4OIiHTe7dvxGDBgN86ff6xsa9HCEVu39oKzs410gRUBJkIlxesVISsmQkREVPyEENi48RK+/HI/UlKy1y/U15fhhx8+xOTJLWFgoH1TjDARKilyKkIyPcDcQdpYiIhI57x8mYWBA4Oxc+cNZVu1amUQGNgLTZtWljCyosVEqKTIqQiZOwD6htLGQkREOsfYWB+ZmXLl82HDGmLRos6wsNDuedu0r8ZVGmWmAalx2du8LUZERBKQyWRYu7Yb6tQpj507P8Xatd20PgkCWBEqGZIfvdpmR2kiIioGN28+w5MnyWjd2lnZVq6cGa5cGQM9Pd2ZU44VoZIgkR2liYioeAghsGpVGBo1Wo0+fXbiyZNklf26lAQBTIRKBpURY07SxUFERFotLi4F3btvx5gx/0NaWhbi4lIwa9ZJqcOSFG+NlQSJD15t89YYEREVgf37/8GQIX/gyZMUZdvnnzfG3LkdJIxKekyESgLeGiMioiKSlpaJb745jKVLLyjbKlQwx/r13fDRR64SRlYyMBEqCZI4qzQREWne5cux8PXdjevXnyrbvL1rYP36brCzs5AwspKDiVBJkJMIGVkCxtbSxkJERFohLS0THTtuRVxc9q0wExMDzJ/fAX5+jVVWj9d17CwtNSFe3RqzrALwf04iItIAU1NDLFzYCQDg5maH8PCR+PzzJkyC3sCKkNTSngLy9Oxt9g8iIqL3IJcroK//qsbh41MPQgh88kltGBvzJz8vrAhJjavOExHRe0pJycDIkXswfPieXPt8feszCXoLfjJS46rzRET0HsLCouHruxu3bj0HAHh7V8enn9aROKrSgxUhqbEiREREhSCXKzBnzil4eq5TJkFmZoZIT5e/40x6HStCUmNFiIiI1BQVlYCBA4Nx8uSrCXk9PBwQGNgLrq62EkZW+jARkhorQkREpIbt269h9Oi9SEjIHmgjkwHffuuF6dNbw9BQX+LoSh8mQlJTVoRkgEUlSUMhIqKSKy0tE6NG7cWWLVeUbVWqWGPr1p7w8uI6lYXFREhqORUhCwdA31DaWIiIqMQyNjZQWSfMx6celi/3ho2NiYRRlX7sLC2lrJdA6pPsba46T0REb6GnJ8PGjd1RrVoZbN3aE4GBvZgEaQArQlJKevhqm/2DiIjoNbdvx+P581Q0bVpZ2WZvb4mbN7+AgQHrGJrCT1JKXHWeiIjeIITAhg0RaNBgFXr3/hXx8Wkq+5kEaRY/TSlx1XkiInpNfHwa+vTZiaFD/0RKSiYeP07CjBnHpQ5Lq/HWmJRYESIiov8cO3YPAwcG4/HjJGXbsGENMXt2Owmj0n5MhKTEihARkc7LyJDju++OYv78UAiR3VamjAkCArqid+/a0ganA5gISYkVISIinXbz5jP4+OxCRESssq1t26rYtKkHKle2kjAy3cFESEo5FSFDC8DYRtJQiIioeKWmZqJVqw14+jQVAGBoqIc5c9rB398TenoyiaPTHewsLRUhXiVCVlWy50gnIiKdYWZmiNmz2wIAatUqhwsXRmDixOZMgooZK0JSSXuWPaEiwP5BREQ6QggB2Wv/8B0+vBGEAAYMqA8zM64uIAUmQlLhqvNERDojLS0T33xzGEIILF3qrWyXyWQYOdJdwsiIiZBUuOo8EZFOuHw5Fr6+u3H9+lMAQOfO1fHRR64SR0U52EdIKqwIERFpNYVCYOHCs2jSZK0yCTIxMVB2jqaSgRUhqbAiRESktaKjkzB48O8ICbmrbHNzs0NQUG/Url1ewsjoTUyEpJL44NU2V54nItIawcGRGDFiD54/f7VG2MSJnpg9uy2MjfmzW9LwG5GK8taYDLCoJGkoRET0/l6+zMLYsfsREHBR2ebgYIlNm3qgfXsXCSOjt2EiJJWcW2MWDoA+h0wSEZV2hoZ6uHnzmfJ5z541ERDQFba2ZhJGRe/CztJSyHoJpD7J3mb/ICIiraCvr4ctW3qiUiVLrF3bFbt29WESVAqwIiSFpEevtjlijIioVHrw4AX+/fclGjSoqGxzcrLBnTtj2ReoFGFFSApcdZ6IqFTbtu0q3NxWoVevHUhMTFfZxySodGEiJAWuOk9EVColJLzEwIHB8PHZjYSEdNy79wIzZhyXOix6D5InQitWrEDVqlVhYmICd3d3nDp16q3HBwYGws3NDWZmZrC3t8eQIUPw/PnzYopWQ1gRIiIqdc6ciUKDBquxdesVZZuPTz1Mm9ZawqjofUmaCO3YsQPjx4/H1KlTERERAS8vL3Tp0gVRUVF5Hn/69Gl89tlnGDZsGK5fv47ffvsNf/31F4YPH17Mkb8nVoSIiEqNzEw5pk07hlatNuL+/RcAACsrY2zd2hOBgb1gbW0ibYD0XtROhC5evIirV68qn//xxx/o0aMHvv32W2RkZKj1Wr/88guGDRuG4cOHo1atWli0aBEcHR2xcuXKPI8/d+4cnJ2dMXbsWFStWhUtW7bEqFGjEBYWpu7bkBYrQkREpcKdO/Hw8tqAWbNOQqEQAICWLavg8uXR8PWtL3F0pAlqJ0KjRo3CrVu3AAB3795Fv379YGZmht9++w2TJk0q8OtkZGQgPDwcHTt2VGnv2LEjQkND8zynefPmePToEfbt2wchBJ48eYKdO3fio48+yvc66enpSExMVHlILqciZGgOmJSRNhYiIspTSkoGmjVbh/PnHwMA9PVl+PHHNjh+fBCcnW2kDY40Ru1E6NatW2jQoAEA4LfffkOrVq0QFBSEjRs3YteuXQV+nWfPnkEul8POzk6l3c7ODrGxsXme07x5cwQGBqJv374wMjJCxYoVYWNjg6VLl+Z7nTlz5sDa2lr5cHR0LHCMRUKIVxUhyyqATCZtPERElCdzcyN8950XAKBatTIIDR2GqVNbQV9f8u61pEFqf5tCCCgUCgDA4cOH4e3tDQBwdHTEs2fP3nZqnmRvJAJCiFxtOW7cuIGxY8di2rRpCA8Px4EDB3Dv3j2MHj0639efMmUKEhISlI+HDx+qHaNGpT0Hsv5bf4b9g4iIShQhhMrzL79sil9+6YhLl0ajSRMuh6SN1J7swMPDAz/++CPat2+PEydOKPvz3Lt3L1d1523KlSsHfX39XNWfuLi4fF9nzpw5aNGiBb7++msAQP369WFubg4vLy/8+OOPsLe3z3WOsbExjI2NCxxXkWP/ICKiEicjQ47vvjsKPT0Zfv65vbJdT08Gf39PCSOjoqZ2RWjRokW4ePEivvjiC0ydOhXVq1cHAOzcuRPNmzcv8OsYGRnB3d0dISEhKu0hISH5vk5qair09FRD1tfXB5A7iy+xuOo8EVGJEhn5FM2arcW8eaGYO/cMjh27J3VIVIzUrgjVr19fZdRYjnnz5imTkoKaMGECBg4cCA8PD3h6emLNmjWIiopS3uqaMmUKHj9+jM2bNwMAunbtihEjRmDlypXo1KkTYmJiMH78eDRp0gQODg7qvhVpJHHoPBFRSSCEwKpVYZg48RDS0rIAAAYGerhz51+0aVNV4uiouBR6HvDw8HBERkZCJpOhVq1aaNSokdqv0bdvXzx//hwzZ85ETEwM6tati3379sHJKbtSEhMTozKn0ODBg5GUlIRly5Zh4sSJsLGxQdu2bfF///d/hX0bxS+Rt8aIiKQWF5eCYcP+xN69t5RttWqVQ1BQb5W1w0j7yYSa95Ti4uLQt29fnDhxAjY2NhBCICEhAW3atMH27dtRvnz5oopVIxITE2FtbY2EhARYWVkVfwB7PgVu7czeHn4XsOa/OoiIitP+/f9g8OA/EBeXomzz8/PAvHkdYWZmKGFk9DZF9futdh+hL7/8EklJSbh+/Tri4+Px77//4tq1a0hMTMTYsWM1FpjWUlaEZIAFRyAQERWXly+zMHbsfnh7BymToPLlzbBnT38sX/4RkyAdpfatsQMHDuDw4cOoVauWsq127dpYvnx5rskRKQ85fYQs7AF9I2ljISLSIfr6Mpw790j53Nu7Btav7wY7OwsJoyKpqV0RUigUMDTMnTUbGhoq5xeifGSlAyn/TRfA/kFERMXK0FAfgYG9UK6cGZYt64K9e/szCSL1E6G2bdti3LhxiI6OVrY9fvwY/v7+aNeunUaD0zrJr/4lwkSIiKhoRUcnITLyqUpbjRq2uH9/HD7/vEm+k/eSblE7EVq2bBmSkpLg7OyMatWqoXr16qhatSqSkpLeutQFgavOExEVk+DgSNSvvxK9e/+K1NRMlX3m5uyWQK+o3UfI0dERFy9eREhICG7evAkhBGrXro327du/+2Rdx1mliYiKVEpKBvz9DyIg4CIA4PnzNMyceUJltmii16mdCN2/fx/Ozs7o0KEDOnToUBQxaS9WhIiIikxYWDR8fXfj1q3nyraePWvi668LvuoB6R61b425uLigZcuWWL16NeLj44siJu3FihARkcbJ5QrMmXMKnp7rlEmQmZkh1q7til27+sDW1kziCKkkUzsRCgsLg6enJ3788Uc4ODige/fu+O2335Cenl4U8WkXVoSIiDQqKioBbdtuxrffHkVWVvbI5caNHXDp0igMG9aIHaLpndROhBo1aoR58+YhKioK+/fvR4UKFTBq1ChUqFABQ4cOLYoYtUdORcjADDApK20sRESlXFJSOjw81uDkyezFrGUyYOpUL5w5MxQ1athKHB2VFmonQjlkMhnatGmDgIAAHD58GC4uLti0aZMmY9MuQrxaed7KKftPLBERFZqlpTHGj28GAKhSxRonTgzGjz+2haGheguAk24r9KKrDx8+xLZt2xAUFISrV6/C09MTy5Yt02Rs2iXtOZCVlr3N22JERBrxzTctoFAIfPFFE9jYmEgdDpVCaidCa9asQWBgIM6cOYMPPvgAvr6++P333+Hs7FwE4WkRdpQmIiq0rCwFZs06AQMDPXz/fWtlu76+Hr77rpWEkVFpp3YiNGvWLPTr1w+LFy9GgwYNiiAkLcWO0kREhXLnTjx8fXfj/PnH0NOToX17F3h6OkodFmkJtROhqKgo9sIvDFaEiIjUIoTApk2X8eWX+5GcnAEgu3vl5ctPmAiRxhQoEbpy5Qrq1q0LPT09XL169a3H1q9fXyOBaR1WhIiICiw+Pg2jRu3Fzp03lG3VqpVBYGAvNG1aWcLISNsUKBFq0KABYmNjUaFCBTRo0AAymQxCCOX+nOcymQxyubzIgi3VWBEiIiqQY8fuYeDAYDx+nKRsGzasIRYt6gwLC64TRppVoETo3r17KF++vHKbCkGZCMkAi0qShkJEVBJlZMjx/fdHMW9eKHL+rV2mjAkCArqid+/a0gZHWqtAiZCTk5Ny+8GDB2jevDkMDFRPzcrKQmhoqMqx9JqcW2PmFQEDY2ljISIqgRQKgf37byuToLZtq2LTph6oXNlK2sBIq6k9oWKbNm3yXGMsISEBbdq00UhQWicrHUiJyd5m/yAiojyZmBggKKg3rKyMMX9+B4SEDGQSREVO7VFjOX2B3vT8+XOYm5trJCitk/z41Tb7BxERAQDi4lKQlJSOatVeLTlUt24FPHgwnpMjUrEpcCLUq1cvANkdowcPHgxj41e3d+RyOa5cuYLmzZtrPkJtwI7SREQq9u//B4MH/wEHB0ucOzcMxsavfo6YBFFxKnAiZG1tDSC7ImRpaQlTU1PlPiMjIzRr1gwjRozQfITagEPniYgAAGlpmfjmm8NYuvQCgOyq0OzZpzBzJrtWkDQKnAht2LABAODs7IyvvvqKt8HUwYoQEREuX46Fr+9uXL/+VNnm7V0Dn3/eWMKoSNep3Udo+vTpRRGHdstZdR7IXnmeiEiHKBQCixefw+TJR5CRkT3XnImJAebP7wA/v8ZcrYAkVaBEqFGjRjhy5AjKlCmDhg0bvvV/2osXL2osOK3BW2NEpKOio5MwaNDvOHz4rrLNzc0OQUG9Ubt2eQkjI8pWoESoe/fuys7RPXr0KMp4tFPOrTEDM8Ck7NuPJSLSEgkJL9GgwSo8fZqqbJs40ROzZ7dV6RxNJCWZeH2tDB2QmJgIa2trJCQkwMqqGOanEAJYYgFkpQJlawJDIov+mkREJcR33x3F7Nmn4OBgiU2beqB9exepQ6JSqqh+v9VOyR8+fAiZTIbKlbMXvbtw4QKCgoJQu3ZtjBw5UmOBaY2X8dlJEMCO0kSkc6ZPbw2FQmDiRE/Y2ppJHQ5RLmrPLO3j44Njx44BAGJjY9G+fXtcuHAB3377LWbOnKnxAEs99g8iIh0glyswZ84pLFx4VqXd0FAfP/3UjkkQlVhqJ0LXrl1DkyZNAAC//vor6tWrh9DQUAQFBWHjxo2ajq/049B5ItJyUVEJaNt2M7799ii++eYwIiJipA6JqMDUToQyMzOVHacPHz6Mbt26AQBq1qyJmBj+z58LK0JEpMW2b7+G+vVX4uTJ7GlCsrIUCA19KHFURAWndiJUp04drFq1CqdOnUJISAg6d+4MAIiOjoatra3GAyz1WBEiIi2UmJiOzz4LRv/+u5CQkA4AqFLFGidODMbnnzeRODqiglM7Efq///s/rF69Gh9++CH69+8PNzc3AMCff/6pvGVGr2FFiIi0zJkzUXBzW4UtW64o23x86uHy5dHw8uKksVS6qD1q7MMPP8SzZ8+QmJiIMmXKKNtHjhwJMzN2hsvl9YqQRWXp4iAiek+ZmXLMmnUSs2efgkKRPfOKlZUxVqzwhq9vfYmjIyqcQs1opa+vj6ysLJw+fRoymQyurq5wdnbWcGhaIicRMq8IGBhLGwsR0XvIyJBjx47ryiSoZcsq2LKlJ5ydbaQNjOg9qH1rLCUlBUOHDoW9vT1atWoFLy8vODg4YNiwYUhNTX33C+gSeQaQ/F8HcvYPIqJSztzcCIGBvWBqaoAff2yD48cHMQmiUk/tRGjChAk4ceIE9uzZgxcvXuDFixf4448/cOLECUycOLEoYiy9kh4B+G/ibvYPIqJSJj4+DQ8fJqi0eXg44P798Zg6tRX09dX+CSEqcdS+NbZr1y7s3LkTH374obLN29sbpqam6NOnD1auXKnJ+Eo3lRFj7EBIRKXHsWP3MHBgMBwdrXHq1BAYGLxKeipUMJcwMiLNUjudT01NhZ2dXa72ChUq8NbYmzhijIhKmYwMOSZNCkG7dpvx+HESzp17hP/7v9NSh0VUZNROhDw9PTF9+nS8fPlS2ZaWloYZM2bA09NTo8GVepxDiIhKkcjIp2jWbC3mzQtFznLcbdtWxaBBDSSNi6goqX1rbNGiRejcuTMqV64MNzc3yGQyXLp0CSYmJjh48GBRxFh6sSJERKWAEAKrV4djwoSDSEvLAgAYGurhp5/aYcIET+jpySSOkKjoqJ0I1atXD7dv30ZgYCAiIyMhhEC/fv3g6+sLU1PTooix9GJFiIhKuLi4FAwf/if27LmlbKtVqxwCA3uhYUN7CSMjKh5qJULnz5/Hn3/+iczMTLRr1w7Dhw8vqri0Q05FyMAUMOXyI0RUsrx48RJubqsQG5usbPPz88C8eR1hZmYoYWRExafAfYSCg4PRokULLF68GGvWrIG3tzcWLVpUhKGVckK8qghZVgFkLC0TUcliY2OCfv3qAADKlzfDnj39sXz5R0yCSKcUOBH66aefMHjwYOXcQTNmzMCPP/5YlLGVbi//BTJTsrfZP4iISqg5c9pj7NgmuHp1DD7+2FXqcIiKnUyInLEBb2dlZYWwsDC4umb/QUlPT4e5uTliY2NRrly5Ig1SkxITE2FtbY2EhARYWVkV3YXiLgFbGmZv1x0GdFpbdNciInoHhUJg8eJzMDc3wsiR7lKHQ6S2ovr9LnAfoeTkZNjY2CifGxsbw9TUFImJiaUqESo2HDFGRCVEdHQSBg/+HSEhd2FiYgAvryqoVau81GERlQhqdZY+ePAgrK2tlc8VCgWOHDmCa9euKdu6deumuehKM44YI6ISIDg4EiNG7MHz52kAgJcvsxAScpeJENF/1EqEBg0alKtt1KhRym2ZTAa5XP7+UWkDVoSISEIpKRnw9z+IgICLyjYHB0ts2tQD7du7SBgZUclS4ERIoVAUZRzahxUhIpJIWFg0fH1349at58q2nj1rIiCgK2xtzSSMjKjkUXtCRSqgxAevti0rSxcHEekMuVyBuXPPYNq048jKyv7Hq5mZIZYs6YyhQxtCxmk8iHJhIlRUcipC5hUBAxNpYyEinZCSkonVq8OVSVDjxg4IDOyFGjU4oStRftRedJUKQJ4BJMdkb/O2GBEVEysrY2zZ0hOGhnqYOtULZ84MZRJE9A6sCBWF5McA/pueiR2liaiIJCamIzU1ExUrWijbvLyccOfOWDg6Wr/lTCLKwYpQUUhkR2kiKlpnzkTBzW0VfHx2QaFQnReXSRBRwamdCD18+BCPHj1SPr9w4QLGjx+PNWvWaDSwUi2JQ+eJqGhkZsoxbdoxtGq1Effvv8CxY/excOFZqcMiKrXUToR8fHxw7NgxAEBsbCw6dOiACxcu4Ntvv8XMmTM1HmCpxIoQERWB27fj4eW1AbNmnVRWgVq2rILevWtLHBlR6aV2InTt2jU0adIEAPDrr7+ibt26CA0NRVBQEDZu3Kjp+EonVoSISIOEENiwIQINGqzC+fOPAQD6+jL8+GMbHD8+CM7ONtIGSFSKqZ0IZWZmwtjYGABw+PBh5ZIaNWvWRExMjNoBrFixAlWrVoWJiQnc3d1x6tSptx6fnp6OqVOnwsnJCcbGxqhWrRrWr1+v9nWLFCtCRKQh8fFp6NNnJ4YO/RMpKZkAgGrVyiA0dBimTm0FfX129SR6H2qPGqtTpw5WrVqFjz76CCEhIZg1axYAIDo6Gra26g3T3LFjB8aPH48VK1agRYsWWL16Nbp06YIbN26gSpW8E4g+ffrgyZMnWLduHapXr464uDhkZWWp+zaKVk5FyMAEMOWCtERUOP/+mwY3t1V49ChR2TZsWEMsWtQZFhZGEkZGpD1kQgjx7sNeOX78OHr27InExEQMGjRIWY359ttvcfPmTezevbvAr9W0aVM0atQIK1euVLbVqlULPXr0wJw5c3Idf+DAAfTr1w93795F2bJl1QlbKTExEdbW1khISICVlVWhXuOthACWWgGZyUAZV2Do35q/BhHpjFGj9mDNmosoU8YEAQFd2R+IdFZR/X6rXRH68MMP8ezZMyQmJqJMmTLK9pEjR8LMrOBr2GRkZCA8PByTJ09Wae/YsSNCQ0PzPOfPP/+Eh4cH5s6diy1btsDc3BzdunXDrFmzYGpqqu5bKRrpL7KTIIC3xYjovf3ySyfI5QI//PAhKlcugn+8Eem4Qk2oqK+vj6ysLJw+fRoymQyurq5wdnZW6zWePXsGuVwOOzs7lXY7OzvExsbmec7du3dx+vRpmJiYIDg4GM+ePYOfnx/i4+Pz7SeUnp6O9PR05fPExMQ8j9MYrjpPRIUghMDq1eGwsDDCgAH1le3m5kZYu7abhJERaTe1e9mlpKRg6NChsLe3R6tWreDl5QUHBwcMGzYMqampagfw5iKAQoh8FwZUKBSQyWQIDAxEkyZN4O3tjV9++QUbN25EWlpanufMmTMH1tbWyoejo6PaMaqFq84TkZri4lLQvft2jBnzP4wZ8z/cuRMvdUhEOkPtRGjChAk4ceIE9uzZgxcvXuDFixf4448/cOLECUycOLHAr1OuXDno6+vnqv7ExcXlqhLlsLe3R6VKlWBt/WrW1Fq1akEIoTLJ4+umTJmChIQE5ePhw4cFjrFQXl91nhUhInqH/fv/Qf36K7Fnzy0AQHJyBvbuvSVxVES6Q+1EaNeuXVi3bh26dOkCKysrWFlZwdvbGwEBAdi5c2eBX8fIyAju7u4ICQlRaQ8JCUHz5s3zPKdFixaIjo5GcnKysu3WrVvQ09ND5cqV8zzH2NhYGWfOo0ip3BpzKtprEVGplZaWibFj98PbOwhPnqQAAMqXN8OePf0xblwziaMj0h1qJ0Kpqal5VmwqVKig9q2xCRMmYO3atVi/fj0iIyPh7++PqKgojB49GkB2Neezzz5THu/j4wNbW1sMGTIEN27cwMmTJ/H1119j6NChJaezNG+NEdE7XLnyBI0bB2Dp0gvKNm/vGrh6dQw+/thVwsiIdI/anaU9PT0xffp0bN68GSYmJgCAtLQ0zJgxA56enmq9Vt++ffH8+XPMnDkTMTExqFu3Lvbt2wcnp+xKSkxMDKKiXiUWFhYWCAkJwZdffgkPDw/Y2tqiT58++PHHH9V9G0VHZTLFvKtURKSbFAqBxYvPYfLkI8jIkAMATEwMMH9+B/j5Nc63fyQRFR215xG6evUqunTpgpcvX8LNzQ0ymQyXLl2CiYkJDh48iDp16hRVrBpR5PMIra4MJD8GzOyAMXmPfiMi3fTvv2moU2cFYmKyb+/Xr2+HoKBeqFOngsSREZV8JWYeoXr16uGff/7B1q1bcfPmTQgh0K9fP/j6+pac21NSkWcCydHZ2+woTURvKFPGFJs29UDnzoHw92+G2bPbwti4ULOYEJGGqPUnMDMzEx988AH27t2LESNGFFVMpVfyYwD/FdjYP4hI56WkZODlyyzY2r6abLZDh2r4++8vUL164WbHJyLNUquztKGhIdLT03kfOz9cdZ6I/hMWFo1Gjdbgs89+x5s9EJgEEZUcao8a+/LLL/F///d/JW+h05KAq84T6Ty5XIE5c07B03Mdbt16jn37/sHKlWFSh0VE+VD75vT58+dx5MgRHDp0CPXq1YO5ubnKfnUWXdU6rAgR6bSoqAQMHBiMkydfTazauLEDOnRwkTAqInobtRMhGxsb9O7duyhiKf1YESLSWdu3X8Po0XuRkJC9tqGengxTprTE9OmtYWioL3F0RJQftROhDRs2FEUc2oEVISKdk5iYji++2IctW64o26pUscbWrT3h5cXZ5YlKukKN28zKysLx48dx584d+Pj4wNLSEtHR0bCysoKFhYWmYyw9cipC+saAaXlpYyGiIvf8eSoaNw7AvXsvlG0+PvWwfLk3bGxMpAuMiApM7UTowYMH6Ny5M6KiopCeno4OHTrA0tISc+fOxcuXL7Fq1aqiiLPkE+LVgqtWVQCOrCPSera2ZmjRogru3XsBKytjrFjhDV/f+lKHRURqUDsRGjduHDw8PHD58mXY2toq23v27Inhw4drNLhSJf0FkPnfYrDsH0SkM5Yt6wK5XIGffmoHZ2cbqcMhIjWpnQidPn0aZ86cgZGRkUq7k5MTHj9+rLHASh2uOk+k1YQQ2LTpMqysjNGrVy1lu7W1CYKCOICEqLRSOxFSKBSQy+W52h89egRLS0uNBFUqcdV5Iq0VH5+GUaP2YufOG7CxMUHjxg5wdLSWOiwi0gC1J1Ts0KEDFi1apHwuk8mQnJyM6dOnw9vbW5OxlS6JHDFGpI2OHbuH+vVXYufOGwCAFy9eKreJqPRTuyK0cOFCtGnTBrVr18bLly/h4+ODf/75B+XKlcO2bduKIsbSgRUhIq2SkSHHd98dxfz5ochZIaNMGRMEBHRF7961pQ2OiDRG7UTIwcEBly5dwrZt23Dx4kUoFAoMGzaMq8+zIkSkNW7efAYfn12IiIhVtrVtWxWbNvVA5cpWEkZGRJpWqHmETE1NMXToUAwdOlTT8ZRer1eELCpLFwcRFZoQAqtXh2PChINIS8teT9HQUA9z5rSDv78n9PQ4LQaRtilQIvTnn38W+AW7detW6GBKtZyKkFkFwFCHK2NEpVh8fBq+//6YMgmqVascgoJ6o0GDihJHRkRFpUCJUI8ePVSey2QyiJyb5q+1AchzRJnWk2cCKdHZ2+wfRFRq2dqaYe3arujRYwf8/Dwwb15HmJkZSh0WERWhAo0aUygUysehQ4fQoEED7N+/Hy9evEBCQgL279+PRo0a4cCBA0Udb8mUEg0IRfY2+wcRlRppaZlISHip0ta9e01cuTIay5d/xCSISAeo3Udo/PjxWLVqFVq2bKls69SpE8zMzDBy5EhERkZqNMBSgavOE5U6V648gY/PLtSqVR6//vqJsqoNAPXq2UkYGREVJ7XnEbpz5w6srXNPJGZtbY379+9rIqbSh6vOE5UaCoXAwoVn0bhxAK5ff4qdO29g06bLUodFRBJROxFq3Lgxxo8fj5iYGGVbbGwsJk6ciCZNmmg0uFKDFSGiUiE6OgmdO2/FhAmHkJGR3Z/Rzc0OTZpUkjgyIpKK2onQ+vXrERcXBycnJ1SvXh3Vq1dHlSpVEBMTg3Xr1hVFjCVfzqrzACtCRCVUcHAk6tdfiZCQu8q2iRM9cf78cNSuXV7CyIhISmr3EapevTquXLmCkJAQ3Lx5E0II1K5dG+3bt1e5x65TOKs0UYmVkpIBf/+DCAi4qGxzcLDEpk090L69i4SREVFJUKgJFWUyGTp27IiOHTtqOp7SKefWmL5x9jxCRFQiPH2agpYtN+DWrefKtp49ayIgoCtsbc0kjIyISopCJUIpKSk4ceIEoqKikJGRobJv7NixGgms1BDi1a0xqyqArlbFiEqgcuXMUKdOedy69RxmZoZYsqQzhg5tqLvVayLKRe1EKCIiAt7e3khNTUVKSgrKli2LZ8+ewczMDBUqVNC9RCg9AchMzt7mbTGiEkUmkyEgoCvkcoH58zugRg1bqUMiohJG7c7S/v7+6Nq1K+Lj42Fqaopz587hwYMHcHd3x/z584sixpKN/YOISozt269h//5/VNpsbc3wxx/9mAQRUZ7UToQuXbqEiRMnQl9fH/r6+khPT4ejoyPmzp2Lb7/9tihiLNm46jyR5BIT0/HZZ8Ho338XBg36HU+eJEsdEhGVEmonQoaGhsr763Z2doiKyk4ErK2tlds6hRUhIkmdORMFN7dV2LLlCgDg6dNUBAZelTgqIiot1O4j1LBhQ4SFhcHV1RVt2rTBtGnT8OzZM2zZsgX16tUrihhLNlaEiCSRmSnHrFknMXv2KSgU2YtAW1kZY8UKb/j61pc4OiIqLdSuCP3000+wt7cHAMyaNQu2trYYM2YM4uLisGbNGo0HWOKxIkRU7G7fjoeX1wbMmnVSmQS1bFkFly+PZhJERGpRuyLk4eGh3C5fvjz27dun0YBKHZXlNRyli4NIBwghsHHjJXz55X6kpGQCAPT1ZZgx40NMntwS+vpq/9uOiHRcoeYRotfkVIRMywOGptLGQqTlnj5Nhb//QWUSVK1aGQQG9kLTppUljoyISqsCJUINGxZ8ArKLFy+++yBtocgCkh9nb7N/EFGRq1DBHKtWfYz+/Xdh2LCGWLSoMywsjKQOi4hKsQIlQj169FBuv3z5EitWrEDt2rXh6ekJADh37hyuX78OPz+/IgmyxEqOBoQie5v9g4g0LiNDjsxMOczNXyU7/frVhYtLGa4YT0QaUaBEaPr06crt4cOHY+zYsZg1a1auYx4+fKjZ6Eo6rjpPVGRu3nwGX9/dqFevAjZu7KGyj0kQEWmK2j0Lf/vtN3z22We52gcMGIBdu3ZpJKhSgyPGiDROCIFVq8LQqNFqXLwYg02bLuPXX69LHRYRaSm1EyFTU1OcPn06V/vp06dhYmKikaBKDZU5hJyki4NISzx9moLu3bdjzJj/IS0tCwBQq1Y51KhRVuLIiEhbqT1qbPz48RgzZgzCw8PRrFkzANl9hNavX49p06ZpPMASLYmTKRJpyoEDtzF48O948iRF2ebn54F58zrCzMxQwsiISJupnQhNnjwZLi4uWLx4MYKCggAAtWrVwsaNG9GnTx+NB1iiJfLWGNH7SkvLxOTJh7FkyQVlW/nyZli/vjs+/thVwsiISBeolQhlZWVh9uzZGDp0qO4lPXnJqQjpGwNm5aWNhagUiotLQbt2m3HtWpyyzdu7Btav7wY7OwsJIyMiXaFWHyEDAwPMmzcPcrm8qOIpXXIqQpaOgIwz2hKpq1w5M1SqZAkAMDExwLJlXbB3b38mQURUbNT+9W7fvj2OHz9eBKGUMukJQEZi9jb7BxEVip6eDBs2dEf79i4IDx+Jzz9vUuDJW4mINEHtPkJdunTBlClTcO3aNbi7u8Pc3Fxlf7du3TQWXInG/kFEavv995uwsTHBhx86K9vs7S0REjJQuqCISKepnQiNGTMGAPDLL7/k2ieTyXTnthnnECIqsJSUDPj7H0RAwEVUqmSJK1fGoGxZrs1HRNJT+9aYQqHI96EzSRDwxhxCTISI8hMWFo1GjdYgICB7HcLHj5OwceMlaYMiIvrPe/XwffnypabiKH1YESJ6K7lcgTlzTsHTcx1u3XoOADAzM8TatV3h799M4uiIiLKpnQjJ5XLMmjULlSpVgoWFBe7evQsA+P7777Fu3TqNB1hisSJElK+oqAS0bbsZ3357FFlZ2QsTe3g4ICJiFIYNa8QO0URUYqidCM2ePRsbN27E3LlzYWT0akXoevXqYe3atRoNrkRTqQg5ShcHUQmzffs11K+/EidPZi9KLJMBU6d6ITR0KFxdbSWOjohIldqJ0ObNm7FmzRr4+vpCX19f2V6/fn3cvHlTo8GVaDkrz5uWAwzNpI2FqISIjU3G8OF/IiEhHQBQpYo1TpwYjB9/bAtDQ/13nE1EVPzUToQeP36M6tWr52pXKBTIzMzUSFAlniILSH6cvc3+QURKFStaYPHizgCA/v3r4vLl0fDy4oLERFRyqT18vk6dOjh16hScnFT/cvvtt9/QsGFDjQVWoiVHAyK73wNXnSddlpkph1wuYGLy6q+SoUMbwsWlDNq0qSphZEREBaN2IjR9+nQMHDgQjx8/hkKhwO7du/H3339j8+bN2Lt3b1HEWPKwozQRbt+Ox4ABu+Hubo/lyz9StstkMiZBRFRqFPjW2NOnTwEAXbt2xY4dO7Bv3z7IZDJMmzYNkZGR2LNnDzp06FBkgZYoHDpPOkwIgQ0bItCgwSqcP/8YK1aEYe/eW1KHRURUKAWuCFWqVAndunXDsGHD0LlzZ3Tq1Kko4yrZWBEiHRUfn4ZRo/Zi584byrZq1cqgQgXzt5xFRFRyFbgitGnTJiQmJqJr165wdHTE999/r5xDSOewIkQ66Nixe6hff6VKEjRsWENcujQaTZpUkjAyIqLCK3Ai1L9/fxw6dAj37t3DiBEjEBgYiBo1aqBNmzYIDAzUrVmmk1gRIt2RkSHHpEkhaNduMx4/TgIAlCljgp07P8Xatd1gYWH0jlcgIiq51B4+7+joiOnTp+Pu3bs4dOgQKlWqhJEjR8Le3h5+fn5FEWPJk3NrTN8IMKsgbSxERSguLgXNmq3FvHmhECK7rV27qrh6dQx6964tbXBERBrwXmuNtWvXDlu3bsXmzZuhp6eH1atXayquki2nImTpCMje6yMkKtFsbU1haWkMADA01MP8+R1w6NBAVKpkJXFkRESaUehf8fv372P69OlwdnZG37590ahRIwQGBqr9OitWrEDVqlVhYmICd3d3nDp1qkDnnTlzBgYGBmjQoIHa13wv6QnZD4D9g0jr6evrYcuWnmje3BEXLozAxInNoafHdcKISHuoNY/Qy5cv8dtvv2HDhg04efIkKlWqhMGDB2PIkCFwdnZW++I7duzA+PHjsWLFCrRo0QKrV69Gly5dcOPGDVSpkn+SkZCQgM8++wzt2rXDkydP1L7ue0l6+Gqb/YNIy+zf/w/KlDFFs2aVlW1Vqljj9OkhXCiViLRSgStCI0eORMWKFTFixAiUL18e//vf/3D//n3MmDGjUEkQAPzyyy8YNmwYhg8fjlq1amHRokVwdHTEypUr33reqFGj4OPjA09Pz0Jd970kcsQYaZ+0tEyMHbsf3t5B8PHZhcTEdJX9TIKISFsVOBE6d+4cZsyYgejoaOzYsQOdOnV6r78cMzIyEB4ejo4dO6q0d+zYEaGhofmet2HDBty5cwfTp08v0HXS09ORmJio8ngvHDpPWuby5Vg0bhyApUsvAADu3XuBdesuShwVEVHxKPCtsStXrmj0ws+ePYNcLoednZ1Ku52dHWJjY/M8559//sHkyZNx6tQpGBgULPQ5c+ZgxowZ7x2vUs6q8wBvjVGpplAILF58DpMnH0FGhhwAYGJigAULOmLMGA+JoyMiKh6SD3l6s6okhMiz0iSXy+Hj44MZM2bA1dW1wK8/ZcoUJCQkKB8PHz5890lvw1tjpAWio5PQufNWTJhwSJkEubnZITx8JPz8GvNWGBHpDLUXXdWUcuXKQV9fP1f1Jy4uLleVCACSkpIQFhaGiIgIfPHFFwAAhUIBIQQMDAxw6NAhtG3bNtd5xsbGMDY21lzgnEyRSrng4EiMGLEHz5+nKdsmTvTE7NltYWws2V8JRESSkOxvPSMjI7i7uyMkJAQ9e/ZUtoeEhKB79+65jreyssLVq1dV2lasWIGjR49i586dqFq1mFa7zqkImZYDDM2K55pEGhIdnYT+/XchPT27CuTgYIlNm3qgfXsXiSMjIpKGpP/8mzBhAgYOHAgPDw94enpizZo1iIqKwujRowFk39Z6/PixcsLGunXrqpxfoUIFmJiY5GovMoosIPlx9jZvi1Ep5OBgiXnzOmDs2APo2bMmAgK6wtaWCT0R6a5CJUKnTp3C6tWrcefOHezcuROVKlXCli1bULVqVbRs2bLAr9O3b188f/4cM2fORExMDOrWrYt9+/bByckJABATE4OoqKh3vEoxSo4BRPa/pHlbjEoDuVwBhULA0FBf2fbFF03g4lIG3t412BeIiHSe2p2ld+3ahU6dOsHU1BQRERFIT8+ebyQpKQk//fST2gH4+fnh/v37SE9PR3h4OFq1aqXct3HjRhw/fjzfc3/44QdcunRJ7WsWGofOUykSFZWAtm03Y+rUoyrtMpkMH33kyiSIiAiFSIR+/PFHrFq1CgEBATA0NFS2N2/eHBcvavncI4nsKE2lw/bt11C//kqcPPkA8+aF4siRu1KHRERUIql9a+zvv/9WqdrksLKywosXLzQRU8nFihCVcImJ6fjii33YsuXVvF9VqljDxISjwYiI8qL234729va4fft2rmU1Tp8+DRcXLR95wooQlWBnzkRhwIBg3L//Qtnm41MPy5d7w8bGRLrAiIhKMLVvjY0aNQrjxo3D+fPnIZPJEB0djcDAQHz11Vfw8/MrihhLDlaEqATKzJRj2rRjaNVqozIJsrIyxtatPREY2ItJEBHRW6hdEZo0aRISEhLQpk0bvHz5Eq1atYKxsTG++uor5USHWisnEdIzBMxzT/pIVNzi4lLQrds2nD//WNnWsmUVbNnSE87ONtIFRkRUShSq48Ds2bMxdepU3LhxAwqFArVr14aFhYWmYyt5cm6NWToCMslXJyFCmTImECJ7W19fhhkzPsTkyS2hr8//P4mICqLQPSjNzMzg4aFDCzOmJwLpL7K32T+ISghDQ30EBvbCp5/+htWrP0aTJpWkDomIqFQpUCLUq1evAr/g7t27Cx1Micb+QVQCHDt2D2XKmKJBg4rKturVy+LixZGcF4iIqBAKVD+3trZWPqysrHDkyBGEhYUp94eHh+PIkSOwtrYuskAlxxFjJKGMDDkmTQpBu3ab0b//LqSmZqrsZxJERFQ4BaoIbdiwQbn9zTffoE+fPli1ahX09bOn7ZfL5fDz84OVlVXRRFkSqFSEnKSLg3TOzZvP4OOzCxERscrnAQHhGDeumcSRERGVfmr3qFy/fj2++uorZRIEAPr6+pgwYQLWr1+v0eBKFFaEqJgJIbBqVRgaNVqtTIIMDfUwf34HfPllU4mjIyLSDmp3ls7KykJkZCQ++OADlfbIyEgoFAqNBVbisI8QFaO4uBQMH/4n9uy5pWyrVascgoJ6q/QPIiKi96N2IjRkyBAMHToUt2/fRrNm2aX5c+fO4eeff8aQIUM0HmCJoVIRcpQuDtJ6+/f/gyFD/sCTJynKNj8/D8yb1xFmZoZvOZOIiNSldiI0f/58VKxYEQsXLkRMTAyA7GU3Jk2ahIkTJ2o8wBIjpyJkYgsYmksbC2mtR48S0b37dmRmZldXy5c3w/r13fHxx64SR0ZEpJ1kQuRMx6a+xMREAChVnaQTExNhbW2NhISEgsetkAOLjAEhByo0BAZeLNogSaf9/PNpTJlyBF26VMeGDd1hZ6cDk5USEb1DoX6/C+C9lqQuTQnQe0mJyU6CAPYPIo1SKASEECozQX/9dXNUq1YGn3xSm8PiiYiKGOfhLwiOGKMiEB2dhM6dt2LWrJMq7fr6evj00zpMgoiIisF7VYR0BkeMkYYFB0dixIg9eP48DUeO3EPHjtXQvDk74RMRFTcmQgXBihBpSEpKBvz9DyIg4FU/Mzs7c2RmyiWMiohIdzERKghWhEgDwsKi4eu7G7duPVe29exZEwEBXWFrayZhZEREuqtQiVBKSgpOnDiBqKgoZGRkqOwbO3asRgIrURIfvNpmRYjUJJcrMHfuGUybdhxZWdnD4s3MDLFkSWcMHdqQfYGIiCSkdiIUEREBb29vpKamIiUlBWXLlsWzZ89gZmaGChUqaGcilFMR0jMEzDmrLxVcXFwKPv30N5w8+SqZbtzYAYGBvVCjhq2EkREREVCIUWP+/v7o2rUr4uPjYWpqinPnzuHBgwdwd3fH/PnziyJG6eX0EbKsDMg40I4KzsrKGC9evAQAyGTA1KleOHNmKJMgIqISQu1f9UuXLmHixInQ19eHvr4+0tPT4ejoiLlz5+Lbb78tihillZ4IpL/I3rbiqvOkHhMTAwQF9cIHH9jixInB+PHHtjA01H/3iUREVCzUToQMDQ2VfRrs7OwQFZVdLbG2tlZua5Wkh6+22VGa3uHMmSjcuPFUpa1OnQq4ft0PXl5MpImIShq1+wg1bNgQYWFhcHV1RZs2bTBt2jQ8e/YMW7ZsQb169YoiRmklceg8vVtmphyzZp3E7NmnUK9eBZw/PxzGxq/+eL0+czQREZUcav/t/NNPP8He3h4AMGvWLNja2mLMmDGIi4vDmjVrNB6g5BI5dJ7e7s6deHh5bcCsWSehUAhcvvwEa9aESx0WEREVgNoVIQ8PD+V2+fLlsW/fPo0GVOKwIkT5EEJg06bL+PLL/UhOzp5GQl9fhhkzPoSfX2NpgyMiogJROxFKS0uDEAJmZtkTwD148ADBwcGoXbs2OnbsqPEAJceKEOUhPj4No0btxc6dN5Rt1aqVQVBQbzRpUknCyIiISB1q3xrr3r07Nm/eDAB48eIFmjRpggULFqB79+5YuXKlxgOUnMqs0lwLioCjR++hfv2VKknQsGENcenSaCZBRESljNqJ0MWLF+Hl5QUA2LlzJypWrIgHDx5g8+bNWLJkicYDlFxORcikLGBkIW0sJLmoqAR06rQVjx8nAQDKlDHBzp2fYu3abrCwMJI4OiIiUpfaiVBqaiosLS0BAIcOHUKvXr2gp6eHZs2a4cGDB+84u5RRyIHkR9nbvC1GAKpUscaUKS0BAG3bVsWVK2PQu3dtiaMiIqLCUjsRql69On7//Xc8fPgQBw8eVPYLiouLg5WVlcYDlFRKLKDIyt5mR2mdJISAQiFU2r7/vhU2buyOkJCBqFxZy/6fJyLSMWonQtOmTcNXX30FZ2dnNG3aFJ6engCyq0MNGzbUeICS4qrzOi0uLgXdu2/HggWhKu2GhvoYNKgB9PS4WCoRUWmn9qixTz75BC1btkRMTAzc3NyU7e3atUPPnj01GpzkuOq8ztq//x8MGfIHnjxJwYEDt9GunQsaNbKXOiwiItIwtRMhAKhYsSIqVlRdhb1JkyYaCahE4dB5nZOWlolvvjmMpUsvKNtsbEzw779pEkZFRERFpVCJ0F9//YXffvsNUVFRyMjIUNm3e/dujQRWInAyRZ1y+XIsfH134/r1V2uFdelSHRs2dIedHUcMEhFpI7X7CG3fvh0tWrTAjRs3EBwcjMzMTNy4cQNHjx6FtbV1UcQondcrQlx5XmspFAILF55FkyZrlUmQiYkBli7tgv/9z4dJEBGRFlO7IvTTTz9h4cKF+Pzzz2FpaYnFixejatWqGDVqlHINMq2RUxHSMwTMK779WCqVnj5NgY/Pbhw+fFfZVr++HYKCeqFOnQoSRkZERMVB7YrQnTt38NFHHwEAjI2NkZKSAplMBn9/f+1bdDUnEbKsDMi4erg2MjMzRFRUgvL5xImeuHBhOJMgIiIdofave9myZZGUlD2rbqVKlXDt2jUA2cttpKamajY6KWUkAS//zd5mR2mtZW5uhKCgXnB2tkFIyEDMn98RxsaF6jpHRESlUIEToaFDhyIpKQleXl4ICQkBAPTp0wfjxo3DiBEj0L9/f7Rr167IAi12SQ9fbbOjtNYIC4vGnTvxKm3u7g64desLtG/vIlFUREQklQInQps2bUJaWhqWLVuGfv36AQCmTJmCr776Ck+ePEGvXr2wbt26Igu02HHovFaRyxWYM+cUPD3Xwdd3NzIz5Sr7DQ31JYqMiIikVOB7AEJkLzNQtmxZZZuenh4mTZqESZMmaT4yqXHovNaIikrAwIHBOHkye4LM8+cfY+3aixgzprHEkRERkdTU6gwhk+nQkgKsCGmF7duvYfTovUhISAcAyGTAt996YfjwRhJHRkREJYFaiZCrq+s7k6H4+Pi37i81WBEq1RIT0/HFF/uwZcsVZVuVKtbYurUnvLw4JxQREWVTKxGaMWOG9k2amB+VipCjdHGQ2kJDH2LAgN24d++Fss3Hpx6WL/eGjY2JdIEREVGJo1Yi1K9fP1SooCPzq+RUhEzKAEaW0sZCBXb//gu0br0RWVkKAICVlTFWrPCGr299iSMjIqKSqMCjxnSqf5BC/mr4PPsHlSrOzjb48svsBYBbtHDE5cujmQQREVG+1B41phNSYgFFVvY2E6ESLef/y9cT9Z9+aofq1cti5Eh3GBhwRnAiIspfgX8lFAqF7t0WA9hRugSLj09Dnz47sWLFXyrtJiYG8PNrzCSIiIjeiWsJ5IWrzpd4x47dw8CBwXj8OAl7997Chx86c30wIiJSG//JnJckziFUUmVkyDFpUgjatduMx4+z17wzNTVQbhMREamDFaG8JPLWWEkUGfkUvr67ERERq2xr27YqNm3qgcqVrSSMjIiISismQnlhRahEEUJg1aowTJx4CGlp2Z3YDQ31MGdOO/j7e0JPT4dGNBIRkUYxEcpLTkVIzwAwryhtLDru+fNUDB78B/buvaVsq1WrHAIDe6FhQ3sJIyMiIm3APkJ5yakIWVQG9LgquZQMDPRw9eoT5XM/Pw+EhY1kEkRERBrBROhNGcnAy//WS2P/IMlZW5tg69ZesLe3wJ49/bF8+UcwMzOUOiwiItISvDX2ppwZpQH2D5LA5cuxKFvWFI6Or9a0a9myCu7eHQcTE/7vSkREmiV5RWjFihWoWrUqTExM4O7ujlOnTuV77O7du9GhQweUL18eVlZW8PT0xMGDBzUbECdTlIRCIbBw4Vk0abIWAwcGQy5XqOxnEkREREVB0kRox44dGD9+PKZOnYqIiAh4eXmhS5cuiIqKyvP4kydPokOHDti3bx/Cw8PRpk0bdO3aFREREZoLKpEjxopbdHQSOnfeigkTDiEjQ44TJx5g/XoNfqdERET5kAkJFxFr2rQpGjVqhJUrVyrbatWqhR49emDOnDkFeo06deqgb9++mDZtWoGOT0xMhLW1NRISEmBllcfcM6e/A87Pzt7utQ+o2qVAr0uFExwciREj9uD58zRl28SJnpg9uy2MjVkFIiKibO/8/S4kyX5pMjIyEB4ejsmTJ6u0d+zYEaGhoQV6DYVCgaSkJJQtWzbfY9LT05Genq58npiY+PYX5RxCxSIlJQP+/gcREHBR2ebgYIlNm3qgfXsXCSMjIiJdItmtsWfPnkEul8POzk6l3c7ODrGxsfmcpWrBggVISUlBnz598j1mzpw5sLa2Vj4cHR3f/qIqt8becSwVSlhYNBo1WqOSBPXqVQtXroxmEkRERMVK8s7SMpnqrMBCiFxtedm2bRt++OEH7NixAxUq5L/Y5pQpU5CQkKB8PHz4MN9jAbyqCBnbAMZctkHT7t79F56e63Dr1nMAgLm5Idat64adOz+Fra2ZxNEREZGukSwRKleuHPT19XNVf+Li4nJVid60Y8cODBs2DL/++ivat2//1mONjY1hZWWl8siXQg4kPcre5qrzRcLFpQyGDWsIAGjc2AEREaMwdGjDAiW/REREmiZZImRkZAR3d3eEhISotIeEhKB58+b5nrdt2zYMHjwYQUFB+OijjzQbVOoTQJGZvc3+QUVmwYKOmD+/A86cGYoaNWylDoeIiHSYpLfGJkyYgLVr12L9+vWIjIyEv78/oqKiMHr0aADZt7U+++wz5fHbtm3DZ599hgULFqBZs2aIjY1FbGwsEhISNBMQV53XqMTEdHz2WTA2bFAdCm9uboSJE5vD0JDLlxARkbQkHZ/ct29fPH/+HDNnzkRMTAzq1q2Lffv2wckp+7ZUTEyMypxCq1evRlZWFj7//HN8/vnnyvZBgwZh48aN7x8QR4xpTGjoQwwYsBv37r1AcPBNeHk5oXr1/Ef3ERERSUHyiVr8/Pzg5+eX5743k5vjx48XbTCsCL23rCwFZs06gR9/PAWFInuKKj09GW7fjmciREREJY7kiVCJworQe7lzJx6+vrtx/vxjZVvLllWwZUtPODvbSBcYERFRPpgIvY4VoUIRQmDTpsv48sv9SE7OAADo68swY8aHmDy5JfT1JZ+lgYiIKE9MhF6XUxGS6QPm9tLGUkr8+28aRo7ci507byjbqlUrg6Cg3mjSpJKEkREREb0bE6HX5VSELCsDehzRVBAKhUBo6KtJKocNa4hFizrDwsJIwqiIiIgKhvcscmSmAC+zZztm/6CCs7U1w6ZNPWBra4qdOz/F2rXdmAQREVGpwYpQDvYPKpDIyKcoW9YUdnYWyrb27V1w7944WFoaSxgZERGR+lgRysERY28lhMCqVWFwd1+DIUP+gBBCZT+TICIiKo2YCOVgRShfcXEp6N59O8aM+R/S0rKwf/9tbNp0WeqwiIiI3htvjeVgRShPBw7cxuDBv+PJkxRlm5+fB/r0qSNhVERERJrBRCiHSkWIK8+npWVi8uTDWLLkgrKtfHkzrF/fHR9/7CphZERERJrDRCiHSkXIUbo4SoCrV5/Ax2c3rl2LU7Z5e9fA+vXdVDpJExERlXZMhHLkVISMbQBjK0lDkdLt2/Hw8AhARoYcAGBiYoD58zvAz68xZDKZxNERERFpFjtLA4BQAEn/TQqo4x2lq1cvi759s/v/uLnZITx8JD7/vAmTICIi0kqsCAFAyhNAkZm9zY7SWLbMGzVqlMWkSS1gbMz/RYiISHuxIgTo7IixlJQMjBy5Bzt2XFNpt7Iyxvfft2YSREREWo+/dIBOziEUFhYNX9/duHXrOX777QaaN3eEo6O11GEREREVK1aEAJ2qCMnlCsyZcwqenutw61b22moZGXJcufJE4siIiIiKHytCgM5UhKKiEjBwYDBOnnygbGvc2AGBgb1Qo4athJERERFJg4kQoBMVoe3br2H06L1ISEgHAMhkwLffemH69NYwNNSXODoiIiJpMBECgMT/KiQyfcDCXtpYNCwxMR1ffLEPW7ZcUbZVqWKNrVt7wsuLM2gTEZFuYyIEvLo1ZlEJ0NOujyQ1NRP7999WPu/fvy5WrPgINjYmEkZFRERUMrCzdGYK8DK707A29g+qWNEC69Z1g5WVMbZu7YmgoN5MgoiIiP6jXeWPwkh8+GpbC/oH3b4djzJlTGBra6Zs69btA9y7Nw5ly5pKGBkREVHJw4pQknasOi+EwIYNEWjQYBVGjdoLIYTKfiZBREREuTER0oKh8/HxaejTZyeGDv0TKSmZ2LUrEtu2XXv3iURERDqOt8ZK+dD5Y8fuYeDAYDx+nKRsGzasIbp1+0DCqIiIiEoHJkJJpbMilJEhx3ffHcX8+aHIuQtWpowJAgK6onfv2tIGR0REVEowEUosfRWhmzefwcdnFyIiYpVtbdtWxaZNPVC5spWEkREREZUuTIRyKkLG1oBxyU8i/v77GRo1Wo20tCwAgKGhHubMaQd/f0/o6ckkjo6IiKh00e3O0kIBJP03fL6UVINcXW3RpUsNAECtWuVw4cIITJzYnEkQERFRIeh2RSg1DpBnZG+Xkv5BMpkMa9Z8DFfXsvj++9YwMzOUOiQiIqJSS7cToRLePygtLRPffHMYHTq4oGvXV6PAbG3NMGdOewkjI9IdQghkZWVBLpdLHQqR1jM0NIS+fvEuBK7biVAJHjp/+XIsfH134/r1p9i27RquXh2DihUtpA6LSKdkZGQgJiYGqampUodCpBNkMhkqV64MC4vi+73T7UQoZ9V5oMTcGlMoBBYvPofJk48gIyP7X6DJyRkIC4vGxx+7Shwdke5QKBS4d+8e9PX14eDgACMjI8hk7ItHVFSEEHj69CkePXqEGjVqFFtlSMcToZJVEYqOTsLgwb8jJOSuss3NzQ5BQb1Ru3Z5CSMj0j0ZGRlQKBRwdHSEmZnZu08govdWvnx53L9/H5mZmUyEikUJmkwxODgSI0bswfPnacq2iRM9MXt2Wxgb6/bXRCQlPT3dHlxLVJykqLrq9i9sTkVIpg9YOEgSQnJyBvz9D2Dt2ghlm4ODJTZt6oH27V0kiYmIiEhX6HYilFMRsqgE6EnzUfz7bxp+++2G8nnPnjURENAVtrYsxRMRERU13a35ZqYCac+ytyW8LeboaI3Vqz+Gubkh1q7til27+jAJIiIqAVq1aoWgoCCpw9Aa6enpqFKlCsLDw6UORYXuJkJJj19tF2NH6aioBCQmpqu09e1bF7dvj8WwYY04KoWINCI2NhZffvklXFxcYGxsDEdHR3Tt2hVHjhyROrQ83b9/HzKZTPmwtrZGs2bNsGfPnlzHpqWlYfr06fjggw9gbGyMcuXK4ZNPPsH169dzHZuYmIipU6eiZs2aMDExQcWKFdG+fXvs3r0bImfF6jzs3bsXsbGx6NevX659P/30E/T19fHzzz/n2vfDDz+gQYMGudpfvHgBmUyG48ePq7Tv2rULH374IaytrWFhYYH69etj5syZiI+Pzze295Weno4vv/wS5cqVg7m5Obp164ZHjx699ZykpCSMHz8eTk5OMDU1RfPmzfHXX3+pHPPDDz+gZs2aMDc3R5kyZdC+fXucP39eud/Y2BhfffUVvvnmmyJ5X4Wlw4nQw1fbxVQR2r79GurXX4kvv9yfax/nCCIiTbl//z7c3d1x9OhRzJ07F1evXsWBAwfQpk0bfP7554V+3ZzJJYvS4cOHERMTg/Pnz6NJkybo3bs3rl27ptyfnp6O9u3bY/369Zg1axZu3bqFffv2QS6Xo2nTpjh37pzy2BcvXqB58+bYvHkzpkyZgosXL+LkyZPo27cvJk2ahISEhHzjWLJkCYYMGZJnZ/kNGzZg0qRJWL9+/Xu916lTp6Jv375o3Lgx9u/fj2vXrmHBggW4fPkytmzZ8l6v/Tbjx49HcHAwtm/fjtOnTyM5ORkff/zxWycNHT58OEJCQrBlyxZcvXoVHTt2RPv27fH48auigqurK5YtW4arV6/i9OnTcHZ2RseOHfH06VPlMb6+vjh16hQiIyOL7P2pTeiYhIQEAUAkhC4VYj6yHxEriviaL8XAgbsF8IPysXPn9SK9JhG9n7S0NHHjxg2RlpYmdShq69Kli6hUqZJITk7Ote/ff/8VQghx7949AUBERESo7AMgjh07JoQQ4tixYwKAOHDggHB3dxeGhoZi1apVAoCIjIxUed0FCxYIJycnoVAoRFZWlhg6dKhwdnYWJiYmwtXVVSxatOitMecVT2JiogAglixZomz7+eefhUwmE5cuXVI5Xy6XCw8PD1G7dm2hUCiEEEKMGTNGmJubi8ePH+e6XlJSksjMzMwzlqdPnwqZTCauXbuWa9/x48dFpUqVREZGhnBwcBAnTpxQ2T99+nTh5uaW67w3P9vz588LAPl+Ljnfk6a9ePFCGBoaiu3btyvbHj9+LPT09MSBAwfyPCc1NVXo6+uLvXv3qrS7ubmJqVOn5nutnN/bw4cPq7R/+OGH4vvvv8/znLf9uVP+fick5HvNwtDdztJJr5UBi7AidOZMFAYMCMb9+y+Ubf3710W7dhwRRlQqbfUAUmKL/7rmFYEBYe88LD4+HgcOHMDs2bNhbm6ea7+NjY3al540aRLmz58PFxcX2NjYICAgAIGBgZg1a5bymKCgIPj4+EAmk0GhUKBy5cr49ddfUa5cOYSGhmLkyJGwt7dHnz59CnTNzMxMBAQEAMheduH163To0AFubm4qx+vp6cHf3x++vr64fPky6tevj+3bt8PX1xcODrlHBb9t5uLTp0/DzMwMtWrVyrVv3bp16N+/PwwNDdG/f3+sW7cOrVq1KtB7el1gYCAsLCzg5+eX5/63fU916tTBgwcP8t3v5OSU521CAAgPD0dmZiY6duyobHNwcEDdunURGhqKTp065TonZ4kZExMTlXZTU1OcPn06z+tkZGRgzZo1sLa2zvVdNWnSBKdOnco3/uKmw4nQa7fGiqCPUGamHLNmncTs2aegUGTfh7ayMsaKFd7w9a2v8esRUTFJiQWSH7/7OIncvn0bQgjUrFlTY685c+ZMdOjQQfnc19cXy5YtUyZCt27dQnh4ODZv3gwgO3GZMWOG8viqVasiNDQUv/766zsToebNm0NPTw9paWlQKBRwdnZWOefWrVto06ZNnufmJC63bt2Cg4MD/v3330J9Dvfv34ednV2u22KJiYnYtWsXQkNDAQADBgxAixYtsHTpUlhZWal1jX/++QcuLi4qSV5B7du3D5mZmfnuf9trxsbGwsjICGXKlFFpt7OzQ2xs3gm+paUlPD09MWvWLNSqVQt2dnbYtm0bzp8/jxo1aqgcu3fvXvTr1w+pqamwt7dHSEgIypUrp3JMpUqVcP/+/Xe8y+Kju4lQctFVhG7fjseAAbtx/vyrvyxbtHDE1q294Oxso9FrEVExM69Yoq8r/usArMmBFx4eHirP+/Xrh6+//hrnzp1Ds2bNEBgYiAYNGqB27drKY1atWoW1a9fiwYMHSEtLQ0ZGRp6diN+0Y8cO1KxZE7du3cL48eOxatUqlC1btkBxvv7e3+dzSEtLy1X9ALKrUS4uLsoKR4MGDeDi4oLt27dj5MiRal1DCFHo78jJyalQ573Nu+LZsmULhg4dikqVKkFfXx+NGjWCj48PLl68qHJcmzZtcOnSJTx79gwBAQHo06cPzp8/jwoVKiiPMTU1LVHr9+luIpRza8zICjC21tjLRkY+RePGAUhJyc7W9fVl+OGHDzF5cksYGOhu33QirVGA21NSqlGjBmQyGSIjI9GjR498j8updojXRk7lV2V48xabvb092rRpg6CgIDRr1gzbtm3DqFGjlPt//fVX+Pv7Y8GCBfD09ISlpSXmzZunMoIoP46OjqhRowZq1KgBCwsL9O7dGzdu3FD+kLq6uuLGjRt5nnvz5k3lZ1C+fHmUKVOmUJ1yy5Urh3///TdX+/r163H9+nUYGLz66VQoFFi3bp0yEbKyssqzE/aLFy8AANbW1sr3cfr0aWRmZqpdFXqfW2MVK1ZERkYG/v33X5WqUFxcHJo3b57va1arVg0nTpxASkoKEhMTYW9vj759+6Jq1aoqx5mbm6N69eqoXr06mjVrhho1amDdunWYMmWK8pj4+HiUL19ylo3S3V/m5P9ujWm4GlSzZjl4eWVn69WqlcGZM0Px3XetmAQRUbEoW7YsOnXqhOXLlyMlJSXX/pwf5JwfopiYGOW+S5cuFfg6vr6+2LFjB86ePYs7d+6oDDM/deoUmjdvDj8/PzRs2BDVq1fHnTt31H4vrVu3Rt26dTF79mxlW79+/XD48GFcvnxZ5ViFQoGFCxeidu3acHNzg56eHvr27YvAwEBER0fneu2UlJR8R8A1bNgQsbGxKsnQ1atXERYWhuPHj+PSpUvKx8mTJ/HXX38pR7bVrFkTjx49ynWb6a+//oKenh6qV68OAPDx8UFycjJWrFiRZww531Ne9u3bpxLDm499+/ble667uzsMDQ0REhKibIuJicG1a9femgjlMDc3h729Pf79918cPHgQ3bt3f+vxQgikp6tOGXPt2jU0bNjwndcqNhrtel0KKHud//jfiLFd3hq/RkxMkhg3br9ISkrX+GsTUfEozaPG7t69KypWrChq164tdu7cKW7duiVu3LghFi9eLGrWrKk8rlmzZsLLy0tcv35dnDhxQjRp0iTPUWN5jWBKSEgQJiYmws3NTbRr105l36JFi4SVlZU4cOCA+Pvvv8V3330nrKys8hxNlSOvUWNCCPHnn38KY2Nj8ejRIyFE9vfStGlT4ejoKH799Vfx4MEDceHCBdGjRw9hbm4uzp49qzw3Pj5e1KxZU1SuXFls2rRJXL9+Xdy6dUusW7dOVK9ePd+RWVlZWaJChQpiz549yrZx48aJpk2b5nl88+bNxfjx44UQQmRmZop69eqJ1q1bi9OnT4u7d++K33//XVSpUkX4+fmpnDdp0iShr68vvv76axEaGiru378vDh8+LD755JN3jrJ7H6NHjxaVK1cWhw8fFhcvXhRt27YVbm5uIisrS3lM27ZtxdKlS5XPDxw4IPbv3y/u3r0rDh06JNzc3ESTJk1ERkaGEEKI5ORkMWXKFHH27Flx//59ER4eLoYNGyaMjY1zjb5zcnISmzdvzjM2KUaNMREKGV3o10pPzxKTJh0SISF3NBghEZUEpTkREkKI6Oho8fnnnwsnJydhZGQkKlWqJLp166ZMcoQQ4saNG6JZs2bC1NRUNGjQQBw6dKjAiZAQQnz66acCgFi/fr1K+8uXL8XgwYOFtbW1sLGxEWPGjBGTJ08uVCKkUCjEBx98IMaMGaNsS0lJEd99952oXr26MDQ0FGXLlhW9e/cWV69ezfW6L168EJMnTxY1atQQRkZGws7OTrRv314EBwcrh9nnZfLkyaJfv35CCCHS09OFra2tmDt3bp7HLliwQJQrV06kp2f/4zcmJkYMGTJEODk5CVNTU1GzZk0xc+ZM8fLly1zn7tixQ7Rq1UpYWloKc3NzUb9+fTFz5swiGz4vRPb/21988YUoW7asMDU1FR9//LGIiopSOcbJyUlMnz5dJU4XFxdhZGQkKlasKD7//HPx4sULldfs2bOncHBwEEZGRsLe3l5069ZNXLhwQeV1Q0NDhY2NjUhNTc03tuJOhGRCvGVqTS2UmJgIa2trJPwIWJkAaPkT0HTKO897082bz+DjswsREbFwcLDElSujuTQGkRZ5+fIl7t27h6pVq+bZcZa025MnT1CnTh2Eh4cXSedkXfXpp5+iYcOG+Pbbb/Pc/7Y/d8rf74QEtUfpvQ07rqjZR0gIgVWrwtCo0WpERGTfA376NAWhoQ/fcSYREZUWdnZ2WLduHaKioqQORWukp6fDzc0N/v7+UoeiQndHjeWwLHimHxeXguHD/8SePbeUbbVqlUNQUG80aCDRkFoiIioS7+oITOoxNjbGd999J3UYuTARKmBF6MCB2xg8+Hc8efJqFIafnwfmzesIMzP1J8QiIiIi6el2IiTTAyxyT73+urS0TEyefBhLllxQtpUvb4b167vj449dizpCIiIiKkK6nQhZVAL03v4RREcnYd26COVzb+8aWL++G+zsuFo8kS7QsfEkRJKS4s+bbneWLsAaY9WqlcWSJV1gYmKAZcu6YO/e/kyCiHRAzmy/JWkpACJtl5GRAQDQ19cvtmvqdkUoj/5B0dFJsLExUen3M2RIA7RrVxVOTjbFGBwRSUlfXx82NjaIi4sDAJiZmWl0/S4iUqVQKPD06VOYmZmpLGNS1HQ7EXqjIhQcHIkRI/bg009rY+XKj5XtMpmMSRCRDqpYMXs0aE4yRERFS09PD1WqVCnWf3TodiL0X0UoOTkD/v4HsHZtdl+gVavC8dFHruwMTaTjZDIZ7O3tUaFChXwXJCUizTEyMlIuCFxcJE+EVqxYgXnz5iEmJgZ16tTBokWL4OXlle/xJ06cwIQJE3D9+nU4ODhg0qRJGD16dOEublkFf/31GL6+u/HPP/HK5p49a8LTs3LhXpOItI6+vn6x9lkgouIjaWfpHTt2YPz48Zg6dSoiIiLg5eWFLl265DuT57179+Dt7Q0vLy9ERETg22+/xdixY7Fr1y61ry1XAHM2pKF58/XKJMjMzBBr13bFrl19uFwGERGRDpB0rbGmTZuiUaNGWLlypbKtVq1a6NGjB+bMmZPr+G+++QZ//vknIiMjlW2jR4/G5cuXcfbs2QJdM2etkubOPgi9/+rWV+PGDggM7IUaNWzf4x0RERFRUdC6tcYyMjIQHh6Ojh07qrR37NgRoaGheZ5z9uzZXMd36tQJYWFhat+/D72f3T9IT0+GqVO9cObMUCZBREREOkayPkLPnj2DXC6HnZ2dSrudnR1iY2PzPCc2NjbP47OysvDs2TPY29vnOic9PR3p6enK5wkJCTl7ULmyNQICPkbz5lWQlpaCtLT3e09ERERUNBITEwFoftJFyTtLvzlETgjx1mFzeR2fV3uOOXPmYMaMGXnsWYhHj4AuXaaoFzARERFJ5vnz57C2ttbY60mWCJUrVw76+vq5qj9xcXG5qj45KlasmOfxBgYGsLXN+7bWlClTMGHCBOXzFy9ewMnJCVFRURr9IKlwEhMT4ejoiIcPH2r0ni+pj99FycHvouTgd1FyJCQkoEqVKihbtqxGX1eyRMjIyAju7u4ICQlBz549le0hISHo3r17nud4enpiz549Km2HDh2Ch4eHcjr8NxkbG8PY2DhXu7W1Nf+nLkGsrKz4fZQQ/C5KDn4XJQe/i5JD0/MMSTp8fsKECVi7di3Wr1+PyMhI+Pv7IyoqSjkv0JQpU/DZZ58pjx89ejQePHiACRMmIDIyEuvXr8e6devw1VdfSfUWiIiIqBSTtI9Q37598fz5c8ycORMxMTGoW7cu9u3bBycnJwBATEyMypxCVatWxb59++Dv74/ly5fDwcEBS5YsQe/evaV6C0RERFSKSd5Z2s/PD35+fnnu27hxY6621q1b4+LFi4W+nrGxMaZPn57n7TIqfvw+Sg5+FyUHv4uSg99FyVFU34WkEyoSERERSUnSPkJEREREUmIiRERERDqLiRARERHpLCZCREREpLO0MhFasWIFqlatChMTE7i7u+PUqVNvPf7EiRNwd3eHiYkJXFxcsGrVqmKKVPup813s3r0bHTp0QPny5WFlZQVPT08cPHiwGKPVfur+2chx5swZGBgYoEGDBkUboA5R97tIT0/H1KlT4eTkBGNjY1SrVg3r168vpmi1m7rfRWBgINzc3GBmZgZ7e3sMGTIEz58/L6ZotdfJkyfRtWtXODg4QCaT4ffff3/nORr5/RZaZvv27cLQ0FAEBASIGzduiHHjxglzc3Px4MGDPI+/e/euMDMzE+PGjRM3btwQAQEBwtDQUOzcubOYI9c+6n4X48aNE//3f/8nLly4IG7duiWmTJkiDA0NxcWLF4s5cu2k7veR48WLF8LFxUV07NhRuLm5FU+wWq4w30W3bt1E06ZNRUhIiLh37544f/68OHPmTDFGrZ3U/S5OnTol9PT0xOLFi8Xdu3fFqVOnRJ06dUSPHj2KOXLts2/fPjF16lSxa9cuAUAEBwe/9XhN/X5rXSLUpEkTMXr0aJW2mjVrismTJ+d5/KRJk0TNmjVV2kaNGiWaNWtWZDHqCnW/i7zUrl1bzJgxQ9Oh6aTCfh99+/YV3333nZg+fToTIQ1R97vYv3+/sLa2Fs+fPy+O8HSKut/FvHnzhIuLi0rbkiVLROXKlYssRl1UkERIU7/fWnVrLCMjA+Hh4ejYsaNKe8eOHREaGprnOWfPns11fKdOnRAWFobMzMwii1XbFea7eJNCoUBSUpLGF9jTRYX9PjZs2IA7d+5g+vTpRR2izijMd/Hnn3/Cw8MDc+fORaVKleDq6oqvvvoKaWlpxRGy1irMd9G8eXM8evQI+/btgxACT548wc6dO/HRRx8VR8j0Gk39fks+s7QmPXv2DHK5PNfq9XZ2drlWrc8RGxub5/FZWVl49uwZ7O3tiyxebVaY7+JNCxYsQEpKCvr06VMUIeqUwnwf//zzDyZPnoxTp07BwECr/qqQVGG+i7t37+L06dMwMTFBcHAwnj17Bj8/P8THx7Of0HsozHfRvHlzBAYGom/fvnj58iWysrLQrVs3LF26tDhCptdo6vdbqypCOWQymcpzIUSutncdn1c7qU/d7yLHtm3b8MMPP2DHjh2oUKFCUYWncwr6fcjlcvj4+GDGjBlwdXUtrvB0ijp/NhQKBWQyGQIDA9GkSRN4e3vjl19+wcaNG1kV0gB1vosbN25g7NixmDZtGsLDw3HgwAHcu3dPuVg4FS9N/H5r1T/zypUrB319/VyZfFxcXK6sMUfFihXzPN7AwAC2trZFFqu2K8x3kWPHjh0YNmwYfvvtN7Rv374ow9QZ6n4fSUlJCAsLQ0REBL744gsA2T/GQggYGBjg0KFDaNu2bbHErm0K82fD3t4elSpVgrW1tbKtVq1aEELg0aNHqFGjRpHGrK0K813MmTMHLVq0wNdffw0AqF+/PszNzeHl5YUff/yRdxGKkaZ+v7WqImRkZAR3d3eEhISotIeEhKB58+Z5nuPp6Znr+EOHDsHDwwOGhoZFFqu2K8x3AWRXggYPHoygoCDec9cgdb8PKysrXL16FZcuXVI+Ro8ejQ8++ACXLl1C06ZNiyt0rVOYPxstWrRAdHQ0kpOTlW23bt2Cnp4eKleuXKTxarPCfBepqanQ01P96dTX1wfwqhpBxUNjv99qda0uBXKGQq5bt07cuHFDjB8/Xpibm4v79+8LIYSYPHmyGDhwoPL4nOF3/v7+4saNG2LdunUcPq8h6n4XQUFBwsDAQCxfvlzExMQoHy9evJDqLWgVdb+PN3HUmOao+10kJSWJypUri08++URcv35dnDhxQtSoUUMMHz5cqregNdT9LjZs2CAMDAzEihUrxJ07d8Tp06eFh4eHaNKkiVRvQWskJSWJiIgIERERIQCIX375RURERCinMiiq32+tS4SEEGL58uXCyclJGBkZiUaNGokTJ04o9w0aNEi0bt1a5fjjx4+Lhg0bCiMjI+Hs7CxWrlxZzBFrL3W+i9atWwsAuR6DBg0q/sC1lLp/Nl7HREiz1P0uIiMjRfv27YWpqamoXLmymDBhgkhNTS3mqLWTut/FkiVLRO3atYWpqamwt7cXvr6+4tGjR8UctfY5duzYW38Diur3WyYEa3lERESkm7SqjxARERGROpgIERERkc5iIkREREQ6i4kQERER6SwmQkRERKSzmAgRERGRzmIiRERERDqLiRARFdjgwYPRo0cPya7/ww8/oEGDBpJdvyg5Oztj0aJFbz1Gm98/kVSYCBGVEDKZ7K2PwYMHSx2iRnz44Yd5vr+srCypQ3urjRs3qsRrb2+PPn364N69exp5/b/++gsjR45UPpfJZPj9999Vjvnqq69w5MgRjVyPiLJp1erzRKVZTEyMcnvHjh2YNm0a/v77b2WbqampFGEViREjRmDmzJkqbQYGJf+vIysrK/z9998QQuDmzZsYNWoUunXrhkuXLikX3iys8uXLv/MYCwsLWFhYvNd1iEgVK0JEJUTFihWVD2tra8hkMuVzQ0NDjB49GpUrV4aZmRnq1auHbdu2qZy/c+dO1KtXD6amprC1tUX79u2RkpICILva0KFDB5QrVw7W1tZo3bo1Ll68+NZ45HI5JkyYABsbG9ja2mLSpEm5VtcWQmDu3LlwcXGBqakp3NzcsHPnzne+VzMzM5X3W7FiRQDAN998A1dXV5iZmcHFxQXff/89MjMz832d48ePo0mTJjA3N4eNjQ1atGiBBw8eKPevXLkS1apVg5GRET744ANs2bJF5fwffvgBVapUgbGxMRwcHDB27Ni3xp3zndjb26NNmzaYPn06rl27htu3b7/39V6/Nebs7AwA6NmzJ2QymfL567fGDh48CBMTE7x48ULlGmPHjkXr1q2Vz3ft2oU6derA2NgYzs7OWLBggcrxK1asQI0aNWBiYgI7Ozt88sknb/0MiLQNEyGiUuDly5dwd3fH3r17ce3aNYwcORIDBw7E+fPnAWRXk/r374+hQ4ciMjISx48fR69evZSJS1JSEgYNGoRTp07h3LlzqFGjBry9vZGUlJTvNRcsWID169dj3bp1OH36NOLj4xEcHKxyzHfffYcNGzZg5cqVuH79Ovz9/TFgwACcOHGiUO/T0tISGzduxI0bN7B48WIEBARg4cKFeR6blZWFHj16oHXr1rhy5QrOnj2LkSNHQiaTAQCCg4Mxbtw4TJw4EdeuXcOoUaMwZMgQHDt2DEB24rhw4UKsXr0a//zzD37//XfUq1dPrXhzqnSZmZkavd5ff/0FANiwYQNiYmKUz1/Xvn172NjYYNeuXco2uVyOX3/9Fb6+vgCA8PBw9OnTB/369cPVq1fxww8/4Pvvv8fGjRsBAGFhYRg7dixmzpyJv//+GwcOHECrVq3U+gyISr33WiqWiIrEhg0bhLW19VuP8fb2FhMnThRCCBEeHi4AiPv37xfo9bOysoSlpaXYs2dPvsfY29uLn3/+Wfk8MzNTVK5cWXTv3l0IIURycrIwMTERoaGhKucNGzZM9O/fP9/Xbd26tTA0NBTm5ubKx4QJE/I8du7cucLd3V35fPr06cLNzU0IIcTz588FAHH8+PE8z23evLkYMWKEStunn34qvL29hRBCLFiwQLi6uoqMjIx8Y33dm9/Jw4cPRbNmzUTlypVFenr6e1/PyclJLFy4UPkcgAgODlY55vX3L4QQY8eOFW3btlU+P3jwoDAyMhLx8fFCCCF8fHxEhw4dVF7j66+/FrVr1xZCCLFr1y5hZWUlEhMTC/QZEGkjVoSISgG5XI7Zs2ejfv36sLW1hYWFBQ4dOoSoqCgAgJubG9q1a4d69erh008/RUBAAP7991/l+XFxcRg9ejRcXV1hbW0Na2trJCcnK89/U0JCAmJiYuDp6alsMzAwgIeHh/L5jRs38PLlS3To0EHZd8XCwgKbN2/GnTt33vp+fH19cenSJeVjypQpALKrJi1btkTFihVhYWGB77//Pt8Yy5Yti8GDB6NTp07o2rUrFi9erNLPKjIyEi1atFA5p0WLFoiMjAQAfPrpp0hLS4OLiwtGjBiB4ODgd3bYTkhIgIWFBczNzeHo6IiMjAzs3r0bRkZGRXK9d/H19cXx48cRHR0NAAgMDIS3tzfKlCnz1s/gn3/+gVwuR4cOHeDk5AQXFxcMHDgQgYGBSE1Nfa+YiEobJkJEpcCCBQuwcOFCTJo0CUePHsWlS5fQqVMnZGRkAAD09fUREhKC/fv3o3bt2li6dCk++OAD5YimwYMHIzw8HIsWLUJoaCguXboEW1tb5fmFoVAoAAD/+9//VJKaGzduvLOfkLW1NapXr658lCtXDufOnUO/fv3QpUsX7N27FxEREZg6depbY9ywYQPOnj2L5s2bY8eOHXB1dcW5c+eU+3Nuk+UQQijbHB0d8ffff2P58uUwNTWFn58fWrVq9dY+SZaWlrh06RKuXr2K5ORkhIeHo3HjxkV2vXdp0qQJqlWrhu3btyMtLQ3BwcEYMGBAntd/ve3193Px4kVs27YN9vb2mDZtGtzc3HL1OyLSZkyEiEqBU6dOoXv37hgwYADc3Nzg4uKCf/75R+UYmUyGFi1aYMaMGYiIiICRkZGyT8+pU6cwduxYeHt7KzvOPnv2LN/rWVtbw97eXiWpyMrKQnh4uPJ57dq1YWxsjKioKJWkpnr16nB0dFT7PZ45cwZOTk6YOnUqPDw8UKNGDZWOz/lp2LAhpkyZgtDQUNStWxdBQUEAgFq1auH06dMqx4aGhqJWrVrK56ampujWrRuWLFmC48eP4+zZs7h69Wq+19LT00P16tXh4uICc3NzlX2avp6hoSHkcvk737+Pjw8CAwOxZ88e6Onp4aOPPlLuq127dp4xubq6Kke5GRgYoH379pg7dy6uXLmC+/fv4+jRo++8LpG2KPnjVYkI1atXx65duxAaGooyZcrgl19+QWxsrPJH9vz58zhy5Ag6duyIChUq4Pz583j69Klyf/Xq1bFlyxZ4eHggMTERX3/99TuH448bNw4///wzatSogVq1auGXX35RqRRYWlriq6++gr+/PxQKBVq2bInExESEhobCwsICgwYNUvs9RkVFYfv27WjcuDH+97//5eqc/bp79+5hzZo16NatGxwcHPD333/j1q1b+OyzzwAAX3/9Nfr06YNGjRqhXbt22LNnD3bv3o3Dhw8DyJ4XSC6Xo2nTpjAzM8OWLVtgamoKJycnteLOoenrOTs748iRI2jRogWMjY2Vt7ve5OvrixkzZmD27Nn45JNPYGJiotw3ceJENG7cGLNmzULfvn1x9uxZLFu2DCtWrAAA7N27F3fv3kWrVq1QpkwZ7Nu3DwqFAh988EGhPgOiUknaLkpElJc3O+Y+f/5cdO/eXVhYWIgKFSqI7777Tnz22WfKjss3btwQnTp1EuXLlxfGxsbC1dVVLF26VHn+xYsXhYeHhzA2NhY1atQQv/32W67OuW/KzMwU48aNE1ZWVsLGxkZMmDBB5ZpCCKFQKMTixYvFBx98IAwNDUX58uVFp06dxIkTJ/J93datW4tx48blue/rr78Wtra2wsLCQvTt21csXLhQ5XN4vbNwbGys6NGjh7C3txdGRkbCyclJTJs2TcjlcuXxK1asEC4uLsLQ0FC4urqKzZs3K/cFBweLpk2bCisrK2Fubi6aNWsmDh8+nG/cBenA/j7Xe/P7+PPPP0X16tWFgYGBcHJyyvX+X9e4cWMBQBw9ejTXvp07d4ratWsLQ0NDUaVKFTFv3jzlvlOnTonWrVuLMmXKCFNTU1G/fn2xY8eOt75HIm0jE+KNiUGIiIiIdAT7CBEREZHOYiJEREREOouJEBEREeksJkJERESks5gIERERkc5iIkREREQ6i4kQERER6SwmQkRERKSzmAgRERGRzmIiRERERDqLiRARERHpLCZCREREpLP+Hwjuij7tt8xOAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.890577</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.889909</td>\n",
              "      <td>0.607662</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.558828</td>\n",
              "      <td>0.888374</td>\n",
              "      <td>147</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>177</td>\n",
              "      <td>85.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928693</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928604</td>\n",
              "      <td>0.733959</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.712060</td>\n",
              "      <td>0.928502</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>184</td>\n",
              "      <td>90.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977957</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977956</td>\n",
              "      <td>0.913688</td>\n",
              "      <td>0.022039</td>\n",
              "      <td>0.910386</td>\n",
              "      <td>0.977358</td>\n",
              "      <td>462</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>603</td>\n",
              "      <td>97.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.929217</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928441</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>0.926976</td>\n",
              "      <td>154</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>184</td>\n",
              "      <td>90.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942345</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942262</td>\n",
              "      <td>0.781936</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.767433</td>\n",
              "      <td>0.941128</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>189</td>\n",
              "      <td>92.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945198</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945012</td>\n",
              "      <td>0.791723</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.779414</td>\n",
              "      <td>0.944187</td>\n",
              "      <td>159</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>185</td>\n",
              "      <td>92.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931298</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931301</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.068681</td>\n",
              "      <td>0.723135</td>\n",
              "      <td>0.930540</td>\n",
              "      <td>153</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>186</td>\n",
              "      <td>90.88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Model         Set  Accuracy  Precision    Recall  \\\n",
              "0          Regresion Lineal    Training  0.795225   0.825122  0.795225   \n",
              "1          Regresion Lineal  Validation  0.815934   0.827130  0.815934   \n",
              "2          Regresion Lineal        Test  0.793956   0.830307  0.793956   \n",
              "3       Regresión Logística    Training  0.835629   0.843677  0.835629   \n",
              "4       Regresión Logística  Validation  0.826923   0.828750  0.826923   \n",
              "5       Regresión Logística        Test  0.826923   0.839082  0.826923   \n",
              "6         Árbol de Decisión    Training  0.943067   0.943447  0.943067   \n",
              "7         Árbol de Decisión  Validation  0.854396   0.856113  0.854396   \n",
              "8         Árbol de Decisión        Test  0.854396   0.854704  0.854396   \n",
              "9             Random Forest    Training  1.000000   1.000000  1.000000   \n",
              "10            Random Forest  Validation  0.887363   0.889476  0.887363   \n",
              "11            Random Forest        Test  0.901099   0.901102  0.901099   \n",
              "12                      KNN    Training  0.852158   0.856418  0.852158   \n",
              "13                      KNN  Validation  0.785714   0.787289  0.785714   \n",
              "14                      KNN        Test  0.793956   0.799964  0.793956   \n",
              "15                      SVN    Training  0.846648   0.850097  0.846648   \n",
              "16                      SVN  Validation  0.821429   0.821337  0.821429   \n",
              "17                      SVN        Test  0.821429   0.826572  0.821429   \n",
              "18   Naive Bayes - Gaussian    Training  0.759412   0.831542  0.759412   \n",
              "19   Naive Bayes - Gaussian  Validation  0.766484   0.809282  0.766484   \n",
              "20   Naive Bayes - Gaussian        Test  0.758242   0.815378  0.758242   \n",
              "21  Naive Bayes - Bernoulli    Training  0.814509   0.820274  0.814509   \n",
              "22  Naive Bayes - Bernoulli  Validation  0.791209   0.791105  0.791209   \n",
              "23  Naive Bayes - Bernoulli        Test  0.826923   0.833036  0.826923   \n",
              "24               AdaBoost 1    Training  1.000000   1.000000  1.000000   \n",
              "25               AdaBoost 1  Validation  0.890110   0.890577  0.890110   \n",
              "26               AdaBoost 1        Test  0.928571   0.928693  0.928571   \n",
              "27               AdaBoost 2    Training  0.977961   0.977957  0.977961   \n",
              "28               AdaBoost 2  Validation  0.928571   0.929217  0.928571   \n",
              "29               AdaBoost 2        Test  0.942308   0.942345  0.942308   \n",
              "30        Gradient Boosting    Training  1.000000   1.000000  1.000000   \n",
              "31        Gradient Boosting  Validation  0.945055   0.945198  0.945055   \n",
              "32        Gradient Boosting        Test  0.931319   0.931298  0.931319   \n",
              "\n",
              "    F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC  \\\n",
              "0   0.794968             0.347828            0.204775   0.167334  0.809587   \n",
              "1   0.815546             0.397597            0.184066   0.261037  0.820425   \n",
              "2   0.791730             0.343680            0.206044   0.169405  0.806225   \n",
              "3   0.836301             0.450090            0.164371   0.331627  0.841128   \n",
              "4   0.827081             0.425939            0.173077   0.305154  0.828122   \n",
              "5   0.826964             0.425926            0.173077   0.302300  0.833120   \n",
              "6   0.943141             0.785002            0.056933   0.768496  0.943554   \n",
              "7   0.853779             0.501006            0.145604   0.415447  0.851362   \n",
              "8   0.854492             0.501008            0.145604   0.413046  0.853992   \n",
              "9   1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "10  0.886886             0.599091            0.112637   0.547799  0.884450   \n",
              "11  0.900990             0.642523            0.098901   0.601314  0.899355   \n",
              "12  0.852701             0.495592            0.147842   0.398838  0.855309   \n",
              "13  0.785909             0.324677            0.214286   0.139715  0.786595   \n",
              "14  0.794316             0.343845            0.206044   0.169405  0.797463   \n",
              "15  0.847172             0.480168            0.153352   0.376434  0.848994   \n",
              "16  0.821327             0.411647            0.178571   0.283095  0.820274   \n",
              "17  0.821762             0.411656            0.178571   0.280151  0.824662   \n",
              "18  0.754310             0.267385            0.240588   0.021711  0.784262   \n",
              "19  0.761488             0.282026            0.233516   0.062509  0.776460   \n",
              "20  0.752494             0.264371            0.241758   0.025435  0.774370   \n",
              "21  0.815240             0.395110            0.185491   0.245747  0.818109   \n",
              "22  0.791133             0.337388            0.208791   0.161773  0.790110   \n",
              "23  0.827225             0.425943            0.173077   0.302300  0.830686   \n",
              "24  1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "25  0.889909             0.607662            0.109890   0.558828  0.888374   \n",
              "26  0.928604             0.733959            0.071429   0.712060  0.928502   \n",
              "27  0.977956             0.913688            0.022039   0.910386  0.977358   \n",
              "28  0.928441             0.733961            0.071429   0.713238  0.926976   \n",
              "29  0.942262             0.781936            0.057692   0.767433  0.941128   \n",
              "30  1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "31  0.945012             0.791723            0.054945   0.779414  0.944187   \n",
              "32  0.931301             0.743431            0.068681   0.723135  0.930540   \n",
              "\n",
              "     TN  FP   FN   TP  Global Score  \n",
              "0   438  37  186  428         74.81  \n",
              "1   153  18   49  144         76.76  \n",
              "2   157   9   66  132         74.65  \n",
              "3   420  55  124  490         79.14  \n",
              "4   145  26   37  156         77.87  \n",
              "5   150  16   47  151         78.15  \n",
              "6   450  25   37  577         92.45  \n",
              "7   137  34   19  174         81.09  \n",
              "8   141  25   28  170         81.16  \n",
              "9   475   0    0  614        100.00  \n",
              "10  143  28   13  180         85.21  \n",
              "11  146  20   16  182         86.96  \n",
              "12  418  57  104  510         81.04  \n",
              "13  137  34   44  149         72.93  \n",
              "14  139  27   48  150         74.04  \n",
              "15  412  63  104  510         80.33  \n",
              "16  137  34   31  162         77.11  \n",
              "17  143  23   42  156         77.31  \n",
              "18  465  10  252  362         71.38  \n",
              "19  161  10   75  118         71.40  \n",
              "20  159   7   81  117         70.79  \n",
              "21  402  73  129  485         76.49  \n",
              "22  132  39   37  156         73.49  \n",
              "23  145  21   42  156         78.00  \n",
              "24  475   0    0  614        100.00  \n",
              "25  147  24   16  177         85.58  \n",
              "26  154  12   14  184         90.55  \n",
              "27  462  13   11  603         97.02  \n",
              "28  154  17    9  184         90.51  \n",
              "29  154  12    9  189         92.30  \n",
              "30  475   0    0  614        100.00  \n",
              "31  159  12    8  185         92.67  \n",
              "32  153  13   12  186         90.88  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model_name = \"Gradient Boosting\"\n",
        "print(model_name)\n",
        "\n",
        "# Configurar la búsqueda de hiperparámetros\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 250, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.07, 0.10, 0.5, 1.0],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "# Inicializar el modelo AdaBoost\n",
        "GradBoost = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=GradBoost, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Entrenar el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Mejores hiperparámetros: {best_params}\")\n",
        "\n",
        "# Mejor modelo\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(best_model, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "best_model.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = best_model.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"SI\")\n",
        "\n",
        "y_pred = best_model.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_pred, \"Validation\",model_name, print_roc = \"SI\")\n",
        "\n",
        "y_test_pred = best_model.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"SI\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Redes Neuronales Artificiales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\FloCr\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\FloCr\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "Artificial Network Layers (ANN)\n",
            "WARNING:tensorflow:From C:\\Users\\FloCr\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Epoch 1/25\n",
            "WARNING:tensorflow:From C:\\Users\\FloCr\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\FloCr\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "35/35 [==============================] - 1s 1ms/step - loss: 5929.9014 - accuracy: 0.5234\n",
            "Epoch 2/25\n",
            "35/35 [==============================] - 0s 969us/step - loss: 2179.5481 - accuracy: 0.6538\n",
            "Epoch 3/25\n",
            "35/35 [==============================] - 0s 972us/step - loss: 1444.1178 - accuracy: 0.6814\n",
            "Epoch 4/25\n",
            "35/35 [==============================] - 0s 917us/step - loss: 1068.6998 - accuracy: 0.6887\n",
            "Epoch 5/25\n",
            "35/35 [==============================] - 0s 954us/step - loss: 750.4091 - accuracy: 0.6896\n",
            "Epoch 6/25\n",
            "35/35 [==============================] - 0s 919us/step - loss: 471.7653 - accuracy: 0.6814\n",
            "Epoch 7/25\n",
            "35/35 [==============================] - 0s 942us/step - loss: 300.3000 - accuracy: 0.6621\n",
            "Epoch 8/25\n",
            "35/35 [==============================] - 0s 930us/step - loss: 273.1596 - accuracy: 0.6391\n",
            "Epoch 9/25\n",
            "35/35 [==============================] - 0s 926us/step - loss: 239.9661 - accuracy: 0.6400\n",
            "Epoch 10/25\n",
            "35/35 [==============================] - 0s 1ms/step - loss: 216.9518 - accuracy: 0.6391\n",
            "Epoch 11/25\n",
            "35/35 [==============================] - 0s 1ms/step - loss: 197.5101 - accuracy: 0.6410\n",
            "Epoch 12/25\n",
            "35/35 [==============================] - 0s 904us/step - loss: 191.5726 - accuracy: 0.6483\n",
            "Epoch 13/25\n",
            "35/35 [==============================] - 0s 916us/step - loss: 163.6693 - accuracy: 0.6318\n",
            "Epoch 14/25\n",
            "35/35 [==============================] - 0s 907us/step - loss: 137.0637 - accuracy: 0.6189\n",
            "Epoch 15/25\n",
            "35/35 [==============================] - 0s 957us/step - loss: 117.6355 - accuracy: 0.6501\n",
            "Epoch 16/25\n",
            "35/35 [==============================] - 0s 992us/step - loss: 110.9663 - accuracy: 0.6474\n",
            "Epoch 17/25\n",
            "35/35 [==============================] - 0s 915us/step - loss: 88.9863 - accuracy: 0.6703\n",
            "Epoch 18/25\n",
            "35/35 [==============================] - 0s 928us/step - loss: 80.0479 - accuracy: 0.6961\n",
            "Epoch 19/25\n",
            "35/35 [==============================] - 0s 973us/step - loss: 122.7103 - accuracy: 0.6520\n",
            "Epoch 20/25\n",
            "35/35 [==============================] - 0s 919us/step - loss: 77.1637 - accuracy: 0.6639\n",
            "Epoch 21/25\n",
            "35/35 [==============================] - 0s 978us/step - loss: 79.6920 - accuracy: 0.6529\n",
            "Epoch 22/25\n",
            "35/35 [==============================] - 0s 914us/step - loss: 79.3401 - accuracy: 0.6465\n",
            "Epoch 23/25\n",
            "35/35 [==============================] - 0s 911us/step - loss: 69.6205 - accuracy: 0.6309\n",
            "Epoch 24/25\n",
            "35/35 [==============================] - 0s 959us/step - loss: 61.5655 - accuracy: 0.6547\n",
            "Epoch 25/25\n",
            "35/35 [==============================] - 0s 930us/step - loss: 58.1824 - accuracy: 0.6520\n",
            "35/35 [==============================] - 0s 722us/step\n",
            "12/12 [==============================] - 0s 820us/step\n",
            "12/12 [==============================] - 0s 902us/step\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.4775\n",
            " - Precision: 0.9617\n",
            " - Recall: 0.4775\n",
            " - F1-Score: 0.6052\n",
            " - Adjusted Rand Index: -0.0120\n",
            " - Mean Squared Error: 0.5225\n",
            " - R-squared: -12.1895\n",
            " - Área bajo la curva : 0.727\n",
            " - Confusion Matrix: \n",
            "[[475 569]\n",
            " [  0  45]]\n",
            " - Global Score : 27.84\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.5110\n",
            " - Precision: 0.9620\n",
            " - Recall: 0.5110\n",
            " - F1-Score: 0.6365\n",
            " - Adjusted Rand Index: -0.0030\n",
            " - Mean Squared Error: 0.4890\n",
            " - R-squared: -11.3767\n",
            " - Área bajo la curva : 0.745\n",
            " - Confusion Matrix: \n",
            "[[171 178]\n",
            " [  0  15]]\n",
            " - Global Score : 31.87\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.4890\n",
            " - Precision: 0.9690\n",
            " - Recall: 0.4890\n",
            " - F1-Score: 0.6236\n",
            " - Adjusted Rand Index: -0.0067\n",
            " - Mean Squared Error: 0.5110\n",
            " - R-squared: -15.0284\n",
            " - Área bajo la curva : 0.736\n",
            " - Confusion Matrix: \n",
            "[[166 186]\n",
            " [  0  12]]\n",
            " - Global Score : 21.7\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.890577</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.889909</td>\n",
              "      <td>0.607662</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.558828</td>\n",
              "      <td>0.888374</td>\n",
              "      <td>147</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>177</td>\n",
              "      <td>85.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928693</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928604</td>\n",
              "      <td>0.733959</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.712060</td>\n",
              "      <td>0.928502</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>184</td>\n",
              "      <td>90.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977957</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977956</td>\n",
              "      <td>0.913688</td>\n",
              "      <td>0.022039</td>\n",
              "      <td>0.910386</td>\n",
              "      <td>0.977358</td>\n",
              "      <td>462</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>603</td>\n",
              "      <td>97.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.929217</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928441</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>0.926976</td>\n",
              "      <td>154</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>184</td>\n",
              "      <td>90.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942345</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942262</td>\n",
              "      <td>0.781936</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.767433</td>\n",
              "      <td>0.941128</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>189</td>\n",
              "      <td>92.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945198</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945012</td>\n",
              "      <td>0.791723</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.779414</td>\n",
              "      <td>0.944187</td>\n",
              "      <td>159</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>185</td>\n",
              "      <td>92.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931298</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931301</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.068681</td>\n",
              "      <td>0.723135</td>\n",
              "      <td>0.930540</td>\n",
              "      <td>153</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>186</td>\n",
              "      <td>90.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.961706</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.605211</td>\n",
              "      <td>-0.011995</td>\n",
              "      <td>0.522498</td>\n",
              "      <td>-12.189464</td>\n",
              "      <td>0.727490</td>\n",
              "      <td>475</td>\n",
              "      <td>569</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>27.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.961994</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.636533</td>\n",
              "      <td>-0.003037</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>-11.376695</td>\n",
              "      <td>0.744986</td>\n",
              "      <td>171</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>31.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.969031</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.623565</td>\n",
              "      <td>-0.006656</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>-15.028409</td>\n",
              "      <td>0.735795</td>\n",
              "      <td>166</td>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>21.70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Model         Set  Accuracy  Precision  \\\n",
              "0                  Regresion Lineal    Training  0.795225   0.825122   \n",
              "1                  Regresion Lineal  Validation  0.815934   0.827130   \n",
              "2                  Regresion Lineal        Test  0.793956   0.830307   \n",
              "3               Regresión Logística    Training  0.835629   0.843677   \n",
              "4               Regresión Logística  Validation  0.826923   0.828750   \n",
              "5               Regresión Logística        Test  0.826923   0.839082   \n",
              "6                 Árbol de Decisión    Training  0.943067   0.943447   \n",
              "7                 Árbol de Decisión  Validation  0.854396   0.856113   \n",
              "8                 Árbol de Decisión        Test  0.854396   0.854704   \n",
              "9                     Random Forest    Training  1.000000   1.000000   \n",
              "10                    Random Forest  Validation  0.887363   0.889476   \n",
              "11                    Random Forest        Test  0.901099   0.901102   \n",
              "12                              KNN    Training  0.852158   0.856418   \n",
              "13                              KNN  Validation  0.785714   0.787289   \n",
              "14                              KNN        Test  0.793956   0.799964   \n",
              "15                              SVN    Training  0.846648   0.850097   \n",
              "16                              SVN  Validation  0.821429   0.821337   \n",
              "17                              SVN        Test  0.821429   0.826572   \n",
              "18           Naive Bayes - Gaussian    Training  0.759412   0.831542   \n",
              "19           Naive Bayes - Gaussian  Validation  0.766484   0.809282   \n",
              "20           Naive Bayes - Gaussian        Test  0.758242   0.815378   \n",
              "21          Naive Bayes - Bernoulli    Training  0.814509   0.820274   \n",
              "22          Naive Bayes - Bernoulli  Validation  0.791209   0.791105   \n",
              "23          Naive Bayes - Bernoulli        Test  0.826923   0.833036   \n",
              "24                       AdaBoost 1    Training  1.000000   1.000000   \n",
              "25                       AdaBoost 1  Validation  0.890110   0.890577   \n",
              "26                       AdaBoost 1        Test  0.928571   0.928693   \n",
              "27                       AdaBoost 2    Training  0.977961   0.977957   \n",
              "28                       AdaBoost 2  Validation  0.928571   0.929217   \n",
              "29                       AdaBoost 2        Test  0.942308   0.942345   \n",
              "30                Gradient Boosting    Training  1.000000   1.000000   \n",
              "31                Gradient Boosting  Validation  0.945055   0.945198   \n",
              "32                Gradient Boosting        Test  0.931319   0.931298   \n",
              "33  Artificial Network Layers (ANN)    Training  0.477502   0.961706   \n",
              "34  Artificial Network Layers (ANN)  Validation  0.510989   0.961994   \n",
              "35  Artificial Network Layers (ANN)        Test  0.489011   0.969031   \n",
              "\n",
              "      Recall  F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared  \\\n",
              "0   0.795225  0.794968             0.347828            0.204775   0.167334   \n",
              "1   0.815934  0.815546             0.397597            0.184066   0.261037   \n",
              "2   0.793956  0.791730             0.343680            0.206044   0.169405   \n",
              "3   0.835629  0.836301             0.450090            0.164371   0.331627   \n",
              "4   0.826923  0.827081             0.425939            0.173077   0.305154   \n",
              "5   0.826923  0.826964             0.425926            0.173077   0.302300   \n",
              "6   0.943067  0.943141             0.785002            0.056933   0.768496   \n",
              "7   0.854396  0.853779             0.501006            0.145604   0.415447   \n",
              "8   0.854396  0.854492             0.501008            0.145604   0.413046   \n",
              "9   1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "10  0.887363  0.886886             0.599091            0.112637   0.547799   \n",
              "11  0.901099  0.900990             0.642523            0.098901   0.601314   \n",
              "12  0.852158  0.852701             0.495592            0.147842   0.398838   \n",
              "13  0.785714  0.785909             0.324677            0.214286   0.139715   \n",
              "14  0.793956  0.794316             0.343845            0.206044   0.169405   \n",
              "15  0.846648  0.847172             0.480168            0.153352   0.376434   \n",
              "16  0.821429  0.821327             0.411647            0.178571   0.283095   \n",
              "17  0.821429  0.821762             0.411656            0.178571   0.280151   \n",
              "18  0.759412  0.754310             0.267385            0.240588   0.021711   \n",
              "19  0.766484  0.761488             0.282026            0.233516   0.062509   \n",
              "20  0.758242  0.752494             0.264371            0.241758   0.025435   \n",
              "21  0.814509  0.815240             0.395110            0.185491   0.245747   \n",
              "22  0.791209  0.791133             0.337388            0.208791   0.161773   \n",
              "23  0.826923  0.827225             0.425943            0.173077   0.302300   \n",
              "24  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "25  0.890110  0.889909             0.607662            0.109890   0.558828   \n",
              "26  0.928571  0.928604             0.733959            0.071429   0.712060   \n",
              "27  0.977961  0.977956             0.913688            0.022039   0.910386   \n",
              "28  0.928571  0.928441             0.733961            0.071429   0.713238   \n",
              "29  0.942308  0.942262             0.781936            0.057692   0.767433   \n",
              "30  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "31  0.945055  0.945012             0.791723            0.054945   0.779414   \n",
              "32  0.931319  0.931301             0.743431            0.068681   0.723135   \n",
              "33  0.477502  0.605211            -0.011995            0.522498 -12.189464   \n",
              "34  0.510989  0.636533            -0.003037            0.489011 -11.376695   \n",
              "35  0.489011  0.623565            -0.006656            0.510989 -15.028409   \n",
              "\n",
              "     AUC-ROC   TN   FP   FN   TP  Global Score  \n",
              "0   0.809587  438   37  186  428         74.81  \n",
              "1   0.820425  153   18   49  144         76.76  \n",
              "2   0.806225  157    9   66  132         74.65  \n",
              "3   0.841128  420   55  124  490         79.14  \n",
              "4   0.828122  145   26   37  156         77.87  \n",
              "5   0.833120  150   16   47  151         78.15  \n",
              "6   0.943554  450   25   37  577         92.45  \n",
              "7   0.851362  137   34   19  174         81.09  \n",
              "8   0.853992  141   25   28  170         81.16  \n",
              "9   1.000000  475    0    0  614        100.00  \n",
              "10  0.884450  143   28   13  180         85.21  \n",
              "11  0.899355  146   20   16  182         86.96  \n",
              "12  0.855309  418   57  104  510         81.04  \n",
              "13  0.786595  137   34   44  149         72.93  \n",
              "14  0.797463  139   27   48  150         74.04  \n",
              "15  0.848994  412   63  104  510         80.33  \n",
              "16  0.820274  137   34   31  162         77.11  \n",
              "17  0.824662  143   23   42  156         77.31  \n",
              "18  0.784262  465   10  252  362         71.38  \n",
              "19  0.776460  161   10   75  118         71.40  \n",
              "20  0.774370  159    7   81  117         70.79  \n",
              "21  0.818109  402   73  129  485         76.49  \n",
              "22  0.790110  132   39   37  156         73.49  \n",
              "23  0.830686  145   21   42  156         78.00  \n",
              "24  1.000000  475    0    0  614        100.00  \n",
              "25  0.888374  147   24   16  177         85.58  \n",
              "26  0.928502  154   12   14  184         90.55  \n",
              "27  0.977358  462   13   11  603         97.02  \n",
              "28  0.926976  154   17    9  184         90.51  \n",
              "29  0.941128  154   12    9  189         92.30  \n",
              "30  1.000000  475    0    0  614        100.00  \n",
              "31  0.944187  159   12    8  185         92.67  \n",
              "32  0.930540  153   13   12  186         90.88  \n",
              "33  0.727490  475  569    0   45         27.84  \n",
              "34  0.744986  171  178    0   15         31.87  \n",
              "35  0.735795  166  186    0   12         21.70  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Crear una instancia del modelo de red neuronal artificial (ANN)\n",
        "model_ANN = tf.keras.models.Sequential()\n",
        "model_name = \"Artificial Network Layers (ANN)\"\n",
        "print(model_name)\n",
        "\n",
        "# Definir la arquitectura del modelo\n",
        "model_ANN.add(Dense(units=6, activation='relu'))  # Primera capa densa con 6 neuronas y activación ReLU\n",
        "model_ANN.add(Dense(units=6, activation='relu'))  # Segunda capa densa con 6 neuronas y activación ReLU\n",
        "model_ANN.add(Dense(units=1, activation='sigmoid'))  # Capa final densa con 1 neurona y activación sigmoide\n",
        "\n",
        "# Compilar el modelo para clasificación binaria\n",
        "model_ANN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Definir un umbral para convertir las predicciones a binario\n",
        "umbral = 0.5  # Ajuste el umbral según sus necesidades\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "model_ANN.fit(X_train, y_train, batch_size=32, epochs=25)\n",
        "\n",
        "# Realizar predicciones en los conjuntos de datos\n",
        "y_train_pred = model_ANN.predict(X_train_prep)\n",
        "y_val_pred = model_ANN.predict(X_val_prep)\n",
        "y_test_pred = model_ANN.predict(X_test_prep)\n",
        "\n",
        "# Convertir las predicciones a formato binario (0 o 1)\n",
        "y_train_bin = np.where(y_train_pred > umbral, 1, 0)\n",
        "y_val_bin = np.where(y_val_pred > umbral, 1, 0)\n",
        "y_test_bin = np.where(y_test_pred > umbral, 1, 0)\n",
        "\n",
        "# Convertir las etiquetas reales a formato binario (0 o 1) \n",
        "y_train_pred = y_train_pred.flatten()  \n",
        "y_val_pred = y_val_pred.flatten() \n",
        "y_test_pred = y_test_pred.flatten() \n",
        "\n",
        "y_train_bin = y_train_bin.flatten()  \n",
        "y_val_bin = y_val_bin.flatten() \n",
        "y_test_bin = y_test_bin.flatten() \n",
        "\n",
        "# Evaluar el modelo (usar las predicciones y etiquetas binarizadas)\n",
        "mostrar_estadisticas_guardar_tabla(y_train_bin, y_train, \"Training\", model_name, print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_bin, y_val, \"Validation\", model_name, print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_bin, y_test, \"Test\", model_name, print_roc=\"NO\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Máquinas de vectores de soporte de regresión (SVR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVR\n",
            "Mejores parámetros {'C': 1, 'epsilon': 0.2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8338\n",
            " - Precision: 0.8439\n",
            " - Recall: 0.8338\n",
            " - F1-Score: 0.8345\n",
            " - Adjusted Rand Index: 0.4452\n",
            " - Mean Squared Error: 0.1662\n",
            " - R-squared: 0.3242\n",
            " - Área bajo la curva : 0.840\n",
            " - Confusion Matrix: \n",
            "[[424  51]\n",
            " [130 484]]\n",
            " - Global Score : 78.98\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8242\n",
            " - Precision: 0.8269\n",
            " - Recall: 0.8242\n",
            " - F1-Score: 0.8243\n",
            " - Adjusted Rand Index: 0.4188\n",
            " - Mean Squared Error: 0.1758\n",
            " - R-squared: 0.2941\n",
            " - Área bajo la curva : 0.826\n",
            " - Confusion Matrix: \n",
            "[[146  25]\n",
            " [ 39 154]]\n",
            " - Global Score : 77.57\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8352\n",
            " - Precision: 0.8513\n",
            " - Recall: 0.8352\n",
            " - F1-Score: 0.8350\n",
            " - Adjusted Rand Index: 0.4478\n",
            " - Mean Squared Error: 0.1648\n",
            " - R-squared: 0.3355\n",
            " - Área bajo la curva : 0.843\n",
            " - Confusion Matrix: \n",
            "[[154  12]\n",
            " [ 48 150]]\n",
            " - Global Score : 79.24\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.890577</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.889909</td>\n",
              "      <td>0.607662</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.558828</td>\n",
              "      <td>0.888374</td>\n",
              "      <td>147</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>177</td>\n",
              "      <td>85.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928693</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928604</td>\n",
              "      <td>0.733959</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.712060</td>\n",
              "      <td>0.928502</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>184</td>\n",
              "      <td>90.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977957</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977956</td>\n",
              "      <td>0.913688</td>\n",
              "      <td>0.022039</td>\n",
              "      <td>0.910386</td>\n",
              "      <td>0.977358</td>\n",
              "      <td>462</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>603</td>\n",
              "      <td>97.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.929217</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928441</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>0.926976</td>\n",
              "      <td>154</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>184</td>\n",
              "      <td>90.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942345</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942262</td>\n",
              "      <td>0.781936</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.767433</td>\n",
              "      <td>0.941128</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>189</td>\n",
              "      <td>92.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945198</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945012</td>\n",
              "      <td>0.791723</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.779414</td>\n",
              "      <td>0.944187</td>\n",
              "      <td>159</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>185</td>\n",
              "      <td>92.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931298</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931301</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.068681</td>\n",
              "      <td>0.723135</td>\n",
              "      <td>0.930540</td>\n",
              "      <td>153</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>186</td>\n",
              "      <td>90.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.961706</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.605211</td>\n",
              "      <td>-0.011995</td>\n",
              "      <td>0.522498</td>\n",
              "      <td>-12.189464</td>\n",
              "      <td>0.727490</td>\n",
              "      <td>475</td>\n",
              "      <td>569</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>27.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.961994</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.636533</td>\n",
              "      <td>-0.003037</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>-11.376695</td>\n",
              "      <td>0.744986</td>\n",
              "      <td>171</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>31.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.969031</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.623565</td>\n",
              "      <td>-0.006656</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>-15.028409</td>\n",
              "      <td>0.735795</td>\n",
              "      <td>166</td>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>21.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.843900</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.834459</td>\n",
              "      <td>0.445165</td>\n",
              "      <td>0.166208</td>\n",
              "      <td>0.324159</td>\n",
              "      <td>0.840453</td>\n",
              "      <td>424</td>\n",
              "      <td>51</td>\n",
              "      <td>130</td>\n",
              "      <td>484</td>\n",
              "      <td>78.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.826912</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.824325</td>\n",
              "      <td>0.418764</td>\n",
              "      <td>0.175824</td>\n",
              "      <td>0.294125</td>\n",
              "      <td>0.825864</td>\n",
              "      <td>146</td>\n",
              "      <td>25</td>\n",
              "      <td>39</td>\n",
              "      <td>154</td>\n",
              "      <td>77.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.851340</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.834986</td>\n",
              "      <td>0.447799</td>\n",
              "      <td>0.164835</td>\n",
              "      <td>0.335524</td>\n",
              "      <td>0.842643</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>79.24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Model         Set  Accuracy  Precision  \\\n",
              "0                  Regresion Lineal    Training  0.795225   0.825122   \n",
              "1                  Regresion Lineal  Validation  0.815934   0.827130   \n",
              "2                  Regresion Lineal        Test  0.793956   0.830307   \n",
              "3               Regresión Logística    Training  0.835629   0.843677   \n",
              "4               Regresión Logística  Validation  0.826923   0.828750   \n",
              "5               Regresión Logística        Test  0.826923   0.839082   \n",
              "6                 Árbol de Decisión    Training  0.943067   0.943447   \n",
              "7                 Árbol de Decisión  Validation  0.854396   0.856113   \n",
              "8                 Árbol de Decisión        Test  0.854396   0.854704   \n",
              "9                     Random Forest    Training  1.000000   1.000000   \n",
              "10                    Random Forest  Validation  0.887363   0.889476   \n",
              "11                    Random Forest        Test  0.901099   0.901102   \n",
              "12                              KNN    Training  0.852158   0.856418   \n",
              "13                              KNN  Validation  0.785714   0.787289   \n",
              "14                              KNN        Test  0.793956   0.799964   \n",
              "15                              SVN    Training  0.846648   0.850097   \n",
              "16                              SVN  Validation  0.821429   0.821337   \n",
              "17                              SVN        Test  0.821429   0.826572   \n",
              "18           Naive Bayes - Gaussian    Training  0.759412   0.831542   \n",
              "19           Naive Bayes - Gaussian  Validation  0.766484   0.809282   \n",
              "20           Naive Bayes - Gaussian        Test  0.758242   0.815378   \n",
              "21          Naive Bayes - Bernoulli    Training  0.814509   0.820274   \n",
              "22          Naive Bayes - Bernoulli  Validation  0.791209   0.791105   \n",
              "23          Naive Bayes - Bernoulli        Test  0.826923   0.833036   \n",
              "24                       AdaBoost 1    Training  1.000000   1.000000   \n",
              "25                       AdaBoost 1  Validation  0.890110   0.890577   \n",
              "26                       AdaBoost 1        Test  0.928571   0.928693   \n",
              "27                       AdaBoost 2    Training  0.977961   0.977957   \n",
              "28                       AdaBoost 2  Validation  0.928571   0.929217   \n",
              "29                       AdaBoost 2        Test  0.942308   0.942345   \n",
              "30                Gradient Boosting    Training  1.000000   1.000000   \n",
              "31                Gradient Boosting  Validation  0.945055   0.945198   \n",
              "32                Gradient Boosting        Test  0.931319   0.931298   \n",
              "33  Artificial Network Layers (ANN)    Training  0.477502   0.961706   \n",
              "34  Artificial Network Layers (ANN)  Validation  0.510989   0.961994   \n",
              "35  Artificial Network Layers (ANN)        Test  0.489011   0.969031   \n",
              "36                              SVR    Training  0.833792   0.843900   \n",
              "37                              SVR  Validation  0.824176   0.826912   \n",
              "38                              SVR        Test  0.835165   0.851340   \n",
              "\n",
              "      Recall  F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared  \\\n",
              "0   0.795225  0.794968             0.347828            0.204775   0.167334   \n",
              "1   0.815934  0.815546             0.397597            0.184066   0.261037   \n",
              "2   0.793956  0.791730             0.343680            0.206044   0.169405   \n",
              "3   0.835629  0.836301             0.450090            0.164371   0.331627   \n",
              "4   0.826923  0.827081             0.425939            0.173077   0.305154   \n",
              "5   0.826923  0.826964             0.425926            0.173077   0.302300   \n",
              "6   0.943067  0.943141             0.785002            0.056933   0.768496   \n",
              "7   0.854396  0.853779             0.501006            0.145604   0.415447   \n",
              "8   0.854396  0.854492             0.501008            0.145604   0.413046   \n",
              "9   1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "10  0.887363  0.886886             0.599091            0.112637   0.547799   \n",
              "11  0.901099  0.900990             0.642523            0.098901   0.601314   \n",
              "12  0.852158  0.852701             0.495592            0.147842   0.398838   \n",
              "13  0.785714  0.785909             0.324677            0.214286   0.139715   \n",
              "14  0.793956  0.794316             0.343845            0.206044   0.169405   \n",
              "15  0.846648  0.847172             0.480168            0.153352   0.376434   \n",
              "16  0.821429  0.821327             0.411647            0.178571   0.283095   \n",
              "17  0.821429  0.821762             0.411656            0.178571   0.280151   \n",
              "18  0.759412  0.754310             0.267385            0.240588   0.021711   \n",
              "19  0.766484  0.761488             0.282026            0.233516   0.062509   \n",
              "20  0.758242  0.752494             0.264371            0.241758   0.025435   \n",
              "21  0.814509  0.815240             0.395110            0.185491   0.245747   \n",
              "22  0.791209  0.791133             0.337388            0.208791   0.161773   \n",
              "23  0.826923  0.827225             0.425943            0.173077   0.302300   \n",
              "24  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "25  0.890110  0.889909             0.607662            0.109890   0.558828   \n",
              "26  0.928571  0.928604             0.733959            0.071429   0.712060   \n",
              "27  0.977961  0.977956             0.913688            0.022039   0.910386   \n",
              "28  0.928571  0.928441             0.733961            0.071429   0.713238   \n",
              "29  0.942308  0.942262             0.781936            0.057692   0.767433   \n",
              "30  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "31  0.945055  0.945012             0.791723            0.054945   0.779414   \n",
              "32  0.931319  0.931301             0.743431            0.068681   0.723135   \n",
              "33  0.477502  0.605211            -0.011995            0.522498 -12.189464   \n",
              "34  0.510989  0.636533            -0.003037            0.489011 -11.376695   \n",
              "35  0.489011  0.623565            -0.006656            0.510989 -15.028409   \n",
              "36  0.833792  0.834459             0.445165            0.166208   0.324159   \n",
              "37  0.824176  0.824325             0.418764            0.175824   0.294125   \n",
              "38  0.835165  0.834986             0.447799            0.164835   0.335524   \n",
              "\n",
              "     AUC-ROC   TN   FP   FN   TP  Global Score  \n",
              "0   0.809587  438   37  186  428         74.81  \n",
              "1   0.820425  153   18   49  144         76.76  \n",
              "2   0.806225  157    9   66  132         74.65  \n",
              "3   0.841128  420   55  124  490         79.14  \n",
              "4   0.828122  145   26   37  156         77.87  \n",
              "5   0.833120  150   16   47  151         78.15  \n",
              "6   0.943554  450   25   37  577         92.45  \n",
              "7   0.851362  137   34   19  174         81.09  \n",
              "8   0.853992  141   25   28  170         81.16  \n",
              "9   1.000000  475    0    0  614        100.00  \n",
              "10  0.884450  143   28   13  180         85.21  \n",
              "11  0.899355  146   20   16  182         86.96  \n",
              "12  0.855309  418   57  104  510         81.04  \n",
              "13  0.786595  137   34   44  149         72.93  \n",
              "14  0.797463  139   27   48  150         74.04  \n",
              "15  0.848994  412   63  104  510         80.33  \n",
              "16  0.820274  137   34   31  162         77.11  \n",
              "17  0.824662  143   23   42  156         77.31  \n",
              "18  0.784262  465   10  252  362         71.38  \n",
              "19  0.776460  161   10   75  118         71.40  \n",
              "20  0.774370  159    7   81  117         70.79  \n",
              "21  0.818109  402   73  129  485         76.49  \n",
              "22  0.790110  132   39   37  156         73.49  \n",
              "23  0.830686  145   21   42  156         78.00  \n",
              "24  1.000000  475    0    0  614        100.00  \n",
              "25  0.888374  147   24   16  177         85.58  \n",
              "26  0.928502  154   12   14  184         90.55  \n",
              "27  0.977358  462   13   11  603         97.02  \n",
              "28  0.926976  154   17    9  184         90.51  \n",
              "29  0.941128  154   12    9  189         92.30  \n",
              "30  1.000000  475    0    0  614        100.00  \n",
              "31  0.944187  159   12    8  185         92.67  \n",
              "32  0.930540  153   13   12  186         90.88  \n",
              "33  0.727490  475  569    0   45         27.84  \n",
              "34  0.744986  171  178    0   15         31.87  \n",
              "35  0.735795  166  186    0   12         21.70  \n",
              "36  0.840453  424   51  130  484         78.98  \n",
              "37  0.825864  146   25   39  154         77.57  \n",
              "38  0.842643  154   12   48  150         79.24  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# TODO + 10 minutos de ejecución -- CORREGIR\n",
        "# Definir el modelo SVR\n",
        "model_name = \"SVR\"\n",
        "print(model_name)\n",
        "svr = SVR()\n",
        "\n",
        "# Definir los hiperparámetros a buscar\n",
        "param_grid = {\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'epsilon': [0.1, 0.2]\n",
        "}\n",
        "\n",
        "''' Tiempo de ejecucion > 13 minutos\n",
        "param_grid = {\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'C': [0.1, 1, 10, 100, 1000],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'epsilon': [0.1, 0.2, 0.5, 0.3]\n",
        "}\n",
        "'''\n",
        "\n",
        "# Definir GridSearchCV\n",
        "grid_search = GridSearchCV(svr, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento y encontrar los mejores hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener el mejor modelo y sus hiperparámetros\n",
        "model_SVR = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Mejores parámetros {grid_search.best_params_}\")\n",
        "\n",
        "y_train_pred = model_SVR.predict(X_train_prep)\n",
        "y_val_pred = model_SVR.predict(X_val_prep)\n",
        "y_test_pred = model_SVR.predict(X_test_prep)\n",
        "\n",
        "umbral = 0.55\n",
        "\n",
        "y_train_pred_binary = (y_train_pred >= umbral).astype(int)\n",
        "y_val_pred_binary = (y_val_pred >= umbral).astype(int)\n",
        "y_test_pred_binary = (y_test_pred >= umbral).astype(int)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred_binary, \"Training\", model_name, print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred_binary, \"Validation\",model_name, print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred_binary, \"Test\", model_name, print_roc = \"NO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión polinomial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regresión polinomial\n",
            "Mejores parámetros {'logistic__C': 1, 'logistic__fit_intercept': True, 'poly__degree': 3}\n",
            "[0 1 0 ... 1 1 0]\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8512\n",
            " - Precision: 0.8526\n",
            " - Recall: 0.8512\n",
            " - F1-Score: 0.8516\n",
            " - Adjusted Rand Index: 0.4930\n",
            " - Mean Squared Error: 0.1488\n",
            " - R-squared: 0.3951\n",
            " - Área bajo la curva : 0.851\n",
            " - Confusion Matrix: \n",
            "[[405  70]\n",
            " [ 92 522]]\n",
            " - Global Score : 80.8\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8104\n",
            " - Precision: 0.8109\n",
            " - Recall: 0.8104\n",
            " - F1-Score: 0.8099\n",
            " - Adjusted Rand Index: 0.3838\n",
            " - Mean Squared Error: 0.1896\n",
            " - R-squared: 0.2390\n",
            " - Área bajo la curva : 0.808\n",
            " - Confusion Matrix: \n",
            "[[131  40]\n",
            " [ 29 164]]\n",
            " - Global Score : 75.74\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8269\n",
            " - Precision: 0.8290\n",
            " - Recall: 0.8269\n",
            " - F1-Score: 0.8272\n",
            " - Adjusted Rand Index: 0.4259\n",
            " - Mean Squared Error: 0.1731\n",
            " - R-squared: 0.3023\n",
            " - Área bajo la curva : 0.828\n",
            " - Confusion Matrix: \n",
            "[[140  26]\n",
            " [ 37 161]]\n",
            " - Global Score : 77.88\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.890577</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.889909</td>\n",
              "      <td>0.607662</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.558828</td>\n",
              "      <td>0.888374</td>\n",
              "      <td>147</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>177</td>\n",
              "      <td>85.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928693</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928604</td>\n",
              "      <td>0.733959</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.712060</td>\n",
              "      <td>0.928502</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>184</td>\n",
              "      <td>90.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977957</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977956</td>\n",
              "      <td>0.913688</td>\n",
              "      <td>0.022039</td>\n",
              "      <td>0.910386</td>\n",
              "      <td>0.977358</td>\n",
              "      <td>462</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>603</td>\n",
              "      <td>97.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.929217</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928441</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>0.926976</td>\n",
              "      <td>154</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>184</td>\n",
              "      <td>90.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942345</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942262</td>\n",
              "      <td>0.781936</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.767433</td>\n",
              "      <td>0.941128</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>189</td>\n",
              "      <td>92.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945198</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945012</td>\n",
              "      <td>0.791723</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.779414</td>\n",
              "      <td>0.944187</td>\n",
              "      <td>159</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>185</td>\n",
              "      <td>92.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931298</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931301</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.068681</td>\n",
              "      <td>0.723135</td>\n",
              "      <td>0.930540</td>\n",
              "      <td>153</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>186</td>\n",
              "      <td>90.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.961706</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.605211</td>\n",
              "      <td>-0.011995</td>\n",
              "      <td>0.522498</td>\n",
              "      <td>-12.189464</td>\n",
              "      <td>0.727490</td>\n",
              "      <td>475</td>\n",
              "      <td>569</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>27.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.961994</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.636533</td>\n",
              "      <td>-0.003037</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>-11.376695</td>\n",
              "      <td>0.744986</td>\n",
              "      <td>171</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>31.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.969031</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.623565</td>\n",
              "      <td>-0.006656</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>-15.028409</td>\n",
              "      <td>0.735795</td>\n",
              "      <td>166</td>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>21.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.843900</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.834459</td>\n",
              "      <td>0.445165</td>\n",
              "      <td>0.166208</td>\n",
              "      <td>0.324159</td>\n",
              "      <td>0.840453</td>\n",
              "      <td>424</td>\n",
              "      <td>51</td>\n",
              "      <td>130</td>\n",
              "      <td>484</td>\n",
              "      <td>78.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.826912</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.824325</td>\n",
              "      <td>0.418764</td>\n",
              "      <td>0.175824</td>\n",
              "      <td>0.294125</td>\n",
              "      <td>0.825864</td>\n",
              "      <td>146</td>\n",
              "      <td>25</td>\n",
              "      <td>39</td>\n",
              "      <td>154</td>\n",
              "      <td>77.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.851340</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.834986</td>\n",
              "      <td>0.447799</td>\n",
              "      <td>0.164835</td>\n",
              "      <td>0.335524</td>\n",
              "      <td>0.842643</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>79.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.851240</td>\n",
              "      <td>0.852591</td>\n",
              "      <td>0.851240</td>\n",
              "      <td>0.851566</td>\n",
              "      <td>0.492959</td>\n",
              "      <td>0.148760</td>\n",
              "      <td>0.395104</td>\n",
              "      <td>0.851397</td>\n",
              "      <td>405</td>\n",
              "      <td>70</td>\n",
              "      <td>92</td>\n",
              "      <td>522</td>\n",
              "      <td>80.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.810440</td>\n",
              "      <td>0.810888</td>\n",
              "      <td>0.810440</td>\n",
              "      <td>0.809916</td>\n",
              "      <td>0.383791</td>\n",
              "      <td>0.189560</td>\n",
              "      <td>0.238978</td>\n",
              "      <td>0.807911</td>\n",
              "      <td>131</td>\n",
              "      <td>40</td>\n",
              "      <td>29</td>\n",
              "      <td>164</td>\n",
              "      <td>75.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.829039</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827226</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.828252</td>\n",
              "      <td>140</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>161</td>\n",
              "      <td>77.88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Model         Set  Accuracy  Precision  \\\n",
              "0                  Regresion Lineal    Training  0.795225   0.825122   \n",
              "1                  Regresion Lineal  Validation  0.815934   0.827130   \n",
              "2                  Regresion Lineal        Test  0.793956   0.830307   \n",
              "3               Regresión Logística    Training  0.835629   0.843677   \n",
              "4               Regresión Logística  Validation  0.826923   0.828750   \n",
              "5               Regresión Logística        Test  0.826923   0.839082   \n",
              "6                 Árbol de Decisión    Training  0.943067   0.943447   \n",
              "7                 Árbol de Decisión  Validation  0.854396   0.856113   \n",
              "8                 Árbol de Decisión        Test  0.854396   0.854704   \n",
              "9                     Random Forest    Training  1.000000   1.000000   \n",
              "10                    Random Forest  Validation  0.887363   0.889476   \n",
              "11                    Random Forest        Test  0.901099   0.901102   \n",
              "12                              KNN    Training  0.852158   0.856418   \n",
              "13                              KNN  Validation  0.785714   0.787289   \n",
              "14                              KNN        Test  0.793956   0.799964   \n",
              "15                              SVN    Training  0.846648   0.850097   \n",
              "16                              SVN  Validation  0.821429   0.821337   \n",
              "17                              SVN        Test  0.821429   0.826572   \n",
              "18           Naive Bayes - Gaussian    Training  0.759412   0.831542   \n",
              "19           Naive Bayes - Gaussian  Validation  0.766484   0.809282   \n",
              "20           Naive Bayes - Gaussian        Test  0.758242   0.815378   \n",
              "21          Naive Bayes - Bernoulli    Training  0.814509   0.820274   \n",
              "22          Naive Bayes - Bernoulli  Validation  0.791209   0.791105   \n",
              "23          Naive Bayes - Bernoulli        Test  0.826923   0.833036   \n",
              "24                       AdaBoost 1    Training  1.000000   1.000000   \n",
              "25                       AdaBoost 1  Validation  0.890110   0.890577   \n",
              "26                       AdaBoost 1        Test  0.928571   0.928693   \n",
              "27                       AdaBoost 2    Training  0.977961   0.977957   \n",
              "28                       AdaBoost 2  Validation  0.928571   0.929217   \n",
              "29                       AdaBoost 2        Test  0.942308   0.942345   \n",
              "30                Gradient Boosting    Training  1.000000   1.000000   \n",
              "31                Gradient Boosting  Validation  0.945055   0.945198   \n",
              "32                Gradient Boosting        Test  0.931319   0.931298   \n",
              "33  Artificial Network Layers (ANN)    Training  0.477502   0.961706   \n",
              "34  Artificial Network Layers (ANN)  Validation  0.510989   0.961994   \n",
              "35  Artificial Network Layers (ANN)        Test  0.489011   0.969031   \n",
              "36                              SVR    Training  0.833792   0.843900   \n",
              "37                              SVR  Validation  0.824176   0.826912   \n",
              "38                              SVR        Test  0.835165   0.851340   \n",
              "39             Regresión polinomial    Training  0.851240   0.852591   \n",
              "40             Regresión polinomial  Validation  0.810440   0.810888   \n",
              "41             Regresión polinomial        Test  0.826923   0.829039   \n",
              "\n",
              "      Recall  F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared  \\\n",
              "0   0.795225  0.794968             0.347828            0.204775   0.167334   \n",
              "1   0.815934  0.815546             0.397597            0.184066   0.261037   \n",
              "2   0.793956  0.791730             0.343680            0.206044   0.169405   \n",
              "3   0.835629  0.836301             0.450090            0.164371   0.331627   \n",
              "4   0.826923  0.827081             0.425939            0.173077   0.305154   \n",
              "5   0.826923  0.826964             0.425926            0.173077   0.302300   \n",
              "6   0.943067  0.943141             0.785002            0.056933   0.768496   \n",
              "7   0.854396  0.853779             0.501006            0.145604   0.415447   \n",
              "8   0.854396  0.854492             0.501008            0.145604   0.413046   \n",
              "9   1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "10  0.887363  0.886886             0.599091            0.112637   0.547799   \n",
              "11  0.901099  0.900990             0.642523            0.098901   0.601314   \n",
              "12  0.852158  0.852701             0.495592            0.147842   0.398838   \n",
              "13  0.785714  0.785909             0.324677            0.214286   0.139715   \n",
              "14  0.793956  0.794316             0.343845            0.206044   0.169405   \n",
              "15  0.846648  0.847172             0.480168            0.153352   0.376434   \n",
              "16  0.821429  0.821327             0.411647            0.178571   0.283095   \n",
              "17  0.821429  0.821762             0.411656            0.178571   0.280151   \n",
              "18  0.759412  0.754310             0.267385            0.240588   0.021711   \n",
              "19  0.766484  0.761488             0.282026            0.233516   0.062509   \n",
              "20  0.758242  0.752494             0.264371            0.241758   0.025435   \n",
              "21  0.814509  0.815240             0.395110            0.185491   0.245747   \n",
              "22  0.791209  0.791133             0.337388            0.208791   0.161773   \n",
              "23  0.826923  0.827225             0.425943            0.173077   0.302300   \n",
              "24  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "25  0.890110  0.889909             0.607662            0.109890   0.558828   \n",
              "26  0.928571  0.928604             0.733959            0.071429   0.712060   \n",
              "27  0.977961  0.977956             0.913688            0.022039   0.910386   \n",
              "28  0.928571  0.928441             0.733961            0.071429   0.713238   \n",
              "29  0.942308  0.942262             0.781936            0.057692   0.767433   \n",
              "30  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "31  0.945055  0.945012             0.791723            0.054945   0.779414   \n",
              "32  0.931319  0.931301             0.743431            0.068681   0.723135   \n",
              "33  0.477502  0.605211            -0.011995            0.522498 -12.189464   \n",
              "34  0.510989  0.636533            -0.003037            0.489011 -11.376695   \n",
              "35  0.489011  0.623565            -0.006656            0.510989 -15.028409   \n",
              "36  0.833792  0.834459             0.445165            0.166208   0.324159   \n",
              "37  0.824176  0.824325             0.418764            0.175824   0.294125   \n",
              "38  0.835165  0.834986             0.447799            0.164835   0.335524   \n",
              "39  0.851240  0.851566             0.492959            0.148760   0.395104   \n",
              "40  0.810440  0.809916             0.383791            0.189560   0.238978   \n",
              "41  0.826923  0.827226             0.425943            0.173077   0.302300   \n",
              "\n",
              "     AUC-ROC   TN   FP   FN   TP  Global Score  \n",
              "0   0.809587  438   37  186  428         74.81  \n",
              "1   0.820425  153   18   49  144         76.76  \n",
              "2   0.806225  157    9   66  132         74.65  \n",
              "3   0.841128  420   55  124  490         79.14  \n",
              "4   0.828122  145   26   37  156         77.87  \n",
              "5   0.833120  150   16   47  151         78.15  \n",
              "6   0.943554  450   25   37  577         92.45  \n",
              "7   0.851362  137   34   19  174         81.09  \n",
              "8   0.853992  141   25   28  170         81.16  \n",
              "9   1.000000  475    0    0  614        100.00  \n",
              "10  0.884450  143   28   13  180         85.21  \n",
              "11  0.899355  146   20   16  182         86.96  \n",
              "12  0.855309  418   57  104  510         81.04  \n",
              "13  0.786595  137   34   44  149         72.93  \n",
              "14  0.797463  139   27   48  150         74.04  \n",
              "15  0.848994  412   63  104  510         80.33  \n",
              "16  0.820274  137   34   31  162         77.11  \n",
              "17  0.824662  143   23   42  156         77.31  \n",
              "18  0.784262  465   10  252  362         71.38  \n",
              "19  0.776460  161   10   75  118         71.40  \n",
              "20  0.774370  159    7   81  117         70.79  \n",
              "21  0.818109  402   73  129  485         76.49  \n",
              "22  0.790110  132   39   37  156         73.49  \n",
              "23  0.830686  145   21   42  156         78.00  \n",
              "24  1.000000  475    0    0  614        100.00  \n",
              "25  0.888374  147   24   16  177         85.58  \n",
              "26  0.928502  154   12   14  184         90.55  \n",
              "27  0.977358  462   13   11  603         97.02  \n",
              "28  0.926976  154   17    9  184         90.51  \n",
              "29  0.941128  154   12    9  189         92.30  \n",
              "30  1.000000  475    0    0  614        100.00  \n",
              "31  0.944187  159   12    8  185         92.67  \n",
              "32  0.930540  153   13   12  186         90.88  \n",
              "33  0.727490  475  569    0   45         27.84  \n",
              "34  0.744986  171  178    0   15         31.87  \n",
              "35  0.735795  166  186    0   12         21.70  \n",
              "36  0.840453  424   51  130  484         78.98  \n",
              "37  0.825864  146   25   39  154         77.57  \n",
              "38  0.842643  154   12   48  150         79.24  \n",
              "39  0.851397  405   70   92  522         80.80  \n",
              "40  0.807911  131   40   29  164         75.74  \n",
              "41  0.828252  140   26   37  161         77.88  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model_name = \"Regresión polinomial\"\n",
        "print(model_name)\n",
        "\n",
        "# Definir el pipeline para regresión polinomial\n",
        "pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures()),\n",
        "    ('logistic', LogisticRegression(max_iter=10000))\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros a buscar. Un grado muy alto del polinomio puede llevar a sobreajuste (overfitting)\n",
        "param_grid = {\n",
        "    'poly__degree': [2, 3, 4, 5],  # grados del polinomio\n",
        "    'logistic__C': [0.1, 1, 10, 100],  # regularización\n",
        "    'logistic__fit_intercept': [True, False]\n",
        "}\n",
        "\n",
        "# Definir GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento y encontrar los mejores hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener el mejor modelo y sus hiperparámetros\n",
        "model_RPoli = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Mejores parámetros {grid_search.best_params_}\")\n",
        "\n",
        "y_train_pred = model_RPoli.predict(X_train_prep)\n",
        "y_val_pred = model_RPoli.predict(X_val_prep)\n",
        "y_test_pred = model_RPoli.predict(X_test_prep)\n",
        "print(y_train_pred)\n",
        "# Evaluar el modelo\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Máquinas de aprendizaje extremo (Extreme Learning Machines, ELM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install hpelm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extreme Learning Machine (ELM)\n",
            "Using slower basic Python solver\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.7126\n",
            " - Precision: 0.7109\n",
            " - Recall: 0.7126\n",
            " - F1-Score: 0.7100\n",
            " - Adjusted Rand Index: 0.1794\n",
            " - Mean Squared Error: 0.2874\n",
            " - R-squared: -0.1687\n",
            " - Área bajo la curva : 0.702\n",
            " - Confusion Matrix: \n",
            "[[292 183]\n",
            " [130 484]]\n",
            " - Global Score : 64.04\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.6758\n",
            " - Precision: 0.6804\n",
            " - Recall: 0.6758\n",
            " - F1-Score: 0.6696\n",
            " - Adjusted Rand Index: 0.1212\n",
            " - Mean Squared Error: 0.3242\n",
            " - R-squared: -0.3015\n",
            " - Área bajo la curva : 0.668\n",
            " - Confusion Matrix: \n",
            "[[ 92  79]\n",
            " [ 39 154]]\n",
            " - Global Score : 60.13\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.6786\n",
            " - Precision: 0.6775\n",
            " - Recall: 0.6786\n",
            " - F1-Score: 0.6762\n",
            " - Adjusted Rand Index: 0.1250\n",
            " - Mean Squared Error: 0.3214\n",
            " - R-squared: -0.2957\n",
            " - Área bajo la curva : 0.671\n",
            " - Confusion Matrix: \n",
            "[[ 98  68]\n",
            " [ 49 149]]\n",
            " - Global Score : 60.42\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\hpelm\\nnets\\slfn.py:62: RuntimeWarning: overflow encountered in exp\n",
            "  self.func[\"sigm\"] = lambda X, W, B: 1 / (1 + np.exp(np.dot(X, W) + B))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.890577</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.889909</td>\n",
              "      <td>0.607662</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.558828</td>\n",
              "      <td>0.888374</td>\n",
              "      <td>147</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>177</td>\n",
              "      <td>85.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928693</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928604</td>\n",
              "      <td>0.733959</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.712060</td>\n",
              "      <td>0.928502</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>184</td>\n",
              "      <td>90.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977957</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977956</td>\n",
              "      <td>0.913688</td>\n",
              "      <td>0.022039</td>\n",
              "      <td>0.910386</td>\n",
              "      <td>0.977358</td>\n",
              "      <td>462</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>603</td>\n",
              "      <td>97.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.929217</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928441</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>0.926976</td>\n",
              "      <td>154</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>184</td>\n",
              "      <td>90.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942345</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942262</td>\n",
              "      <td>0.781936</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.767433</td>\n",
              "      <td>0.941128</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>189</td>\n",
              "      <td>92.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945198</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945012</td>\n",
              "      <td>0.791723</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.779414</td>\n",
              "      <td>0.944187</td>\n",
              "      <td>159</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>185</td>\n",
              "      <td>92.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931298</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931301</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.068681</td>\n",
              "      <td>0.723135</td>\n",
              "      <td>0.930540</td>\n",
              "      <td>153</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>186</td>\n",
              "      <td>90.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.961706</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.605211</td>\n",
              "      <td>-0.011995</td>\n",
              "      <td>0.522498</td>\n",
              "      <td>-12.189464</td>\n",
              "      <td>0.727490</td>\n",
              "      <td>475</td>\n",
              "      <td>569</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>27.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.961994</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.636533</td>\n",
              "      <td>-0.003037</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>-11.376695</td>\n",
              "      <td>0.744986</td>\n",
              "      <td>171</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>31.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.969031</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.623565</td>\n",
              "      <td>-0.006656</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>-15.028409</td>\n",
              "      <td>0.735795</td>\n",
              "      <td>166</td>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>21.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.843900</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.834459</td>\n",
              "      <td>0.445165</td>\n",
              "      <td>0.166208</td>\n",
              "      <td>0.324159</td>\n",
              "      <td>0.840453</td>\n",
              "      <td>424</td>\n",
              "      <td>51</td>\n",
              "      <td>130</td>\n",
              "      <td>484</td>\n",
              "      <td>78.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.826912</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.824325</td>\n",
              "      <td>0.418764</td>\n",
              "      <td>0.175824</td>\n",
              "      <td>0.294125</td>\n",
              "      <td>0.825864</td>\n",
              "      <td>146</td>\n",
              "      <td>25</td>\n",
              "      <td>39</td>\n",
              "      <td>154</td>\n",
              "      <td>77.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.851340</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.834986</td>\n",
              "      <td>0.447799</td>\n",
              "      <td>0.164835</td>\n",
              "      <td>0.335524</td>\n",
              "      <td>0.842643</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>79.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.851240</td>\n",
              "      <td>0.852591</td>\n",
              "      <td>0.851240</td>\n",
              "      <td>0.851566</td>\n",
              "      <td>0.492959</td>\n",
              "      <td>0.148760</td>\n",
              "      <td>0.395104</td>\n",
              "      <td>0.851397</td>\n",
              "      <td>405</td>\n",
              "      <td>70</td>\n",
              "      <td>92</td>\n",
              "      <td>522</td>\n",
              "      <td>80.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.810440</td>\n",
              "      <td>0.810888</td>\n",
              "      <td>0.810440</td>\n",
              "      <td>0.809916</td>\n",
              "      <td>0.383791</td>\n",
              "      <td>0.189560</td>\n",
              "      <td>0.238978</td>\n",
              "      <td>0.807911</td>\n",
              "      <td>131</td>\n",
              "      <td>40</td>\n",
              "      <td>29</td>\n",
              "      <td>164</td>\n",
              "      <td>75.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.829039</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827226</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.828252</td>\n",
              "      <td>140</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>161</td>\n",
              "      <td>77.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.712580</td>\n",
              "      <td>0.710941</td>\n",
              "      <td>0.712580</td>\n",
              "      <td>0.710035</td>\n",
              "      <td>0.179381</td>\n",
              "      <td>0.287420</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.701505</td>\n",
              "      <td>292</td>\n",
              "      <td>183</td>\n",
              "      <td>130</td>\n",
              "      <td>484</td>\n",
              "      <td>64.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.675824</td>\n",
              "      <td>0.680368</td>\n",
              "      <td>0.675824</td>\n",
              "      <td>0.669575</td>\n",
              "      <td>0.121182</td>\n",
              "      <td>0.324176</td>\n",
              "      <td>-0.301457</td>\n",
              "      <td>0.667970</td>\n",
              "      <td>92</td>\n",
              "      <td>79</td>\n",
              "      <td>39</td>\n",
              "      <td>154</td>\n",
              "      <td>60.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.677529</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.676174</td>\n",
              "      <td>0.124998</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>-0.295728</td>\n",
              "      <td>0.671443</td>\n",
              "      <td>98</td>\n",
              "      <td>68</td>\n",
              "      <td>49</td>\n",
              "      <td>149</td>\n",
              "      <td>60.42</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Model         Set  Accuracy  Precision  \\\n",
              "0                  Regresion Lineal    Training  0.795225   0.825122   \n",
              "1                  Regresion Lineal  Validation  0.815934   0.827130   \n",
              "2                  Regresion Lineal        Test  0.793956   0.830307   \n",
              "3               Regresión Logística    Training  0.835629   0.843677   \n",
              "4               Regresión Logística  Validation  0.826923   0.828750   \n",
              "5               Regresión Logística        Test  0.826923   0.839082   \n",
              "6                 Árbol de Decisión    Training  0.943067   0.943447   \n",
              "7                 Árbol de Decisión  Validation  0.854396   0.856113   \n",
              "8                 Árbol de Decisión        Test  0.854396   0.854704   \n",
              "9                     Random Forest    Training  1.000000   1.000000   \n",
              "10                    Random Forest  Validation  0.887363   0.889476   \n",
              "11                    Random Forest        Test  0.901099   0.901102   \n",
              "12                              KNN    Training  0.852158   0.856418   \n",
              "13                              KNN  Validation  0.785714   0.787289   \n",
              "14                              KNN        Test  0.793956   0.799964   \n",
              "15                              SVN    Training  0.846648   0.850097   \n",
              "16                              SVN  Validation  0.821429   0.821337   \n",
              "17                              SVN        Test  0.821429   0.826572   \n",
              "18           Naive Bayes - Gaussian    Training  0.759412   0.831542   \n",
              "19           Naive Bayes - Gaussian  Validation  0.766484   0.809282   \n",
              "20           Naive Bayes - Gaussian        Test  0.758242   0.815378   \n",
              "21          Naive Bayes - Bernoulli    Training  0.814509   0.820274   \n",
              "22          Naive Bayes - Bernoulli  Validation  0.791209   0.791105   \n",
              "23          Naive Bayes - Bernoulli        Test  0.826923   0.833036   \n",
              "24                       AdaBoost 1    Training  1.000000   1.000000   \n",
              "25                       AdaBoost 1  Validation  0.890110   0.890577   \n",
              "26                       AdaBoost 1        Test  0.928571   0.928693   \n",
              "27                       AdaBoost 2    Training  0.977961   0.977957   \n",
              "28                       AdaBoost 2  Validation  0.928571   0.929217   \n",
              "29                       AdaBoost 2        Test  0.942308   0.942345   \n",
              "30                Gradient Boosting    Training  1.000000   1.000000   \n",
              "31                Gradient Boosting  Validation  0.945055   0.945198   \n",
              "32                Gradient Boosting        Test  0.931319   0.931298   \n",
              "33  Artificial Network Layers (ANN)    Training  0.477502   0.961706   \n",
              "34  Artificial Network Layers (ANN)  Validation  0.510989   0.961994   \n",
              "35  Artificial Network Layers (ANN)        Test  0.489011   0.969031   \n",
              "36                              SVR    Training  0.833792   0.843900   \n",
              "37                              SVR  Validation  0.824176   0.826912   \n",
              "38                              SVR        Test  0.835165   0.851340   \n",
              "39             Regresión polinomial    Training  0.851240   0.852591   \n",
              "40             Regresión polinomial  Validation  0.810440   0.810888   \n",
              "41             Regresión polinomial        Test  0.826923   0.829039   \n",
              "42   Extreme Learning Machine (ELM)    Training  0.712580   0.710941   \n",
              "43   Extreme Learning Machine (ELM)  Validation  0.675824   0.680368   \n",
              "44   Extreme Learning Machine (ELM)        Test  0.678571   0.677529   \n",
              "\n",
              "      Recall  F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared  \\\n",
              "0   0.795225  0.794968             0.347828            0.204775   0.167334   \n",
              "1   0.815934  0.815546             0.397597            0.184066   0.261037   \n",
              "2   0.793956  0.791730             0.343680            0.206044   0.169405   \n",
              "3   0.835629  0.836301             0.450090            0.164371   0.331627   \n",
              "4   0.826923  0.827081             0.425939            0.173077   0.305154   \n",
              "5   0.826923  0.826964             0.425926            0.173077   0.302300   \n",
              "6   0.943067  0.943141             0.785002            0.056933   0.768496   \n",
              "7   0.854396  0.853779             0.501006            0.145604   0.415447   \n",
              "8   0.854396  0.854492             0.501008            0.145604   0.413046   \n",
              "9   1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "10  0.887363  0.886886             0.599091            0.112637   0.547799   \n",
              "11  0.901099  0.900990             0.642523            0.098901   0.601314   \n",
              "12  0.852158  0.852701             0.495592            0.147842   0.398838   \n",
              "13  0.785714  0.785909             0.324677            0.214286   0.139715   \n",
              "14  0.793956  0.794316             0.343845            0.206044   0.169405   \n",
              "15  0.846648  0.847172             0.480168            0.153352   0.376434   \n",
              "16  0.821429  0.821327             0.411647            0.178571   0.283095   \n",
              "17  0.821429  0.821762             0.411656            0.178571   0.280151   \n",
              "18  0.759412  0.754310             0.267385            0.240588   0.021711   \n",
              "19  0.766484  0.761488             0.282026            0.233516   0.062509   \n",
              "20  0.758242  0.752494             0.264371            0.241758   0.025435   \n",
              "21  0.814509  0.815240             0.395110            0.185491   0.245747   \n",
              "22  0.791209  0.791133             0.337388            0.208791   0.161773   \n",
              "23  0.826923  0.827225             0.425943            0.173077   0.302300   \n",
              "24  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "25  0.890110  0.889909             0.607662            0.109890   0.558828   \n",
              "26  0.928571  0.928604             0.733959            0.071429   0.712060   \n",
              "27  0.977961  0.977956             0.913688            0.022039   0.910386   \n",
              "28  0.928571  0.928441             0.733961            0.071429   0.713238   \n",
              "29  0.942308  0.942262             0.781936            0.057692   0.767433   \n",
              "30  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "31  0.945055  0.945012             0.791723            0.054945   0.779414   \n",
              "32  0.931319  0.931301             0.743431            0.068681   0.723135   \n",
              "33  0.477502  0.605211            -0.011995            0.522498 -12.189464   \n",
              "34  0.510989  0.636533            -0.003037            0.489011 -11.376695   \n",
              "35  0.489011  0.623565            -0.006656            0.510989 -15.028409   \n",
              "36  0.833792  0.834459             0.445165            0.166208   0.324159   \n",
              "37  0.824176  0.824325             0.418764            0.175824   0.294125   \n",
              "38  0.835165  0.834986             0.447799            0.164835   0.335524   \n",
              "39  0.851240  0.851566             0.492959            0.148760   0.395104   \n",
              "40  0.810440  0.809916             0.383791            0.189560   0.238978   \n",
              "41  0.826923  0.827226             0.425943            0.173077   0.302300   \n",
              "42  0.712580  0.710035             0.179381            0.287420  -0.168719   \n",
              "43  0.675824  0.669575             0.121182            0.324176  -0.301457   \n",
              "44  0.678571  0.676174             0.124998            0.321429  -0.295728   \n",
              "\n",
              "     AUC-ROC   TN   FP   FN   TP  Global Score  \n",
              "0   0.809587  438   37  186  428         74.81  \n",
              "1   0.820425  153   18   49  144         76.76  \n",
              "2   0.806225  157    9   66  132         74.65  \n",
              "3   0.841128  420   55  124  490         79.14  \n",
              "4   0.828122  145   26   37  156         77.87  \n",
              "5   0.833120  150   16   47  151         78.15  \n",
              "6   0.943554  450   25   37  577         92.45  \n",
              "7   0.851362  137   34   19  174         81.09  \n",
              "8   0.853992  141   25   28  170         81.16  \n",
              "9   1.000000  475    0    0  614        100.00  \n",
              "10  0.884450  143   28   13  180         85.21  \n",
              "11  0.899355  146   20   16  182         86.96  \n",
              "12  0.855309  418   57  104  510         81.04  \n",
              "13  0.786595  137   34   44  149         72.93  \n",
              "14  0.797463  139   27   48  150         74.04  \n",
              "15  0.848994  412   63  104  510         80.33  \n",
              "16  0.820274  137   34   31  162         77.11  \n",
              "17  0.824662  143   23   42  156         77.31  \n",
              "18  0.784262  465   10  252  362         71.38  \n",
              "19  0.776460  161   10   75  118         71.40  \n",
              "20  0.774370  159    7   81  117         70.79  \n",
              "21  0.818109  402   73  129  485         76.49  \n",
              "22  0.790110  132   39   37  156         73.49  \n",
              "23  0.830686  145   21   42  156         78.00  \n",
              "24  1.000000  475    0    0  614        100.00  \n",
              "25  0.888374  147   24   16  177         85.58  \n",
              "26  0.928502  154   12   14  184         90.55  \n",
              "27  0.977358  462   13   11  603         97.02  \n",
              "28  0.926976  154   17    9  184         90.51  \n",
              "29  0.941128  154   12    9  189         92.30  \n",
              "30  1.000000  475    0    0  614        100.00  \n",
              "31  0.944187  159   12    8  185         92.67  \n",
              "32  0.930540  153   13   12  186         90.88  \n",
              "33  0.727490  475  569    0   45         27.84  \n",
              "34  0.744986  171  178    0   15         31.87  \n",
              "35  0.735795  166  186    0   12         21.70  \n",
              "36  0.840453  424   51  130  484         78.98  \n",
              "37  0.825864  146   25   39  154         77.57  \n",
              "38  0.842643  154   12   48  150         79.24  \n",
              "39  0.851397  405   70   92  522         80.80  \n",
              "40  0.807911  131   40   29  164         75.74  \n",
              "41  0.828252  140   26   37  161         77.88  \n",
              "42  0.701505  292  183  130  484         64.04  \n",
              "43  0.667970   92   79   39  154         60.13  \n",
              "44  0.671443   98   68   49  149         60.42  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hpelm import ELM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_name = 'Extreme Learning Machine (ELM)'\n",
        "print(model_name)\n",
        "\n",
        "# Definir el número de neuronas y el tipo de activación para ELM\n",
        "num_neuronas = 100  \n",
        "tipo_activacion = 'sigm'  \n",
        "\n",
        "# Convert labels to NumPy arrays\n",
        "X_train_np = np.array(X_train)\n",
        "X_val_np = np.array(X_val)\n",
        "X_test_np = np.array(X_test)\n",
        "\n",
        "# Convert labels to NumPy arrays for one-hot encoding\n",
        "y_train_np = y_train.to_numpy().reshape(-1, 1)  # Convert to NumPy array and reshape\n",
        "y_val_np = y_val.to_numpy().reshape(-1, 1)\n",
        "y_test_np = y_test.to_numpy().reshape(-1, 1)\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_enc = encoder.fit_transform(y_train_np)\n",
        "y_val_enc = encoder.transform(y_val_np)\n",
        "y_test_enc = encoder.transform(y_test_np)\n",
        "\n",
        "# Train the ELM model (assuming X_train is a NumPy array now)\n",
        "model_ELM = ELM(X_train_np.shape[1], y_train_enc.shape[1], batch=256, accelerator=\"basic\", classification=\"c\")\n",
        "model_ELM.add_neurons(num_neuronas, tipo_activacion)\n",
        "model_ELM.train(X_train_np, y_train_enc, \"c\")\n",
        "\n",
        "# Evaluar el modelo en los datos de entrenamiento, evaluación y test\n",
        "y_train_pred = model_ELM.predict(X_train_np)\n",
        "y_val_pred = model_ELM.predict(X_val_np)\n",
        "y_test_pred = model_ELM.predict(X_test_np)\n",
        "\n",
        "# Convertir las predicciones a etiquetas de clase\n",
        "y_train_pred_labels = y_train_pred.argmax(axis=1)\n",
        "y_val_pred_labels = y_val_pred.argmax(axis=1)\n",
        "y_test_pred_labels = y_test_pred.argmax(axis=1)\n",
        "\n",
        "# Convertir las etiquetas verdaderas a etiquetas de clase\n",
        "y_train_true_labels = y_train_enc.argmax(axis=1)\n",
        "y_val_true_labels = y_val_enc.argmax(axis=1)\n",
        "y_test_true_labels = y_test_enc.argmax(axis=1)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_true_labels, y_train_pred_labels, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla(y_val_true_labels, y_val_pred_labels, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla(y_test_true_labels, y_test_pred_labels, \"Test\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perceptrón multicapa (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptrón multicapa (MLP)\n",
            "Epoch 1/25\n",
            "35/35 [==============================] - 1s 6ms/step - loss: 916.1649 - accuracy: 0.5354 - val_loss: 278.1741 - val_accuracy: 0.5220\n",
            "Epoch 2/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 219.3460 - accuracy: 0.6171 - val_loss: 151.3920 - val_accuracy: 0.6154\n",
            "Epoch 3/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 145.5412 - accuracy: 0.6272 - val_loss: 193.0987 - val_accuracy: 0.5989\n",
            "Epoch 4/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 140.0982 - accuracy: 0.6327 - val_loss: 188.9885 - val_accuracy: 0.5879\n",
            "Epoch 5/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 129.4796 - accuracy: 0.6354 - val_loss: 314.6234 - val_accuracy: 0.5440\n",
            "Epoch 6/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 240.6479 - accuracy: 0.6345 - val_loss: 88.4473 - val_accuracy: 0.6209\n",
            "Epoch 7/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 115.4109 - accuracy: 0.6483 - val_loss: 84.5267 - val_accuracy: 0.6429\n",
            "Epoch 8/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 62.0018 - accuracy: 0.6575 - val_loss: 96.2542 - val_accuracy: 0.5632\n",
            "Epoch 9/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 66.3903 - accuracy: 0.6465 - val_loss: 325.9929 - val_accuracy: 0.5604\n",
            "Epoch 10/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 165.7227 - accuracy: 0.6235 - val_loss: 321.2566 - val_accuracy: 0.5495\n",
            "Epoch 11/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 149.4566 - accuracy: 0.6198 - val_loss: 288.3364 - val_accuracy: 0.5302\n",
            "Epoch 12/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 201.9241 - accuracy: 0.6134 - val_loss: 145.3068 - val_accuracy: 0.4725\n",
            "Epoch 13/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 82.6805 - accuracy: 0.6566 - val_loss: 143.4971 - val_accuracy: 0.5879\n",
            "Epoch 14/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 77.4794 - accuracy: 0.6575 - val_loss: 266.9074 - val_accuracy: 0.5302\n",
            "Epoch 15/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 101.8203 - accuracy: 0.6630 - val_loss: 103.6395 - val_accuracy: 0.6923\n",
            "Epoch 16/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 103.8685 - accuracy: 0.6272 - val_loss: 88.3204 - val_accuracy: 0.6648\n",
            "Epoch 17/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 205.5811 - accuracy: 0.6410 - val_loss: 157.9670 - val_accuracy: 0.5852\n",
            "Epoch 18/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 115.3850 - accuracy: 0.6345 - val_loss: 67.4011 - val_accuracy: 0.5797\n",
            "Epoch 19/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 51.4708 - accuracy: 0.6639 - val_loss: 195.5797 - val_accuracy: 0.5797\n",
            "Epoch 20/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 130.8036 - accuracy: 0.6143 - val_loss: 145.2574 - val_accuracy: 0.5934\n",
            "Epoch 21/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 66.6486 - accuracy: 0.6602 - val_loss: 101.0719 - val_accuracy: 0.5852\n",
            "Epoch 22/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 82.5484 - accuracy: 0.6465 - val_loss: 189.6921 - val_accuracy: 0.5879\n",
            "Epoch 23/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 76.8172 - accuracy: 0.6667 - val_loss: 64.0897 - val_accuracy: 0.5742\n",
            "Epoch 24/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 51.1676 - accuracy: 0.6575 - val_loss: 132.1871 - val_accuracy: 0.5714\n",
            "Epoch 25/25\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 100.0953 - accuracy: 0.6354 - val_loss: 161.8632 - val_accuracy: 0.5467\n",
            "35/35 [==============================] - 0s 762us/step\n",
            "12/12 [==============================] - 0s 796us/step\n",
            "12/12 [==============================] - 0s 866us/step\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.6033\n",
            " - Precision: 0.7671\n",
            " - Recall: 0.6033\n",
            " - F1-Score: 0.4895\n",
            " - Adjusted Rand Index: 0.0291\n",
            " - Mean Squared Error: 0.3967\n",
            " - R-squared: -0.6131\n",
            " - Área bajo la curva : 0.545\n",
            " - Confusion Matrix: \n",
            "[[ 43 432]\n",
            " [  0 614]]\n",
            " - Global Score : 51.28\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.5467\n",
            " - Precision: 0.6970\n",
            " - Recall: 0.5467\n",
            " - F1-Score: 0.4076\n",
            " - Adjusted Rand Index: 0.0052\n",
            " - Mean Squared Error: 0.4533\n",
            " - R-squared: -0.8198\n",
            " - Área bajo la curva : 0.518\n",
            " - Confusion Matrix: \n",
            "[[  7 164]\n",
            " [  1 192]]\n",
            " - Global Score : 46.18\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.5879\n",
            " - Precision: 0.7404\n",
            " - Recall: 0.5879\n",
            " - F1-Score: 0.4782\n",
            " - Adjusted Rand Index: 0.0243\n",
            " - Mean Squared Error: 0.4121\n",
            " - R-squared: -0.6612\n",
            " - Área bajo la curva : 0.549\n",
            " - Confusion Matrix: \n",
            "[[ 17 149]\n",
            " [  1 197]]\n",
            " - Global Score : 50.32\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.890577</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.889909</td>\n",
              "      <td>0.607662</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.558828</td>\n",
              "      <td>0.888374</td>\n",
              "      <td>147</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>177</td>\n",
              "      <td>85.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928693</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928604</td>\n",
              "      <td>0.733959</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.712060</td>\n",
              "      <td>0.928502</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>184</td>\n",
              "      <td>90.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977957</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977956</td>\n",
              "      <td>0.913688</td>\n",
              "      <td>0.022039</td>\n",
              "      <td>0.910386</td>\n",
              "      <td>0.977358</td>\n",
              "      <td>462</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>603</td>\n",
              "      <td>97.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.929217</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928441</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>0.926976</td>\n",
              "      <td>154</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>184</td>\n",
              "      <td>90.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942345</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942262</td>\n",
              "      <td>0.781936</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.767433</td>\n",
              "      <td>0.941128</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>189</td>\n",
              "      <td>92.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945198</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945012</td>\n",
              "      <td>0.791723</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.779414</td>\n",
              "      <td>0.944187</td>\n",
              "      <td>159</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>185</td>\n",
              "      <td>92.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931298</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931301</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.068681</td>\n",
              "      <td>0.723135</td>\n",
              "      <td>0.930540</td>\n",
              "      <td>153</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>186</td>\n",
              "      <td>90.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.961706</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.605211</td>\n",
              "      <td>-0.011995</td>\n",
              "      <td>0.522498</td>\n",
              "      <td>-12.189464</td>\n",
              "      <td>0.727490</td>\n",
              "      <td>475</td>\n",
              "      <td>569</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>27.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.961994</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.636533</td>\n",
              "      <td>-0.003037</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>-11.376695</td>\n",
              "      <td>0.744986</td>\n",
              "      <td>171</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>31.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.969031</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.623565</td>\n",
              "      <td>-0.006656</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>-15.028409</td>\n",
              "      <td>0.735795</td>\n",
              "      <td>166</td>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>21.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.843900</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.834459</td>\n",
              "      <td>0.445165</td>\n",
              "      <td>0.166208</td>\n",
              "      <td>0.324159</td>\n",
              "      <td>0.840453</td>\n",
              "      <td>424</td>\n",
              "      <td>51</td>\n",
              "      <td>130</td>\n",
              "      <td>484</td>\n",
              "      <td>78.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.826912</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.824325</td>\n",
              "      <td>0.418764</td>\n",
              "      <td>0.175824</td>\n",
              "      <td>0.294125</td>\n",
              "      <td>0.825864</td>\n",
              "      <td>146</td>\n",
              "      <td>25</td>\n",
              "      <td>39</td>\n",
              "      <td>154</td>\n",
              "      <td>77.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.851340</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.834986</td>\n",
              "      <td>0.447799</td>\n",
              "      <td>0.164835</td>\n",
              "      <td>0.335524</td>\n",
              "      <td>0.842643</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>79.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.851240</td>\n",
              "      <td>0.852591</td>\n",
              "      <td>0.851240</td>\n",
              "      <td>0.851566</td>\n",
              "      <td>0.492959</td>\n",
              "      <td>0.148760</td>\n",
              "      <td>0.395104</td>\n",
              "      <td>0.851397</td>\n",
              "      <td>405</td>\n",
              "      <td>70</td>\n",
              "      <td>92</td>\n",
              "      <td>522</td>\n",
              "      <td>80.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.810440</td>\n",
              "      <td>0.810888</td>\n",
              "      <td>0.810440</td>\n",
              "      <td>0.809916</td>\n",
              "      <td>0.383791</td>\n",
              "      <td>0.189560</td>\n",
              "      <td>0.238978</td>\n",
              "      <td>0.807911</td>\n",
              "      <td>131</td>\n",
              "      <td>40</td>\n",
              "      <td>29</td>\n",
              "      <td>164</td>\n",
              "      <td>75.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.829039</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827226</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.828252</td>\n",
              "      <td>140</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>161</td>\n",
              "      <td>77.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.712580</td>\n",
              "      <td>0.710941</td>\n",
              "      <td>0.712580</td>\n",
              "      <td>0.710035</td>\n",
              "      <td>0.179381</td>\n",
              "      <td>0.287420</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.701505</td>\n",
              "      <td>292</td>\n",
              "      <td>183</td>\n",
              "      <td>130</td>\n",
              "      <td>484</td>\n",
              "      <td>64.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.675824</td>\n",
              "      <td>0.680368</td>\n",
              "      <td>0.675824</td>\n",
              "      <td>0.669575</td>\n",
              "      <td>0.121182</td>\n",
              "      <td>0.324176</td>\n",
              "      <td>-0.301457</td>\n",
              "      <td>0.667970</td>\n",
              "      <td>92</td>\n",
              "      <td>79</td>\n",
              "      <td>39</td>\n",
              "      <td>154</td>\n",
              "      <td>60.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.677529</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.676174</td>\n",
              "      <td>0.124998</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>-0.295728</td>\n",
              "      <td>0.671443</td>\n",
              "      <td>98</td>\n",
              "      <td>68</td>\n",
              "      <td>49</td>\n",
              "      <td>149</td>\n",
              "      <td>60.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>Perceptrón multicapa (MLP)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.603306</td>\n",
              "      <td>0.767141</td>\n",
              "      <td>0.603306</td>\n",
              "      <td>0.489507</td>\n",
              "      <td>0.029137</td>\n",
              "      <td>0.396694</td>\n",
              "      <td>-0.613057</td>\n",
              "      <td>0.545263</td>\n",
              "      <td>43</td>\n",
              "      <td>432</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>51.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Perceptrón multicapa (MLP)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.546703</td>\n",
              "      <td>0.697019</td>\n",
              "      <td>0.546703</td>\n",
              "      <td>0.407607</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.453297</td>\n",
              "      <td>-0.819835</td>\n",
              "      <td>0.517877</td>\n",
              "      <td>7</td>\n",
              "      <td>164</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>46.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Perceptrón multicapa (MLP)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.587912</td>\n",
              "      <td>0.740417</td>\n",
              "      <td>0.587912</td>\n",
              "      <td>0.478237</td>\n",
              "      <td>0.024290</td>\n",
              "      <td>0.412088</td>\n",
              "      <td>-0.661190</td>\n",
              "      <td>0.548680</td>\n",
              "      <td>17</td>\n",
              "      <td>149</td>\n",
              "      <td>1</td>\n",
              "      <td>197</td>\n",
              "      <td>50.32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Model         Set  Accuracy  Precision  \\\n",
              "0                  Regresion Lineal    Training  0.795225   0.825122   \n",
              "1                  Regresion Lineal  Validation  0.815934   0.827130   \n",
              "2                  Regresion Lineal        Test  0.793956   0.830307   \n",
              "3               Regresión Logística    Training  0.835629   0.843677   \n",
              "4               Regresión Logística  Validation  0.826923   0.828750   \n",
              "5               Regresión Logística        Test  0.826923   0.839082   \n",
              "6                 Árbol de Decisión    Training  0.943067   0.943447   \n",
              "7                 Árbol de Decisión  Validation  0.854396   0.856113   \n",
              "8                 Árbol de Decisión        Test  0.854396   0.854704   \n",
              "9                     Random Forest    Training  1.000000   1.000000   \n",
              "10                    Random Forest  Validation  0.887363   0.889476   \n",
              "11                    Random Forest        Test  0.901099   0.901102   \n",
              "12                              KNN    Training  0.852158   0.856418   \n",
              "13                              KNN  Validation  0.785714   0.787289   \n",
              "14                              KNN        Test  0.793956   0.799964   \n",
              "15                              SVN    Training  0.846648   0.850097   \n",
              "16                              SVN  Validation  0.821429   0.821337   \n",
              "17                              SVN        Test  0.821429   0.826572   \n",
              "18           Naive Bayes - Gaussian    Training  0.759412   0.831542   \n",
              "19           Naive Bayes - Gaussian  Validation  0.766484   0.809282   \n",
              "20           Naive Bayes - Gaussian        Test  0.758242   0.815378   \n",
              "21          Naive Bayes - Bernoulli    Training  0.814509   0.820274   \n",
              "22          Naive Bayes - Bernoulli  Validation  0.791209   0.791105   \n",
              "23          Naive Bayes - Bernoulli        Test  0.826923   0.833036   \n",
              "24                       AdaBoost 1    Training  1.000000   1.000000   \n",
              "25                       AdaBoost 1  Validation  0.890110   0.890577   \n",
              "26                       AdaBoost 1        Test  0.928571   0.928693   \n",
              "27                       AdaBoost 2    Training  0.977961   0.977957   \n",
              "28                       AdaBoost 2  Validation  0.928571   0.929217   \n",
              "29                       AdaBoost 2        Test  0.942308   0.942345   \n",
              "30                Gradient Boosting    Training  1.000000   1.000000   \n",
              "31                Gradient Boosting  Validation  0.945055   0.945198   \n",
              "32                Gradient Boosting        Test  0.931319   0.931298   \n",
              "33  Artificial Network Layers (ANN)    Training  0.477502   0.961706   \n",
              "34  Artificial Network Layers (ANN)  Validation  0.510989   0.961994   \n",
              "35  Artificial Network Layers (ANN)        Test  0.489011   0.969031   \n",
              "36                              SVR    Training  0.833792   0.843900   \n",
              "37                              SVR  Validation  0.824176   0.826912   \n",
              "38                              SVR        Test  0.835165   0.851340   \n",
              "39             Regresión polinomial    Training  0.851240   0.852591   \n",
              "40             Regresión polinomial  Validation  0.810440   0.810888   \n",
              "41             Regresión polinomial        Test  0.826923   0.829039   \n",
              "42   Extreme Learning Machine (ELM)    Training  0.712580   0.710941   \n",
              "43   Extreme Learning Machine (ELM)  Validation  0.675824   0.680368   \n",
              "44   Extreme Learning Machine (ELM)        Test  0.678571   0.677529   \n",
              "45       Perceptrón multicapa (MLP)    Training  0.603306   0.767141   \n",
              "46       Perceptrón multicapa (MLP)  Validation  0.546703   0.697019   \n",
              "47       Perceptrón multicapa (MLP)        Test  0.587912   0.740417   \n",
              "\n",
              "      Recall  F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared  \\\n",
              "0   0.795225  0.794968             0.347828            0.204775   0.167334   \n",
              "1   0.815934  0.815546             0.397597            0.184066   0.261037   \n",
              "2   0.793956  0.791730             0.343680            0.206044   0.169405   \n",
              "3   0.835629  0.836301             0.450090            0.164371   0.331627   \n",
              "4   0.826923  0.827081             0.425939            0.173077   0.305154   \n",
              "5   0.826923  0.826964             0.425926            0.173077   0.302300   \n",
              "6   0.943067  0.943141             0.785002            0.056933   0.768496   \n",
              "7   0.854396  0.853779             0.501006            0.145604   0.415447   \n",
              "8   0.854396  0.854492             0.501008            0.145604   0.413046   \n",
              "9   1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "10  0.887363  0.886886             0.599091            0.112637   0.547799   \n",
              "11  0.901099  0.900990             0.642523            0.098901   0.601314   \n",
              "12  0.852158  0.852701             0.495592            0.147842   0.398838   \n",
              "13  0.785714  0.785909             0.324677            0.214286   0.139715   \n",
              "14  0.793956  0.794316             0.343845            0.206044   0.169405   \n",
              "15  0.846648  0.847172             0.480168            0.153352   0.376434   \n",
              "16  0.821429  0.821327             0.411647            0.178571   0.283095   \n",
              "17  0.821429  0.821762             0.411656            0.178571   0.280151   \n",
              "18  0.759412  0.754310             0.267385            0.240588   0.021711   \n",
              "19  0.766484  0.761488             0.282026            0.233516   0.062509   \n",
              "20  0.758242  0.752494             0.264371            0.241758   0.025435   \n",
              "21  0.814509  0.815240             0.395110            0.185491   0.245747   \n",
              "22  0.791209  0.791133             0.337388            0.208791   0.161773   \n",
              "23  0.826923  0.827225             0.425943            0.173077   0.302300   \n",
              "24  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "25  0.890110  0.889909             0.607662            0.109890   0.558828   \n",
              "26  0.928571  0.928604             0.733959            0.071429   0.712060   \n",
              "27  0.977961  0.977956             0.913688            0.022039   0.910386   \n",
              "28  0.928571  0.928441             0.733961            0.071429   0.713238   \n",
              "29  0.942308  0.942262             0.781936            0.057692   0.767433   \n",
              "30  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "31  0.945055  0.945012             0.791723            0.054945   0.779414   \n",
              "32  0.931319  0.931301             0.743431            0.068681   0.723135   \n",
              "33  0.477502  0.605211            -0.011995            0.522498 -12.189464   \n",
              "34  0.510989  0.636533            -0.003037            0.489011 -11.376695   \n",
              "35  0.489011  0.623565            -0.006656            0.510989 -15.028409   \n",
              "36  0.833792  0.834459             0.445165            0.166208   0.324159   \n",
              "37  0.824176  0.824325             0.418764            0.175824   0.294125   \n",
              "38  0.835165  0.834986             0.447799            0.164835   0.335524   \n",
              "39  0.851240  0.851566             0.492959            0.148760   0.395104   \n",
              "40  0.810440  0.809916             0.383791            0.189560   0.238978   \n",
              "41  0.826923  0.827226             0.425943            0.173077   0.302300   \n",
              "42  0.712580  0.710035             0.179381            0.287420  -0.168719   \n",
              "43  0.675824  0.669575             0.121182            0.324176  -0.301457   \n",
              "44  0.678571  0.676174             0.124998            0.321429  -0.295728   \n",
              "45  0.603306  0.489507             0.029137            0.396694  -0.613057   \n",
              "46  0.546703  0.407607             0.005168            0.453297  -0.819835   \n",
              "47  0.587912  0.478237             0.024290            0.412088  -0.661190   \n",
              "\n",
              "     AUC-ROC   TN   FP   FN   TP  Global Score  \n",
              "0   0.809587  438   37  186  428         74.81  \n",
              "1   0.820425  153   18   49  144         76.76  \n",
              "2   0.806225  157    9   66  132         74.65  \n",
              "3   0.841128  420   55  124  490         79.14  \n",
              "4   0.828122  145   26   37  156         77.87  \n",
              "5   0.833120  150   16   47  151         78.15  \n",
              "6   0.943554  450   25   37  577         92.45  \n",
              "7   0.851362  137   34   19  174         81.09  \n",
              "8   0.853992  141   25   28  170         81.16  \n",
              "9   1.000000  475    0    0  614        100.00  \n",
              "10  0.884450  143   28   13  180         85.21  \n",
              "11  0.899355  146   20   16  182         86.96  \n",
              "12  0.855309  418   57  104  510         81.04  \n",
              "13  0.786595  137   34   44  149         72.93  \n",
              "14  0.797463  139   27   48  150         74.04  \n",
              "15  0.848994  412   63  104  510         80.33  \n",
              "16  0.820274  137   34   31  162         77.11  \n",
              "17  0.824662  143   23   42  156         77.31  \n",
              "18  0.784262  465   10  252  362         71.38  \n",
              "19  0.776460  161   10   75  118         71.40  \n",
              "20  0.774370  159    7   81  117         70.79  \n",
              "21  0.818109  402   73  129  485         76.49  \n",
              "22  0.790110  132   39   37  156         73.49  \n",
              "23  0.830686  145   21   42  156         78.00  \n",
              "24  1.000000  475    0    0  614        100.00  \n",
              "25  0.888374  147   24   16  177         85.58  \n",
              "26  0.928502  154   12   14  184         90.55  \n",
              "27  0.977358  462   13   11  603         97.02  \n",
              "28  0.926976  154   17    9  184         90.51  \n",
              "29  0.941128  154   12    9  189         92.30  \n",
              "30  1.000000  475    0    0  614        100.00  \n",
              "31  0.944187  159   12    8  185         92.67  \n",
              "32  0.930540  153   13   12  186         90.88  \n",
              "33  0.727490  475  569    0   45         27.84  \n",
              "34  0.744986  171  178    0   15         31.87  \n",
              "35  0.735795  166  186    0   12         21.70  \n",
              "36  0.840453  424   51  130  484         78.98  \n",
              "37  0.825864  146   25   39  154         77.57  \n",
              "38  0.842643  154   12   48  150         79.24  \n",
              "39  0.851397  405   70   92  522         80.80  \n",
              "40  0.807911  131   40   29  164         75.74  \n",
              "41  0.828252  140   26   37  161         77.88  \n",
              "42  0.701505  292  183  130  484         64.04  \n",
              "43  0.667970   92   79   39  154         60.13  \n",
              "44  0.671443   98   68   49  149         60.42  \n",
              "45  0.545263   43  432    0  614         51.28  \n",
              "46  0.517877    7  164    1  192         46.18  \n",
              "47  0.548680   17  149    1  197         50.32  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define the MLP model\n",
        "model_MLP = Sequential()\n",
        "model_name = 'Perceptrón multicapa (MLP)'\n",
        "print(model_name) \n",
        "\n",
        "# Add hidden layers with appropriate number of neurons and activation functions\n",
        "model_MLP.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model_MLP.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Add the output layer with sigmoid activation for binary classification\n",
        "model_MLP.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_MLP.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_MLP.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the train, val y test set\n",
        "y_train_pred = model_MLP.predict(X_train)\n",
        "y_val_pred = model_MLP.predict(X_val)\n",
        "y_test_pred = model_MLP.predict(X_test)\n",
        "\n",
        "\n",
        "# Convert predictions to binary labels (0 or 1)\n",
        "y_train_pred_labels = np.where(y_train_pred > 0.5, 1, 0)\n",
        "y_val_pred_labels = np.where(y_val_pred > 0.5, 1, 0)\n",
        "y_test_pred_labels = np.where(y_test_pred > 0.5, 1, 0)\n",
        "\n",
        "# Convertir las etiquetas reales a formato binario (0 o 1) \n",
        "y_train_pred_labels = y_train_pred_labels.flatten()  \n",
        "y_val_pred_labels = y_val_pred_labels.flatten() \n",
        "y_test_pred_labels = y_test_pred_labels.flatten() \n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_test_pred_labels)\n",
        "precision = precision_score(y_test, y_test_pred_labels)\n",
        "recall = recall_score(y_test, y_test_pred_labels)\n",
        "f1 = f1_score(y_test, y_test_pred_labels)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred_labels, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred_labels, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred_labels, \"Test\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Red Neuronal Recurrente (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensiones de X_train: (1089, 9)\n",
            "Red Neuronal Recurrente (RNN)\n",
            "Epoch 1/25\n",
            "35/35 [==============================] - 1s 8ms/step - loss: 262.0164 - accuracy: 0.6088 - val_loss: 78.3056 - val_accuracy: 0.6346\n",
            "Epoch 2/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.0747 - accuracy: 0.6556 - val_loss: 33.1663 - val_accuracy: 0.6703\n",
            "Epoch 3/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.2511 - accuracy: 0.6639 - val_loss: 39.1396 - val_accuracy: 0.5495\n",
            "Epoch 4/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.2369 - accuracy: 0.6208 - val_loss: 26.6724 - val_accuracy: 0.6648\n",
            "Epoch 5/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.2267 - accuracy: 0.6226 - val_loss: 41.3026 - val_accuracy: 0.5824\n",
            "Epoch 6/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.1019 - accuracy: 0.6685 - val_loss: 28.0822 - val_accuracy: 0.5275\n",
            "Epoch 7/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.4688 - accuracy: 0.6171 - val_loss: 100.9656 - val_accuracy: 0.5687\n",
            "Epoch 8/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.5216 - accuracy: 0.6189 - val_loss: 77.2375 - val_accuracy: 0.5577\n",
            "Epoch 9/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.1324 - accuracy: 0.6455 - val_loss: 42.3859 - val_accuracy: 0.6236\n",
            "Epoch 10/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.3728 - accuracy: 0.6474 - val_loss: 79.2123 - val_accuracy: 0.6374\n",
            "Epoch 11/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.1969 - accuracy: 0.6538 - val_loss: 25.1319 - val_accuracy: 0.6016\n",
            "Epoch 12/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.6482 - accuracy: 0.5859 - val_loss: 42.1383 - val_accuracy: 0.6291\n",
            "Epoch 13/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.0012 - accuracy: 0.6263 - val_loss: 116.8067 - val_accuracy: 0.5522\n",
            "Epoch 14/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 54.7371 - accuracy: 0.6088 - val_loss: 22.1682 - val_accuracy: 0.6566\n",
            "Epoch 15/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 37.1612 - accuracy: 0.6152 - val_loss: 46.7926 - val_accuracy: 0.5302\n",
            "Epoch 16/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.1332 - accuracy: 0.6162 - val_loss: 19.5801 - val_accuracy: 0.6648\n",
            "Epoch 17/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.9067 - accuracy: 0.6180 - val_loss: 54.8498 - val_accuracy: 0.5989\n",
            "Epoch 18/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.8823 - accuracy: 0.6217 - val_loss: 42.6745 - val_accuracy: 0.6538\n",
            "Epoch 19/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 38.9238 - accuracy: 0.6198 - val_loss: 77.9854 - val_accuracy: 0.6044\n",
            "Epoch 20/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 85.0119 - accuracy: 0.5813 - val_loss: 55.8449 - val_accuracy: 0.6978\n",
            "Epoch 21/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 54.9198 - accuracy: 0.6272 - val_loss: 25.0317 - val_accuracy: 0.6813\n",
            "Epoch 22/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.8895 - accuracy: 0.6318 - val_loss: 9.5138 - val_accuracy: 0.5027\n",
            "Epoch 23/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.4307 - accuracy: 0.6217 - val_loss: 21.8446 - val_accuracy: 0.5247\n",
            "Epoch 24/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.1864 - accuracy: 0.6107 - val_loss: 19.9223 - val_accuracy: 0.6786\n",
            "Epoch 25/25\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.7340 - accuracy: 0.6217 - val_loss: 8.4563 - val_accuracy: 0.7143\n",
            "35/35 [==============================] - 0s 991us/step\n",
            "12/12 [==============================] - 0s 1ms/step\n",
            "12/12 [==============================] - 0s 1ms/step\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.7236\n",
            " - Precision: 0.7283\n",
            " - Recall: 0.7236\n",
            " - F1-Score: 0.7246\n",
            " - Adjusted Rand Index: 0.1992\n",
            " - Mean Squared Error: 0.2764\n",
            " - R-squared: -0.1239\n",
            " - Área bajo la curva : 0.725\n",
            " - Confusion Matrix: \n",
            "[[348 127]\n",
            " [174 440]]\n",
            " - Global Score : 65.79\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.7143\n",
            " - Precision: 0.7143\n",
            " - Recall: 0.7143\n",
            " - F1-Score: 0.7143\n",
            " - Adjusted Rand Index: 0.1814\n",
            " - Mean Squared Error: 0.2857\n",
            " - R-squared: -0.1470\n",
            " - Área bajo la curva : 0.713\n",
            " - Confusion Matrix: \n",
            "[[119  52]\n",
            " [ 52 141]]\n",
            " - Global Score : 64.63\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.6951\n",
            " - Precision: 0.6974\n",
            " - Recall: 0.6951\n",
            " - F1-Score: 0.6956\n",
            " - Adjusted Rand Index: 0.1499\n",
            " - Mean Squared Error: 0.3049\n",
            " - R-squared: -0.2293\n",
            " - Área bajo la curva : 0.695\n",
            " - Confusion Matrix: \n",
            "[[116  50]\n",
            " [ 61 137]]\n",
            " - Global Score : 62.55\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.825122</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.204775</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.809587</td>\n",
              "      <td>438</td>\n",
              "      <td>37</td>\n",
              "      <td>186</td>\n",
              "      <td>428</td>\n",
              "      <td>74.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.827130</td>\n",
              "      <td>0.815934</td>\n",
              "      <td>0.815546</td>\n",
              "      <td>0.397597</td>\n",
              "      <td>0.184066</td>\n",
              "      <td>0.261037</td>\n",
              "      <td>0.820425</td>\n",
              "      <td>153</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>144</td>\n",
              "      <td>76.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.830307</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.791730</td>\n",
              "      <td>0.343680</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.806225</td>\n",
              "      <td>157</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>74.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.843677</td>\n",
              "      <td>0.835629</td>\n",
              "      <td>0.836301</td>\n",
              "      <td>0.450090</td>\n",
              "      <td>0.164371</td>\n",
              "      <td>0.331627</td>\n",
              "      <td>0.841128</td>\n",
              "      <td>420</td>\n",
              "      <td>55</td>\n",
              "      <td>124</td>\n",
              "      <td>490</td>\n",
              "      <td>79.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827081</td>\n",
              "      <td>0.425939</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.305154</td>\n",
              "      <td>0.828122</td>\n",
              "      <td>145</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>77.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.839082</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.833120</td>\n",
              "      <td>150</td>\n",
              "      <td>16</td>\n",
              "      <td>47</td>\n",
              "      <td>151</td>\n",
              "      <td>78.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943447</td>\n",
              "      <td>0.943067</td>\n",
              "      <td>0.943141</td>\n",
              "      <td>0.785002</td>\n",
              "      <td>0.056933</td>\n",
              "      <td>0.768496</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>450</td>\n",
              "      <td>25</td>\n",
              "      <td>37</td>\n",
              "      <td>577</td>\n",
              "      <td>92.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.856113</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.853779</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.415447</td>\n",
              "      <td>0.851362</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>19</td>\n",
              "      <td>174</td>\n",
              "      <td>81.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854704</td>\n",
              "      <td>0.854396</td>\n",
              "      <td>0.854492</td>\n",
              "      <td>0.501008</td>\n",
              "      <td>0.145604</td>\n",
              "      <td>0.413046</td>\n",
              "      <td>0.853992</td>\n",
              "      <td>141</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>170</td>\n",
              "      <td>81.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.887363</td>\n",
              "      <td>0.886886</td>\n",
              "      <td>0.599091</td>\n",
              "      <td>0.112637</td>\n",
              "      <td>0.547799</td>\n",
              "      <td>0.884450</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>180</td>\n",
              "      <td>85.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.901099</td>\n",
              "      <td>0.900990</td>\n",
              "      <td>0.642523</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.601314</td>\n",
              "      <td>0.899355</td>\n",
              "      <td>146</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>182</td>\n",
              "      <td>86.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.856418</td>\n",
              "      <td>0.852158</td>\n",
              "      <td>0.852701</td>\n",
              "      <td>0.495592</td>\n",
              "      <td>0.147842</td>\n",
              "      <td>0.398838</td>\n",
              "      <td>0.855309</td>\n",
              "      <td>418</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>81.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.787289</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785909</td>\n",
              "      <td>0.324677</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.139715</td>\n",
              "      <td>0.786595</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>149</td>\n",
              "      <td>72.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.799964</td>\n",
              "      <td>0.793956</td>\n",
              "      <td>0.794316</td>\n",
              "      <td>0.343845</td>\n",
              "      <td>0.206044</td>\n",
              "      <td>0.169405</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>139</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>74.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.850097</td>\n",
              "      <td>0.846648</td>\n",
              "      <td>0.847172</td>\n",
              "      <td>0.480168</td>\n",
              "      <td>0.153352</td>\n",
              "      <td>0.376434</td>\n",
              "      <td>0.848994</td>\n",
              "      <td>412</td>\n",
              "      <td>63</td>\n",
              "      <td>104</td>\n",
              "      <td>510</td>\n",
              "      <td>80.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821337</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821327</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.283095</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>137</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>162</td>\n",
              "      <td>77.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.826572</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.821762</td>\n",
              "      <td>0.411656</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.280151</td>\n",
              "      <td>0.824662</td>\n",
              "      <td>143</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>77.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.831542</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>0.754310</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>0.240588</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.784262</td>\n",
              "      <td>465</td>\n",
              "      <td>10</td>\n",
              "      <td>252</td>\n",
              "      <td>362</td>\n",
              "      <td>71.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.809282</td>\n",
              "      <td>0.766484</td>\n",
              "      <td>0.761488</td>\n",
              "      <td>0.282026</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.062509</td>\n",
              "      <td>0.776460</td>\n",
              "      <td>161</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "      <td>118</td>\n",
              "      <td>71.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>0.758242</td>\n",
              "      <td>0.752494</td>\n",
              "      <td>0.264371</td>\n",
              "      <td>0.241758</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>0.774370</td>\n",
              "      <td>159</td>\n",
              "      <td>7</td>\n",
              "      <td>81</td>\n",
              "      <td>117</td>\n",
              "      <td>70.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.820274</td>\n",
              "      <td>0.814509</td>\n",
              "      <td>0.815240</td>\n",
              "      <td>0.395110</td>\n",
              "      <td>0.185491</td>\n",
              "      <td>0.245747</td>\n",
              "      <td>0.818109</td>\n",
              "      <td>402</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>485</td>\n",
              "      <td>76.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791105</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.337388</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.161773</td>\n",
              "      <td>0.790110</td>\n",
              "      <td>132</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>156</td>\n",
              "      <td>73.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827225</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.830686</td>\n",
              "      <td>145</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>156</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.890577</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.889909</td>\n",
              "      <td>0.607662</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.558828</td>\n",
              "      <td>0.888374</td>\n",
              "      <td>147</td>\n",
              "      <td>24</td>\n",
              "      <td>16</td>\n",
              "      <td>177</td>\n",
              "      <td>85.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928693</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928604</td>\n",
              "      <td>0.733959</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.712060</td>\n",
              "      <td>0.928502</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>184</td>\n",
              "      <td>90.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977957</td>\n",
              "      <td>0.977961</td>\n",
              "      <td>0.977956</td>\n",
              "      <td>0.913688</td>\n",
              "      <td>0.022039</td>\n",
              "      <td>0.910386</td>\n",
              "      <td>0.977358</td>\n",
              "      <td>462</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>603</td>\n",
              "      <td>97.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.929217</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928441</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>0.926976</td>\n",
              "      <td>154</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>184</td>\n",
              "      <td>90.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>AdaBoost 2</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942345</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.942262</td>\n",
              "      <td>0.781936</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.767433</td>\n",
              "      <td>0.941128</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>189</td>\n",
              "      <td>92.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>475</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945198</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.945012</td>\n",
              "      <td>0.791723</td>\n",
              "      <td>0.054945</td>\n",
              "      <td>0.779414</td>\n",
              "      <td>0.944187</td>\n",
              "      <td>159</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>185</td>\n",
              "      <td>92.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931298</td>\n",
              "      <td>0.931319</td>\n",
              "      <td>0.931301</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.068681</td>\n",
              "      <td>0.723135</td>\n",
              "      <td>0.930540</td>\n",
              "      <td>153</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>186</td>\n",
              "      <td>90.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.961706</td>\n",
              "      <td>0.477502</td>\n",
              "      <td>0.605211</td>\n",
              "      <td>-0.011995</td>\n",
              "      <td>0.522498</td>\n",
              "      <td>-12.189464</td>\n",
              "      <td>0.727490</td>\n",
              "      <td>475</td>\n",
              "      <td>569</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>27.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.961994</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>0.636533</td>\n",
              "      <td>-0.003037</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>-11.376695</td>\n",
              "      <td>0.744986</td>\n",
              "      <td>171</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>31.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Artificial Network Layers (ANN)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.969031</td>\n",
              "      <td>0.489011</td>\n",
              "      <td>0.623565</td>\n",
              "      <td>-0.006656</td>\n",
              "      <td>0.510989</td>\n",
              "      <td>-15.028409</td>\n",
              "      <td>0.735795</td>\n",
              "      <td>166</td>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>21.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.843900</td>\n",
              "      <td>0.833792</td>\n",
              "      <td>0.834459</td>\n",
              "      <td>0.445165</td>\n",
              "      <td>0.166208</td>\n",
              "      <td>0.324159</td>\n",
              "      <td>0.840453</td>\n",
              "      <td>424</td>\n",
              "      <td>51</td>\n",
              "      <td>130</td>\n",
              "      <td>484</td>\n",
              "      <td>78.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.826912</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.824325</td>\n",
              "      <td>0.418764</td>\n",
              "      <td>0.175824</td>\n",
              "      <td>0.294125</td>\n",
              "      <td>0.825864</td>\n",
              "      <td>146</td>\n",
              "      <td>25</td>\n",
              "      <td>39</td>\n",
              "      <td>154</td>\n",
              "      <td>77.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>SVR</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.851340</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.834986</td>\n",
              "      <td>0.447799</td>\n",
              "      <td>0.164835</td>\n",
              "      <td>0.335524</td>\n",
              "      <td>0.842643</td>\n",
              "      <td>154</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>150</td>\n",
              "      <td>79.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.851240</td>\n",
              "      <td>0.852591</td>\n",
              "      <td>0.851240</td>\n",
              "      <td>0.851566</td>\n",
              "      <td>0.492959</td>\n",
              "      <td>0.148760</td>\n",
              "      <td>0.395104</td>\n",
              "      <td>0.851397</td>\n",
              "      <td>405</td>\n",
              "      <td>70</td>\n",
              "      <td>92</td>\n",
              "      <td>522</td>\n",
              "      <td>80.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.810440</td>\n",
              "      <td>0.810888</td>\n",
              "      <td>0.810440</td>\n",
              "      <td>0.809916</td>\n",
              "      <td>0.383791</td>\n",
              "      <td>0.189560</td>\n",
              "      <td>0.238978</td>\n",
              "      <td>0.807911</td>\n",
              "      <td>131</td>\n",
              "      <td>40</td>\n",
              "      <td>29</td>\n",
              "      <td>164</td>\n",
              "      <td>75.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Regresión polinomial</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.829039</td>\n",
              "      <td>0.826923</td>\n",
              "      <td>0.827226</td>\n",
              "      <td>0.425943</td>\n",
              "      <td>0.173077</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>0.828252</td>\n",
              "      <td>140</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>161</td>\n",
              "      <td>77.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.712580</td>\n",
              "      <td>0.710941</td>\n",
              "      <td>0.712580</td>\n",
              "      <td>0.710035</td>\n",
              "      <td>0.179381</td>\n",
              "      <td>0.287420</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.701505</td>\n",
              "      <td>292</td>\n",
              "      <td>183</td>\n",
              "      <td>130</td>\n",
              "      <td>484</td>\n",
              "      <td>64.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.675824</td>\n",
              "      <td>0.680368</td>\n",
              "      <td>0.675824</td>\n",
              "      <td>0.669575</td>\n",
              "      <td>0.121182</td>\n",
              "      <td>0.324176</td>\n",
              "      <td>-0.301457</td>\n",
              "      <td>0.667970</td>\n",
              "      <td>92</td>\n",
              "      <td>79</td>\n",
              "      <td>39</td>\n",
              "      <td>154</td>\n",
              "      <td>60.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.677529</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.676174</td>\n",
              "      <td>0.124998</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>-0.295728</td>\n",
              "      <td>0.671443</td>\n",
              "      <td>98</td>\n",
              "      <td>68</td>\n",
              "      <td>49</td>\n",
              "      <td>149</td>\n",
              "      <td>60.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>Perceptrón multicapa (MLP)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.603306</td>\n",
              "      <td>0.767141</td>\n",
              "      <td>0.603306</td>\n",
              "      <td>0.489507</td>\n",
              "      <td>0.029137</td>\n",
              "      <td>0.396694</td>\n",
              "      <td>-0.613057</td>\n",
              "      <td>0.545263</td>\n",
              "      <td>43</td>\n",
              "      <td>432</td>\n",
              "      <td>0</td>\n",
              "      <td>614</td>\n",
              "      <td>51.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Perceptrón multicapa (MLP)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.546703</td>\n",
              "      <td>0.697019</td>\n",
              "      <td>0.546703</td>\n",
              "      <td>0.407607</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.453297</td>\n",
              "      <td>-0.819835</td>\n",
              "      <td>0.517877</td>\n",
              "      <td>7</td>\n",
              "      <td>164</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>46.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Perceptrón multicapa (MLP)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.587912</td>\n",
              "      <td>0.740417</td>\n",
              "      <td>0.587912</td>\n",
              "      <td>0.478237</td>\n",
              "      <td>0.024290</td>\n",
              "      <td>0.412088</td>\n",
              "      <td>-0.661190</td>\n",
              "      <td>0.548680</td>\n",
              "      <td>17</td>\n",
              "      <td>149</td>\n",
              "      <td>1</td>\n",
              "      <td>197</td>\n",
              "      <td>50.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>Red Neuronal Recurrente (RNN)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.723600</td>\n",
              "      <td>0.728319</td>\n",
              "      <td>0.723600</td>\n",
              "      <td>0.724615</td>\n",
              "      <td>0.199242</td>\n",
              "      <td>0.276400</td>\n",
              "      <td>-0.123912</td>\n",
              "      <td>0.724622</td>\n",
              "      <td>348</td>\n",
              "      <td>127</td>\n",
              "      <td>174</td>\n",
              "      <td>440</td>\n",
              "      <td>65.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Red Neuronal Recurrente (RNN)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.181424</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>-0.147047</td>\n",
              "      <td>0.713238</td>\n",
              "      <td>119</td>\n",
              "      <td>52</td>\n",
              "      <td>52</td>\n",
              "      <td>141</td>\n",
              "      <td>64.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Red Neuronal Recurrente (RNN)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.695055</td>\n",
              "      <td>0.697390</td>\n",
              "      <td>0.695055</td>\n",
              "      <td>0.695588</td>\n",
              "      <td>0.149859</td>\n",
              "      <td>0.304945</td>\n",
              "      <td>-0.229281</td>\n",
              "      <td>0.695357</td>\n",
              "      <td>116</td>\n",
              "      <td>50</td>\n",
              "      <td>61</td>\n",
              "      <td>137</td>\n",
              "      <td>62.55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Model         Set  Accuracy  Precision  \\\n",
              "0                  Regresion Lineal    Training  0.795225   0.825122   \n",
              "1                  Regresion Lineal  Validation  0.815934   0.827130   \n",
              "2                  Regresion Lineal        Test  0.793956   0.830307   \n",
              "3               Regresión Logística    Training  0.835629   0.843677   \n",
              "4               Regresión Logística  Validation  0.826923   0.828750   \n",
              "5               Regresión Logística        Test  0.826923   0.839082   \n",
              "6                 Árbol de Decisión    Training  0.943067   0.943447   \n",
              "7                 Árbol de Decisión  Validation  0.854396   0.856113   \n",
              "8                 Árbol de Decisión        Test  0.854396   0.854704   \n",
              "9                     Random Forest    Training  1.000000   1.000000   \n",
              "10                    Random Forest  Validation  0.887363   0.889476   \n",
              "11                    Random Forest        Test  0.901099   0.901102   \n",
              "12                              KNN    Training  0.852158   0.856418   \n",
              "13                              KNN  Validation  0.785714   0.787289   \n",
              "14                              KNN        Test  0.793956   0.799964   \n",
              "15                              SVN    Training  0.846648   0.850097   \n",
              "16                              SVN  Validation  0.821429   0.821337   \n",
              "17                              SVN        Test  0.821429   0.826572   \n",
              "18           Naive Bayes - Gaussian    Training  0.759412   0.831542   \n",
              "19           Naive Bayes - Gaussian  Validation  0.766484   0.809282   \n",
              "20           Naive Bayes - Gaussian        Test  0.758242   0.815378   \n",
              "21          Naive Bayes - Bernoulli    Training  0.814509   0.820274   \n",
              "22          Naive Bayes - Bernoulli  Validation  0.791209   0.791105   \n",
              "23          Naive Bayes - Bernoulli        Test  0.826923   0.833036   \n",
              "24                       AdaBoost 1    Training  1.000000   1.000000   \n",
              "25                       AdaBoost 1  Validation  0.890110   0.890577   \n",
              "26                       AdaBoost 1        Test  0.928571   0.928693   \n",
              "27                       AdaBoost 2    Training  0.977961   0.977957   \n",
              "28                       AdaBoost 2  Validation  0.928571   0.929217   \n",
              "29                       AdaBoost 2        Test  0.942308   0.942345   \n",
              "30                Gradient Boosting    Training  1.000000   1.000000   \n",
              "31                Gradient Boosting  Validation  0.945055   0.945198   \n",
              "32                Gradient Boosting        Test  0.931319   0.931298   \n",
              "33  Artificial Network Layers (ANN)    Training  0.477502   0.961706   \n",
              "34  Artificial Network Layers (ANN)  Validation  0.510989   0.961994   \n",
              "35  Artificial Network Layers (ANN)        Test  0.489011   0.969031   \n",
              "36                              SVR    Training  0.833792   0.843900   \n",
              "37                              SVR  Validation  0.824176   0.826912   \n",
              "38                              SVR        Test  0.835165   0.851340   \n",
              "39             Regresión polinomial    Training  0.851240   0.852591   \n",
              "40             Regresión polinomial  Validation  0.810440   0.810888   \n",
              "41             Regresión polinomial        Test  0.826923   0.829039   \n",
              "42   Extreme Learning Machine (ELM)    Training  0.712580   0.710941   \n",
              "43   Extreme Learning Machine (ELM)  Validation  0.675824   0.680368   \n",
              "44   Extreme Learning Machine (ELM)        Test  0.678571   0.677529   \n",
              "45       Perceptrón multicapa (MLP)    Training  0.603306   0.767141   \n",
              "46       Perceptrón multicapa (MLP)  Validation  0.546703   0.697019   \n",
              "47       Perceptrón multicapa (MLP)        Test  0.587912   0.740417   \n",
              "48    Red Neuronal Recurrente (RNN)    Training  0.723600   0.728319   \n",
              "49    Red Neuronal Recurrente (RNN)  Validation  0.714286   0.714286   \n",
              "50    Red Neuronal Recurrente (RNN)        Test  0.695055   0.697390   \n",
              "\n",
              "      Recall  F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared  \\\n",
              "0   0.795225  0.794968             0.347828            0.204775   0.167334   \n",
              "1   0.815934  0.815546             0.397597            0.184066   0.261037   \n",
              "2   0.793956  0.791730             0.343680            0.206044   0.169405   \n",
              "3   0.835629  0.836301             0.450090            0.164371   0.331627   \n",
              "4   0.826923  0.827081             0.425939            0.173077   0.305154   \n",
              "5   0.826923  0.826964             0.425926            0.173077   0.302300   \n",
              "6   0.943067  0.943141             0.785002            0.056933   0.768496   \n",
              "7   0.854396  0.853779             0.501006            0.145604   0.415447   \n",
              "8   0.854396  0.854492             0.501008            0.145604   0.413046   \n",
              "9   1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "10  0.887363  0.886886             0.599091            0.112637   0.547799   \n",
              "11  0.901099  0.900990             0.642523            0.098901   0.601314   \n",
              "12  0.852158  0.852701             0.495592            0.147842   0.398838   \n",
              "13  0.785714  0.785909             0.324677            0.214286   0.139715   \n",
              "14  0.793956  0.794316             0.343845            0.206044   0.169405   \n",
              "15  0.846648  0.847172             0.480168            0.153352   0.376434   \n",
              "16  0.821429  0.821327             0.411647            0.178571   0.283095   \n",
              "17  0.821429  0.821762             0.411656            0.178571   0.280151   \n",
              "18  0.759412  0.754310             0.267385            0.240588   0.021711   \n",
              "19  0.766484  0.761488             0.282026            0.233516   0.062509   \n",
              "20  0.758242  0.752494             0.264371            0.241758   0.025435   \n",
              "21  0.814509  0.815240             0.395110            0.185491   0.245747   \n",
              "22  0.791209  0.791133             0.337388            0.208791   0.161773   \n",
              "23  0.826923  0.827225             0.425943            0.173077   0.302300   \n",
              "24  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "25  0.890110  0.889909             0.607662            0.109890   0.558828   \n",
              "26  0.928571  0.928604             0.733959            0.071429   0.712060   \n",
              "27  0.977961  0.977956             0.913688            0.022039   0.910386   \n",
              "28  0.928571  0.928441             0.733961            0.071429   0.713238   \n",
              "29  0.942308  0.942262             0.781936            0.057692   0.767433   \n",
              "30  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "31  0.945055  0.945012             0.791723            0.054945   0.779414   \n",
              "32  0.931319  0.931301             0.743431            0.068681   0.723135   \n",
              "33  0.477502  0.605211            -0.011995            0.522498 -12.189464   \n",
              "34  0.510989  0.636533            -0.003037            0.489011 -11.376695   \n",
              "35  0.489011  0.623565            -0.006656            0.510989 -15.028409   \n",
              "36  0.833792  0.834459             0.445165            0.166208   0.324159   \n",
              "37  0.824176  0.824325             0.418764            0.175824   0.294125   \n",
              "38  0.835165  0.834986             0.447799            0.164835   0.335524   \n",
              "39  0.851240  0.851566             0.492959            0.148760   0.395104   \n",
              "40  0.810440  0.809916             0.383791            0.189560   0.238978   \n",
              "41  0.826923  0.827226             0.425943            0.173077   0.302300   \n",
              "42  0.712580  0.710035             0.179381            0.287420  -0.168719   \n",
              "43  0.675824  0.669575             0.121182            0.324176  -0.301457   \n",
              "44  0.678571  0.676174             0.124998            0.321429  -0.295728   \n",
              "45  0.603306  0.489507             0.029137            0.396694  -0.613057   \n",
              "46  0.546703  0.407607             0.005168            0.453297  -0.819835   \n",
              "47  0.587912  0.478237             0.024290            0.412088  -0.661190   \n",
              "48  0.723600  0.724615             0.199242            0.276400  -0.123912   \n",
              "49  0.714286  0.714286             0.181424            0.285714  -0.147047   \n",
              "50  0.695055  0.695588             0.149859            0.304945  -0.229281   \n",
              "\n",
              "     AUC-ROC   TN   FP   FN   TP  Global Score  \n",
              "0   0.809587  438   37  186  428         74.81  \n",
              "1   0.820425  153   18   49  144         76.76  \n",
              "2   0.806225  157    9   66  132         74.65  \n",
              "3   0.841128  420   55  124  490         79.14  \n",
              "4   0.828122  145   26   37  156         77.87  \n",
              "5   0.833120  150   16   47  151         78.15  \n",
              "6   0.943554  450   25   37  577         92.45  \n",
              "7   0.851362  137   34   19  174         81.09  \n",
              "8   0.853992  141   25   28  170         81.16  \n",
              "9   1.000000  475    0    0  614        100.00  \n",
              "10  0.884450  143   28   13  180         85.21  \n",
              "11  0.899355  146   20   16  182         86.96  \n",
              "12  0.855309  418   57  104  510         81.04  \n",
              "13  0.786595  137   34   44  149         72.93  \n",
              "14  0.797463  139   27   48  150         74.04  \n",
              "15  0.848994  412   63  104  510         80.33  \n",
              "16  0.820274  137   34   31  162         77.11  \n",
              "17  0.824662  143   23   42  156         77.31  \n",
              "18  0.784262  465   10  252  362         71.38  \n",
              "19  0.776460  161   10   75  118         71.40  \n",
              "20  0.774370  159    7   81  117         70.79  \n",
              "21  0.818109  402   73  129  485         76.49  \n",
              "22  0.790110  132   39   37  156         73.49  \n",
              "23  0.830686  145   21   42  156         78.00  \n",
              "24  1.000000  475    0    0  614        100.00  \n",
              "25  0.888374  147   24   16  177         85.58  \n",
              "26  0.928502  154   12   14  184         90.55  \n",
              "27  0.977358  462   13   11  603         97.02  \n",
              "28  0.926976  154   17    9  184         90.51  \n",
              "29  0.941128  154   12    9  189         92.30  \n",
              "30  1.000000  475    0    0  614        100.00  \n",
              "31  0.944187  159   12    8  185         92.67  \n",
              "32  0.930540  153   13   12  186         90.88  \n",
              "33  0.727490  475  569    0   45         27.84  \n",
              "34  0.744986  171  178    0   15         31.87  \n",
              "35  0.735795  166  186    0   12         21.70  \n",
              "36  0.840453  424   51  130  484         78.98  \n",
              "37  0.825864  146   25   39  154         77.57  \n",
              "38  0.842643  154   12   48  150         79.24  \n",
              "39  0.851397  405   70   92  522         80.80  \n",
              "40  0.807911  131   40   29  164         75.74  \n",
              "41  0.828252  140   26   37  161         77.88  \n",
              "42  0.701505  292  183  130  484         64.04  \n",
              "43  0.667970   92   79   39  154         60.13  \n",
              "44  0.671443   98   68   49  149         60.42  \n",
              "45  0.545263   43  432    0  614         51.28  \n",
              "46  0.517877    7  164    1  192         46.18  \n",
              "47  0.548680   17  149    1  197         50.32  \n",
              "48  0.724622  348  127  174  440         65.79  \n",
              "49  0.713238  119   52   52  141         64.63  \n",
              "50  0.695357  116   50   61  137         62.55  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Verificar las dimensiones de X_train\n",
        "print(\"Dimensiones de X_train:\", X_train.shape)\n",
        "\n",
        "# Definir the RNN model structure\n",
        "model_RNN = Sequential()\n",
        "model_name = 'Red Neuronal Recurrente (RNN)'\n",
        "print(model_name)\n",
        "\n",
        "# Asegurar que X_train tenga al menos tres dimensiones para la capa recurrente\n",
        "if len(X_train.shape) == 2:\n",
        "    X_train = np.expand_dims(X_train, axis=-1)\n",
        "\n",
        "# Añadir la capa recurrente\n",
        "model_RNN.add(SimpleRNN(64, activation='relu', input_shape=X_train.shape[1:]))\n",
        "\n",
        "# Añade la capa de salida con activación sigmoidea para la clasificación binaria\n",
        "model_RNN.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar el modelo\n",
        "model_RNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model_RNN.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluar el modelo en los conjuntos de entrenamiento, validación y prueba.\n",
        "y_train_pred = model_RNN.predict(X_train)\n",
        "y_val_pred = model_RNN.predict(X_val)\n",
        "y_test_pred = model_RNN.predict(X_test)\n",
        "\n",
        "# Convertir las predicciones en etiquetas binarias (0 o 1)\n",
        "y_train_pred_labels = np.where(y_train_pred > 0.5, 1, 0)\n",
        "y_val_pred_labels = np.where(y_val_pred > 0.5, 1, 0)\n",
        "y_test_pred_labels = np.where(y_test_pred > 0.5, 1, 0)\n",
        "\n",
        "# Asegurar que las etiquetas predichas sean unidimensionales\n",
        "y_train_pred_labels = y_train_pred_labels.flatten()\n",
        "y_val_pred_labels = y_val_pred_labels.flatten()\n",
        "y_test_pred_labels = y_test_pred_labels.flatten()\n",
        "\n",
        "# Calcular las métricas de evaluación\n",
        "accuracy = accuracy_score(y_test, y_test_pred_labels)\n",
        "precision = precision_score(y_test, y_test_pred_labels)\n",
        "recall = recall_score(y_test, y_test_pred_labels)\n",
        "f1 = f1_score(y_test, y_test_pred_labels)\n",
        "\n",
        "# Mostrar estadísticas\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred_labels, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred_labels, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred_labels, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Borrar informacion tabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Borrar todos los registros\n",
        "tabla_results_df = tabla_results_df.iloc[0:0]\n",
        "tabla_results_NS_df = tabla_results_NS_df.iloc[0:0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aprendizaje no supervisado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering jerárquico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusión :\n",
            " [[171   0]\n",
            " [192   1]]\n",
            "Precision: 1.0\n",
            "Recall: 0.0051813471502590676\n",
            "F1-score: 0.010309278350515464\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Crear una instancia del modelo de clustering jerárquico\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "model_AC = AgglomerativeClustering(n_clusters=2)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_AC.fit(X_train_prep)\n",
        "\n",
        "# Predecir en el conjunto de validación\n",
        "y_pred = model_AC.fit_predict(X_val_prep)\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "# Calcular la precisión\n",
        "precision = precision_score(y_val, y_pred)\n",
        "\n",
        "# Calcular la exhaustividad\n",
        "recall = recall_score(y_val, y_pred)\n",
        "\n",
        "# Calcular la puntuación F1\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(\"Matriz de Confusión :\\n\", conf_matrix)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering K-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KMeans\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número óptimo de clusters según el método del codo: 2\n",
            "Metrics for Training set (KMeans):\n",
            " - Silhouette Score: 0.5892\n",
            " - Davies-Bouldin Index: 1.6353\n",
            " - Calinski-Harabasz Index: 215.7722\n",
            " - Global Score: 87.8223\n",
            "\n",
            "Metrics for Validation set (KMeans):\n",
            " - Silhouette Score: 0.6177\n",
            " - Davies-Bouldin Index: 1.7655\n",
            " - Calinski-Harabasz Index: 65.4537\n",
            " - Global Score: 36.0215\n",
            "\n",
            "Metrics for Test set (KMeans):\n",
            " - Silhouette Score: 0.6531\n",
            " - Davies-Bouldin Index: 1.4111\n",
            " - Calinski-Harabasz Index: 112.5643\n",
            " - Global Score: 58.2229\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\FloCr\\AppData\\Local\\Temp\\ipykernel_30552\\3849515159.py:412: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  tabla_results_NS_df = pd.concat([tabla_results_NS_df, new_row], ignore_index=True)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Model         Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0  KMeans    Training          0.589238              1.635342   \n",
              "1  KMeans  Validation          0.617695              1.765477   \n",
              "2  KMeans        Test          0.653144              1.411056   \n",
              "\n",
              "   Calinski-Harabasz Index  Global Score  \n",
              "0               215.772165     87.822329  \n",
              "1                65.453656     36.021524  \n",
              "2               112.564337     58.222912  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model_name = 'KMeans'\n",
        "print(model_name)\n",
        "model_KM = KMeans(random_state=42)\n",
        "\n",
        "# Obtención del número de clusters optimo, escoger entre elbow o silhouette\n",
        "optimal_k = optimal_cluster_number(X_train_prep, X_val_prep, model_KM, method='silhouette')\n",
        "print(\"Número óptimo de clusters según el método del codo:\", optimal_k)\n",
        "\n",
        "# Ajustar el modelo con el número óptimo de clusters\n",
        "model_KM.n_clusters = optimal_k\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_KM.fit(X_train_prep)\n",
        "\n",
        "labels_train = model_KM.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "\n",
        "labels_val = model_KM.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "\n",
        "labels_test = model_KM.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KMeans\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de clusteres optimo : 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Entrenamiento set (KMeans):\n",
            " - Silhouette Score: 0.4276\n",
            " - Davies-Bouldin Index: 1.0653\n",
            " - Calinski-Harabasz Index: 215.2480\n",
            " - Global Score: 93.3649\n",
            "\n",
            "Metrics for Evaluación set (KMeans):\n",
            " - Silhouette Score: 0.5128\n",
            " - Davies-Bouldin Index: 1.1492\n",
            " - Calinski-Harabasz Index: 72.7247\n",
            " - Global Score: 44.4833\n",
            "\n",
            "Metrics for Test set (KMeans):\n",
            " - Silhouette Score: 0.4927\n",
            " - Davies-Bouldin Index: 0.9756\n",
            " - Calinski-Harabasz Index: 140.3259\n",
            " - Global Score: 72.4658\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Model            Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0  KMeans       Training          0.589238              1.635342   \n",
              "1  KMeans     Validation          0.617695              1.765477   \n",
              "2  KMeans           Test          0.653144              1.411056   \n",
              "3  KMeans  Entrenamiento          0.427582              1.065326   \n",
              "4  KMeans     Evaluación          0.512824              1.149159   \n",
              "5  KMeans           Test          0.492667              0.975618   \n",
              "\n",
              "   Calinski-Harabasz Index  Global Score  \n",
              "0               215.772165     87.822329  \n",
              "1                65.453656     36.021524  \n",
              "2               112.564337     58.222912  \n",
              "3               215.248011     93.364851  \n",
              "4                72.724670     44.483306  \n",
              "5               140.325935     72.465829  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' Pruebas K-means \n",
        "La función encontrar_numero_optimo_clusters es algo más compleja y puede que ayude a ajustar mejor el modelo K-means\n",
        "Utiliza la predicción con los datos de validación para establecer las métricas para cada valor de cluster (0-10).\n",
        "Con ello, toma una decisión final donde el global_score sea máximo\n",
        "# establece qué número de clusteres es el más correcto, en un tiempo decente de en entorno a 1 min\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_name = 'KMeans'\n",
        "print(model_name)\n",
        "model_KM = KMeans(random_state=42)\n",
        "\n",
        "# Encontrar el número óptimo de clústeres\n",
        "num_clusters_optimo = encontrar_numero_optimo_clusters(X_train_prep, X_val_prep, model_KM, plot_grafica = 'NO')\n",
        "print(\"Número de clusteres optimo : \" + str(num_clusters_optimo))\n",
        "# Entrenar el modelo final con el número óptimo de clústeres usando los datos de entrenamiento\n",
        "kmeans_final = KMeans(n_clusters=num_clusters_optimo, random_state=0)\n",
        "kmeans_final.fit(X_train_prep)\n",
        "labels_train = model_KM.predict(X_train_prep)\n",
        "show_save_results_no_supervised(X_train_prep, labels_train, 'Entrenamiento',  model_name)\n",
        "\n",
        "# Evaluar el modelo en los datos de evaluación\n",
        "labels_val = kmeans_final.predict(X_val_prep)\n",
        "show_save_results_no_supervised(X_val_prep, labels_val,'Evaluación', model_name)\n",
        "\n",
        "labels_test = kmeans_final.predict(X_test_prep)\n",
        "show_save_results_no_supervised(X_test_prep, labels_test,'Test', model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Shift\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_mean_shift.py:293: UserWarning: Binning data failed with provided bin_size=0.100000, using data points as seeds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bandwidth óptimo: 2.0\n",
            "Mejor Silhouette Score en Validación: 0.49431989853400954\n",
            "Número de clústeres estimados en Validación: 24\n",
            "Metrics for Training set (Mean Shift):\n",
            " - Silhouette Score: 0.4582\n",
            " - Davies-Bouldin Index: 0.5735\n",
            " - Calinski-Harabasz Index: 114.9748\n",
            " - Global Score: 69.7375\n",
            "\n",
            "Metrics for Validation set (Mean Shift):\n",
            " - Silhouette Score: 0.4821\n",
            " - Davies-Bouldin Index: 0.6987\n",
            " - Calinski-Harabasz Index: 74.6950\n",
            " - Global Score: 54.6217\n",
            "\n",
            "Metrics for Test set (Mean Shift):\n",
            " - Silhouette Score: 0.4413\n",
            " - Davies-Bouldin Index: 0.8014\n",
            " - Calinski-Harabasz Index: 68.3151\n",
            " - Global Score: 50.1032\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Model            Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0      KMeans       Training          0.589238              1.635342   \n",
              "1      KMeans     Validation          0.617695              1.765477   \n",
              "2      KMeans           Test          0.653144              1.411056   \n",
              "3      KMeans  Entrenamiento          0.427582              1.065326   \n",
              "4      KMeans     Evaluación          0.512824              1.149159   \n",
              "5      KMeans           Test          0.492667              0.975618   \n",
              "6  Mean Shift       Training          0.458234              0.573478   \n",
              "7  Mean Shift     Validation          0.482059              0.698659   \n",
              "8  Mean Shift           Test          0.441301              0.801412   \n",
              "\n",
              "   Calinski-Harabasz Index  Global Score  \n",
              "0               215.772165     87.822329  \n",
              "1                65.453656     36.021524  \n",
              "2               112.564337     58.222912  \n",
              "3               215.248011     93.364851  \n",
              "4                72.724670     44.483306  \n",
              "5               140.325935     72.465829  \n",
              "6               114.974797     69.737530  \n",
              "7                74.694969     54.621658  \n",
              "8                68.315078     50.103171  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "model_name = 'Mean Shift'\n",
        "print(model_name)\n",
        "\n",
        "# Función para realizar la búsqueda de bandwidth óptimo\n",
        "def find_optimal_bandwidth(X, bandwidths):\n",
        "    best_bandwidth = None\n",
        "    best_score = -1\n",
        "    best_labels = None\n",
        "    best_cluster_centers = None\n",
        "    \n",
        "    for bandwidth in bandwidths:\n",
        "        ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "        ms.fit(X)\n",
        "        labels = ms.labels_\n",
        "        if len(np.unique(labels)) > 1:\n",
        "            score = silhouette_score(X, labels)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_bandwidth = bandwidth\n",
        "                best_labels = labels\n",
        "                best_cluster_centers = ms.cluster_centers_\n",
        "    \n",
        "    return best_bandwidth, best_labels, best_cluster_centers, best_score\n",
        "\n",
        "# Definir el rango de valores de bandwidth para la búsqueda\n",
        "bandwidths = np.linspace(0.1, 2.0, 20)\n",
        "\n",
        "# Buscar el bandwidth óptimo usando el conjunto de validación\n",
        "optimal_bandwidth, labels_val, cluster_centers, best_score = find_optimal_bandwidth(X_val_prep, bandwidths)\n",
        "\n",
        "print(\"Bandwidth óptimo:\", optimal_bandwidth)\n",
        "print(\"Mejor Silhouette Score en Validación:\", best_score)\n",
        "print(\"Número de clústeres estimados en Validación:\", len(np.unique(labels_val)))\n",
        "\n",
        "def map_clusters_to_labels(labels, true_labels):\n",
        "    from scipy.stats import mode\n",
        "    # Asegurarse de que labels y true_labels sean arreglos\n",
        "    labels = np.asarray(labels)\n",
        "    true_labels = np.asarray(true_labels)\n",
        "    \n",
        "    # Crear una matriz de confusión\n",
        "    conf_matrix = confusion_matrix(true_labels, labels)\n",
        "    \n",
        "    # Crear un arreglo para mapear los clusters a las etiquetas reales\n",
        "    cluster_label_map = np.zeros_like(labels)\n",
        "    \n",
        "    # Asignar la etiqueta de clase más frecuente a cada cluster\n",
        "    for i in range(conf_matrix.shape[1]):\n",
        "        mask = labels == i\n",
        "        if np.any(mask):  # Asegurarse de que el cluster tenga etiquetas\n",
        "            most_common_label = mode(true_labels[mask])[0][0]\n",
        "            cluster_label_map[mask] = most_common_label\n",
        "    \n",
        "    return cluster_label_map\n",
        "\n",
        "# Entrenar el modelo final en el conjunto de entrenamiento usando el bandwidth óptimo\n",
        "ms = MeanShift(bandwidth=optimal_bandwidth, bin_seeding=True)\n",
        "ms.fit(X_train_prep)\n",
        "labels_train = ms.labels_\n",
        "\n",
        "labels_train = ms.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "\n",
        "labels_val = ms.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "\n",
        "labels_test = ms.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DBSCAN\n",
            "Epsilon óptimo: 4.8\n",
            "Mejor Silhouette Score en Entrenamiento: 0.8349455307521673\n",
            "Número de clústeres estimados en Entrenamiento: 2\n",
            "Metrics for Training set (DBSCAN):\n",
            " - Silhouette Score: 0.8349\n",
            " - Davies-Bouldin Index: 1.7217\n",
            " - Calinski-Harabasz Index: 170.5236\n",
            " - Global Score: 75.3953\n",
            "\n",
            "Metrics for Validation set (DBSCAN):\n",
            " - Silhouette Score: 0.8204\n",
            " - Davies-Bouldin Index: 1.6787\n",
            " - Calinski-Harabasz Index: 52.0399\n",
            " - Global Score: 36.3747\n",
            "\n",
            "Metrics for Test set (DBSCAN):\n",
            " - Silhouette Score: 0.8233\n",
            " - Davies-Bouldin Index: 1.1945\n",
            " - Calinski-Harabasz Index: 88.5002\n",
            " - Global Score: 56.6458\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Model            Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0       KMeans       Training          0.589238              1.635342   \n",
              "1       KMeans     Validation          0.617695              1.765477   \n",
              "2       KMeans           Test          0.653144              1.411056   \n",
              "3       KMeans  Entrenamiento          0.427582              1.065326   \n",
              "4       KMeans     Evaluación          0.512824              1.149159   \n",
              "5       KMeans           Test          0.492667              0.975618   \n",
              "6   Mean Shift       Training          0.458234              0.573478   \n",
              "7   Mean Shift     Validation          0.482059              0.698659   \n",
              "8   Mean Shift           Test          0.441301              0.801412   \n",
              "9       DBSCAN       Training          0.834946              1.721699   \n",
              "10      DBSCAN     Validation          0.820430              1.678743   \n",
              "11      DBSCAN           Test          0.823251              1.194508   \n",
              "\n",
              "    Calinski-Harabasz Index  Global Score  \n",
              "0                215.772165     87.822329  \n",
              "1                 65.453656     36.021524  \n",
              "2                112.564337     58.222912  \n",
              "3                215.248011     93.364851  \n",
              "4                 72.724670     44.483306  \n",
              "5                140.325935     72.465829  \n",
              "6                114.974797     69.737530  \n",
              "7                 74.694969     54.621658  \n",
              "8                 68.315078     50.103171  \n",
              "9                170.523605     75.395314  \n",
              "10                52.039884     36.374742  \n",
              "11                88.500184     56.645778  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_name = 'DBSCAN'\n",
        "print(model_name)\n",
        "\n",
        "# Función para encontrar el mejor valor de eps\n",
        "def find_optimal_eps(X, eps_values, min_samples):\n",
        "    best_eps = None\n",
        "    best_score = -1\n",
        "    best_labels = None\n",
        "    \n",
        "    for eps in eps_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(X)\n",
        "        \n",
        "        # Solo evaluar si hay más de un cluster (ignorar ruido)\n",
        "        if len(np.unique(labels)) > 1 and np.sum(labels != -1) > 1:\n",
        "            score = silhouette_score(X, labels)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_eps = eps\n",
        "                best_labels = labels\n",
        "                \n",
        "    return best_eps, best_labels, best_score\n",
        "\n",
        "# Definir el rango de valores de eps para la búsqueda\n",
        "eps_values = np.linspace(0.1, 5.0, 50)\n",
        "min_samples = 5  # Puedes ajustar este valor según sea necesario\n",
        "\n",
        "# Buscar el eps óptimo usando el conjunto de entrenamiento\n",
        "optimal_eps, labels_train, best_score = find_optimal_eps(X_train_prep, eps_values, min_samples)\n",
        "\n",
        "print(\"Epsilon óptimo:\", optimal_eps)\n",
        "print(\"Mejor Silhouette Score en Entrenamiento:\", best_score)\n",
        "print(\"Número de clústeres estimados en Entrenamiento:\", len(np.unique(labels_train)))\n",
        "\n",
        "# Entrenar el modelo final en el conjunto de entrenamiento usando el eps óptimo\n",
        "dbscan_final = DBSCAN(eps=optimal_eps, min_samples=min_samples)\n",
        "\n",
        "labels_train = dbscan_final.fit_predict(X_train_prep)\n",
        "labels_val = dbscan_final.fit_predict(X_val_prep)\n",
        "labels_test = dbscan_final.fit_predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores parámetros:  {'gmm__covariance_type': 'spherical', 'gmm__n_components': 2}\n",
            "Metrics for Training set (Clustering GMM):\n",
            " - Silhouette Score: 0.4229\n",
            " - Davies-Bouldin Index: 1.7908\n",
            " - Calinski-Harabasz Index: 157.8364\n",
            " - Global Score: 63.1478\n",
            "\n",
            "Metrics for Validation set (Clustering GMM):\n",
            " - Silhouette Score: 0.4492\n",
            " - Davies-Bouldin Index: 1.8741\n",
            " - Calinski-Harabasz Index: 54.3106\n",
            " - Global Score: 27.6885\n",
            "\n",
            "Metrics for Test set (Clustering GMM):\n",
            " - Silhouette Score: 0.4781\n",
            " - Davies-Bouldin Index: 1.7677\n",
            " - Calinski-Harabasz Index: 66.4847\n",
            " - Global Score: 34.0007\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.422905</td>\n",
              "      <td>1.790767</td>\n",
              "      <td>157.836376</td>\n",
              "      <td>63.147762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.449208</td>\n",
              "      <td>1.874110</td>\n",
              "      <td>54.310631</td>\n",
              "      <td>27.688509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.478050</td>\n",
              "      <td>1.767702</td>\n",
              "      <td>66.484695</td>\n",
              "      <td>34.000696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Model            Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0           KMeans       Training          0.589238              1.635342   \n",
              "1           KMeans     Validation          0.617695              1.765477   \n",
              "2           KMeans           Test          0.653144              1.411056   \n",
              "3           KMeans  Entrenamiento          0.427582              1.065326   \n",
              "4           KMeans     Evaluación          0.512824              1.149159   \n",
              "5           KMeans           Test          0.492667              0.975618   \n",
              "6       Mean Shift       Training          0.458234              0.573478   \n",
              "7       Mean Shift     Validation          0.482059              0.698659   \n",
              "8       Mean Shift           Test          0.441301              0.801412   \n",
              "9           DBSCAN       Training          0.834946              1.721699   \n",
              "10          DBSCAN     Validation          0.820430              1.678743   \n",
              "11          DBSCAN           Test          0.823251              1.194508   \n",
              "12  Clustering GMM       Training          0.422905              1.790767   \n",
              "13  Clustering GMM     Validation          0.449208              1.874110   \n",
              "14  Clustering GMM           Test          0.478050              1.767702   \n",
              "\n",
              "    Calinski-Harabasz Index  Global Score  \n",
              "0                215.772165     87.822329  \n",
              "1                 65.453656     36.021524  \n",
              "2                112.564337     58.222912  \n",
              "3                215.248011     93.364851  \n",
              "4                 72.724670     44.483306  \n",
              "5                140.325935     72.465829  \n",
              "6                114.974797     69.737530  \n",
              "7                 74.694969     54.621658  \n",
              "8                 68.315078     50.103171  \n",
              "9                170.523605     75.395314  \n",
              "10                52.039884     36.374742  \n",
              "11                88.500184     56.645778  \n",
              "12               157.836376     63.147762  \n",
              "13                54.310631     27.688509  \n",
              "14                66.484695     34.000696  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "model_name = 'Clustering GMM'\n",
        "# Define un pipeline que combina un modelo de mezcla gaussiana\n",
        "pipeline = Pipeline([\n",
        "    ('gmm', GaussianMixture(random_state=42))\n",
        "])\n",
        "\n",
        "# Define la cuadrícula de parámetros para buscar\n",
        "param_grid = {\n",
        "    'gmm__n_components': [1, 2, 3, 4, 5],\n",
        "    'gmm__covariance_type': ['full', 'tied', 'diag', 'spherical']\n",
        "}\n",
        "\n",
        "# Crea un objeto GridSearchCV\n",
        "model_GMM = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro')\n",
        "\n",
        "# Ajusta el objeto GridSearchCV a los datos\n",
        "model_GMM.fit(X_train_prep, y_train)\n",
        "\n",
        "# Imprime los mejores parámetros\n",
        "print(\"Mejores parámetros: \", model_GMM.best_params_)\n",
        "\n",
        "\n",
        "labels_train = model_GMM.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "\n",
        "labels_val = model_GMM.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "\n",
        "labels_test = model_GMM.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pasar a formato excel \n",
        "tabla_results_df.to_excel('model_results.xlsx', index=False)\n",
        "tabla_results_NS_df.to_excel('model_results_NS.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis de componentes principales (PCA) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número óptimo de componentes para explicar al menos el 95% de la variancia es de 9 que coincide con el número actual de variables es de 9. \n",
            "Por tanto no se aplica el algoritmo PCA\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Crear y ajustar el modelo PCA para calcular la variancia explicada\n",
        "pca_temp = PCA()\n",
        "pca_temp.fit(X_train_prep)\n",
        "\n",
        "# Calcular la variancia explicada acumulada\n",
        "explained_variance = np.cumsum(pca_temp.explained_variance_ratio_)\n",
        "\n",
        "# Determinar el número de componentes que explican al menos el 95% de la variancia\n",
        "n_components = np.argmax(explained_variance >= 0.95) + 1\n",
        "\n",
        "if n_components == X_train_prep.shape[1] :\n",
        "    print(f\"Número óptimo de componentes para explicar al menos el 95% de la variancia es de {n_components}\" \n",
        "      f\" que coincide con el número actual de variables es de {X_train_prep.shape[1]}. \\n\"\n",
        "       \"Por tanto no se aplica el algoritmo PCA\")\n",
        "    \n",
        "else :\n",
        "    print(f\"Número óptimo de componentes para explicar al menos el 95% de la variancia: {n_components} \")\n",
        "\n",
        "    # Crear y ajustar el modelo PCA con el número óptimo de componentes\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train_prep)\n",
        "    X_val_pca = pca.transform(X_val_prep)\n",
        "    X_test_pca = pca.transform(X_test_prep)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis discriminante lineal (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LDA\n",
            "Best parameters : {'n_components': None, 'solver': 'svd'}\n",
            "Metrics for Training set (LDA):\n",
            " - Silhouette Score: 0.3091\n",
            " - Davies-Bouldin Index: 1.7594\n",
            " - Calinski-Harabasz Index: 133.1994\n",
            " - Global Score: 53.5623\n",
            "\n",
            "Metrics for Validation set (LDA):\n",
            " - Silhouette Score: 0.2775\n",
            " - Davies-Bouldin Index: 1.8459\n",
            " - Calinski-Harabasz Index: 41.1149\n",
            " - Global Score: 20.8976\n",
            "\n",
            "Metrics for Test set (LDA):\n",
            " - Silhouette Score: 0.3448\n",
            " - Davies-Bouldin Index: 1.7564\n",
            " - Calinski-Harabasz Index: 53.9366\n",
            " - Global Score: 27.7848\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.422905</td>\n",
              "      <td>1.790767</td>\n",
              "      <td>157.836376</td>\n",
              "      <td>63.147762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.449208</td>\n",
              "      <td>1.874110</td>\n",
              "      <td>54.310631</td>\n",
              "      <td>27.688509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.478050</td>\n",
              "      <td>1.767702</td>\n",
              "      <td>66.484695</td>\n",
              "      <td>34.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Model            Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0           KMeans       Training          0.589238              1.635342   \n",
              "1           KMeans     Validation          0.617695              1.765477   \n",
              "2           KMeans           Test          0.653144              1.411056   \n",
              "3           KMeans  Entrenamiento          0.427582              1.065326   \n",
              "4           KMeans     Evaluación          0.512824              1.149159   \n",
              "5           KMeans           Test          0.492667              0.975618   \n",
              "6       Mean Shift       Training          0.458234              0.573478   \n",
              "7       Mean Shift     Validation          0.482059              0.698659   \n",
              "8       Mean Shift           Test          0.441301              0.801412   \n",
              "9           DBSCAN       Training          0.834946              1.721699   \n",
              "10          DBSCAN     Validation          0.820430              1.678743   \n",
              "11          DBSCAN           Test          0.823251              1.194508   \n",
              "12  Clustering GMM       Training          0.422905              1.790767   \n",
              "13  Clustering GMM     Validation          0.449208              1.874110   \n",
              "14  Clustering GMM           Test          0.478050              1.767702   \n",
              "15             LDA       Training          0.309113              1.759360   \n",
              "16             LDA     Validation          0.277450              1.845891   \n",
              "17             LDA           Test          0.344773              1.756416   \n",
              "\n",
              "    Calinski-Harabasz Index  Global Score  \n",
              "0                215.772165     87.822329  \n",
              "1                 65.453656     36.021524  \n",
              "2                112.564337     58.222912  \n",
              "3                215.248011     93.364851  \n",
              "4                 72.724670     44.483306  \n",
              "5                140.325935     72.465829  \n",
              "6                114.974797     69.737530  \n",
              "7                 74.694969     54.621658  \n",
              "8                 68.315078     50.103171  \n",
              "9                170.523605     75.395314  \n",
              "10                52.039884     36.374742  \n",
              "11                88.500184     56.645778  \n",
              "12               157.836376     63.147762  \n",
              "13                54.310631     27.688509  \n",
              "14                66.484695     34.000696  \n",
              "15               133.199381     53.562338  \n",
              "16                41.114890     20.897624  \n",
              "17                53.936624     27.784813  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model_name = 'LDA'\n",
        "print(model_name)\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "n_features = X_train_prep.shape[1]\n",
        "n_classes = len(np.unique(y_train))\n",
        "max_components = min(n_features, n_classes - 1)\n",
        "\n",
        "# Definir los hiperparámetros a buscar\n",
        "param_grid = {\n",
        "    'solver': ['svd', 'lsqr', 'eigen'],\n",
        "    'n_components': [None] + list(range(1, max_components + 1))\n",
        "}\n",
        "\n",
        "# Definir GridSearchCV\n",
        "grid_search = GridSearchCV(lda, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento y encontrar los mejores hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener el mejor modelo y sus hiperparámetros\n",
        "model_LDA = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best parameters : {best_params}\")\n",
        "\n",
        "model_LDA.fit(X_train_prep, y_train)\n",
        "\n",
        "labels_train = model_LDA.predict(X_train_prep)\n",
        "labels_val = model_LDA.predict(X_val_prep)\n",
        "labels_test = model_LDA.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis de componentes independientes (ICA) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Análisis de Componentes Independientes (ICA)\n",
            "Metrics for Training set (Análisis de Componentes Independientes (ICA)):\n",
            " - Silhouette Score: 0.6631\n",
            " - Davies-Bouldin Index: 1.9897\n",
            " - Calinski-Harabasz Index: 176.4902\n",
            " - Global Score: 70.0536\n",
            "\n",
            "Metrics for Validation set (Análisis de Componentes Independientes (ICA)):\n",
            " - Silhouette Score: 0.6347\n",
            " - Davies-Bouldin Index: 2.0932\n",
            " - Calinski-Harabasz Index: 55.2447\n",
            " - Global Score: 27.4396\n",
            "\n",
            "Metrics for Test set (Análisis de Componentes Independientes (ICA)):\n",
            " - Silhouette Score: 0.6882\n",
            " - Davies-Bouldin Index: 1.6414\n",
            " - Calinski-Harabasz Index: 94.2292\n",
            " - Global Score: 48.8558\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.422905</td>\n",
              "      <td>1.790767</td>\n",
              "      <td>157.836376</td>\n",
              "      <td>63.147762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.449208</td>\n",
              "      <td>1.874110</td>\n",
              "      <td>54.310631</td>\n",
              "      <td>27.688509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.478050</td>\n",
              "      <td>1.767702</td>\n",
              "      <td>66.484695</td>\n",
              "      <td>34.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.663067</td>\n",
              "      <td>1.989653</td>\n",
              "      <td>176.490160</td>\n",
              "      <td>70.053615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.634664</td>\n",
              "      <td>2.093181</td>\n",
              "      <td>55.244726</td>\n",
              "      <td>27.439626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.688177</td>\n",
              "      <td>1.641412</td>\n",
              "      <td>94.229176</td>\n",
              "      <td>48.855808</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Model            Set  \\\n",
              "0                                         KMeans       Training   \n",
              "1                                         KMeans     Validation   \n",
              "2                                         KMeans           Test   \n",
              "3                                         KMeans  Entrenamiento   \n",
              "4                                         KMeans     Evaluación   \n",
              "5                                         KMeans           Test   \n",
              "6                                     Mean Shift       Training   \n",
              "7                                     Mean Shift     Validation   \n",
              "8                                     Mean Shift           Test   \n",
              "9                                         DBSCAN       Training   \n",
              "10                                        DBSCAN     Validation   \n",
              "11                                        DBSCAN           Test   \n",
              "12                                Clustering GMM       Training   \n",
              "13                                Clustering GMM     Validation   \n",
              "14                                Clustering GMM           Test   \n",
              "15                                           LDA       Training   \n",
              "16                                           LDA     Validation   \n",
              "17                                           LDA           Test   \n",
              "18  Análisis de Componentes Independientes (ICA)       Training   \n",
              "19  Análisis de Componentes Independientes (ICA)     Validation   \n",
              "20  Análisis de Componentes Independientes (ICA)           Test   \n",
              "\n",
              "    Silhouette Score  Davies-Bouldin Index  Calinski-Harabasz Index  \\\n",
              "0           0.589238              1.635342               215.772165   \n",
              "1           0.617695              1.765477                65.453656   \n",
              "2           0.653144              1.411056               112.564337   \n",
              "3           0.427582              1.065326               215.248011   \n",
              "4           0.512824              1.149159                72.724670   \n",
              "5           0.492667              0.975618               140.325935   \n",
              "6           0.458234              0.573478               114.974797   \n",
              "7           0.482059              0.698659                74.694969   \n",
              "8           0.441301              0.801412                68.315078   \n",
              "9           0.834946              1.721699               170.523605   \n",
              "10          0.820430              1.678743                52.039884   \n",
              "11          0.823251              1.194508                88.500184   \n",
              "12          0.422905              1.790767               157.836376   \n",
              "13          0.449208              1.874110                54.310631   \n",
              "14          0.478050              1.767702                66.484695   \n",
              "15          0.309113              1.759360               133.199381   \n",
              "16          0.277450              1.845891                41.114890   \n",
              "17          0.344773              1.756416                53.936624   \n",
              "18          0.663067              1.989653               176.490160   \n",
              "19          0.634664              2.093181                55.244726   \n",
              "20          0.688177              1.641412                94.229176   \n",
              "\n",
              "    Global Score  \n",
              "0      87.822329  \n",
              "1      36.021524  \n",
              "2      58.222912  \n",
              "3      93.364851  \n",
              "4      44.483306  \n",
              "5      72.465829  \n",
              "6      69.737530  \n",
              "7      54.621658  \n",
              "8      50.103171  \n",
              "9      75.395314  \n",
              "10     36.374742  \n",
              "11     56.645778  \n",
              "12     63.147762  \n",
              "13     27.688509  \n",
              "14     34.000696  \n",
              "15     53.562338  \n",
              "16     20.897624  \n",
              "17     27.784813  \n",
              "18     70.053615  \n",
              "19     27.439626  \n",
              "20     48.855808  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Crear y ajustar el modelo ICA\n",
        "model_name = 'Análisis de Componentes Independientes (ICA)'\n",
        "print(model_name)\n",
        "model_ICA = FastICA()\n",
        "model_ICA.fit(X_train_prep)\n",
        "\n",
        "# No se aplica el modelo ICA a los datos de entrenamiento, validación y prueba\n",
        "X_train_ica = model_ICA.transform(X_train_prep)\n",
        "X_val_ica = model_ICA.transform(X_val_prep)\n",
        "X_test_ica = model_ICA.transform(X_test_prep)\n",
        "\n",
        "# Crear y ajustar el modelo Isolation Forest\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "isolation_forest.fit(X_train_ica)\n",
        "\n",
        "# Predicción de anomalías en el conjunto de entrenamiento, validación y prueba\n",
        "anomalies_train = isolation_forest.predict(X_train_ica)\n",
        "anomalies_val = isolation_forest.predict(X_val_ica)\n",
        "anomalies_test = isolation_forest.predict(X_test_ica)\n",
        "\n",
        "# Convertir las predicciones a etiquetas binarias (0 para normal, 1 para anomalía)\n",
        "anomalies_train_labels = np.where(anomalies_train == -1, 1, 0)\n",
        "anomalies_val_labels = np.where(anomalies_val == -1, 1, 0)\n",
        "anomalies_test_labels = np.where(anomalies_test == -1, 1, 0)\n",
        "\n",
        "# Mostrar estadísticas de anomalías\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, anomalies_train_labels, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, anomalies_val_labels, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, anomalies_test_labels, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agrupamiento Jerárquico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agrupamiento Jerárquico\n",
            "Número óptimo de clústeres: 2\n",
            "Mejor Silhouette Score en Entrenamiento: 0.8362519169089486\n",
            "Metrics for Training set (Agrupamiento Jerárquico):\n",
            " - Silhouette Score: 0.8363\n",
            " - Davies-Bouldin Index: 1.3482\n",
            " - Calinski-Harabasz Index: 197.3254\n",
            " - Global Score: 90.5767\n",
            "\n",
            "Metrics for Validation set (Agrupamiento Jerárquico):\n",
            " - Silhouette Score: 0.8887\n",
            " - Davies-Bouldin Index: 0.0743\n",
            " - Calinski-Harabasz Index: 82.0443\n",
            " - Global Score: 74.2551\n",
            "\n",
            "Metrics for Test set (Agrupamiento Jerárquico):\n",
            " - Silhouette Score: 0.6206\n",
            " - Davies-Bouldin Index: 1.4987\n",
            " - Calinski-Harabasz Index: 100.8270\n",
            " - Global Score: 52.3069\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.422905</td>\n",
              "      <td>1.790767</td>\n",
              "      <td>157.836376</td>\n",
              "      <td>63.147762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.449208</td>\n",
              "      <td>1.874110</td>\n",
              "      <td>54.310631</td>\n",
              "      <td>27.688509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.478050</td>\n",
              "      <td>1.767702</td>\n",
              "      <td>66.484695</td>\n",
              "      <td>34.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.663067</td>\n",
              "      <td>1.989653</td>\n",
              "      <td>176.490160</td>\n",
              "      <td>70.053615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.634664</td>\n",
              "      <td>2.093181</td>\n",
              "      <td>55.244726</td>\n",
              "      <td>27.439626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.688177</td>\n",
              "      <td>1.641412</td>\n",
              "      <td>94.229176</td>\n",
              "      <td>48.855808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.836252</td>\n",
              "      <td>1.348159</td>\n",
              "      <td>197.325390</td>\n",
              "      <td>90.576674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.888725</td>\n",
              "      <td>0.074304</td>\n",
              "      <td>82.044252</td>\n",
              "      <td>74.255095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.620584</td>\n",
              "      <td>1.498707</td>\n",
              "      <td>100.826994</td>\n",
              "      <td>52.306950</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Model            Set  \\\n",
              "0                                         KMeans       Training   \n",
              "1                                         KMeans     Validation   \n",
              "2                                         KMeans           Test   \n",
              "3                                         KMeans  Entrenamiento   \n",
              "4                                         KMeans     Evaluación   \n",
              "5                                         KMeans           Test   \n",
              "6                                     Mean Shift       Training   \n",
              "7                                     Mean Shift     Validation   \n",
              "8                                     Mean Shift           Test   \n",
              "9                                         DBSCAN       Training   \n",
              "10                                        DBSCAN     Validation   \n",
              "11                                        DBSCAN           Test   \n",
              "12                                Clustering GMM       Training   \n",
              "13                                Clustering GMM     Validation   \n",
              "14                                Clustering GMM           Test   \n",
              "15                                           LDA       Training   \n",
              "16                                           LDA     Validation   \n",
              "17                                           LDA           Test   \n",
              "18  Análisis de Componentes Independientes (ICA)       Training   \n",
              "19  Análisis de Componentes Independientes (ICA)     Validation   \n",
              "20  Análisis de Componentes Independientes (ICA)           Test   \n",
              "21                       Agrupamiento Jerárquico       Training   \n",
              "22                       Agrupamiento Jerárquico     Validation   \n",
              "23                       Agrupamiento Jerárquico           Test   \n",
              "\n",
              "    Silhouette Score  Davies-Bouldin Index  Calinski-Harabasz Index  \\\n",
              "0           0.589238              1.635342               215.772165   \n",
              "1           0.617695              1.765477                65.453656   \n",
              "2           0.653144              1.411056               112.564337   \n",
              "3           0.427582              1.065326               215.248011   \n",
              "4           0.512824              1.149159                72.724670   \n",
              "5           0.492667              0.975618               140.325935   \n",
              "6           0.458234              0.573478               114.974797   \n",
              "7           0.482059              0.698659                74.694969   \n",
              "8           0.441301              0.801412                68.315078   \n",
              "9           0.834946              1.721699               170.523605   \n",
              "10          0.820430              1.678743                52.039884   \n",
              "11          0.823251              1.194508                88.500184   \n",
              "12          0.422905              1.790767               157.836376   \n",
              "13          0.449208              1.874110                54.310631   \n",
              "14          0.478050              1.767702                66.484695   \n",
              "15          0.309113              1.759360               133.199381   \n",
              "16          0.277450              1.845891                41.114890   \n",
              "17          0.344773              1.756416                53.936624   \n",
              "18          0.663067              1.989653               176.490160   \n",
              "19          0.634664              2.093181                55.244726   \n",
              "20          0.688177              1.641412                94.229176   \n",
              "21          0.836252              1.348159               197.325390   \n",
              "22          0.888725              0.074304                82.044252   \n",
              "23          0.620584              1.498707               100.826994   \n",
              "\n",
              "    Global Score  \n",
              "0      87.822329  \n",
              "1      36.021524  \n",
              "2      58.222912  \n",
              "3      93.364851  \n",
              "4      44.483306  \n",
              "5      72.465829  \n",
              "6      69.737530  \n",
              "7      54.621658  \n",
              "8      50.103171  \n",
              "9      75.395314  \n",
              "10     36.374742  \n",
              "11     56.645778  \n",
              "12     63.147762  \n",
              "13     27.688509  \n",
              "14     34.000696  \n",
              "15     53.562338  \n",
              "16     20.897624  \n",
              "17     27.784813  \n",
              "18     70.053615  \n",
              "19     27.439626  \n",
              "20     48.855808  \n",
              "21     90.576674  \n",
              "22     74.255095  \n",
              "23     52.306950  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_name = 'Agrupamiento Jerárquico'\n",
        "print(model_name)\n",
        "\n",
        "# Función para encontrar el mejor número de clústeres\n",
        "def find_optimal_clusters(X, max_clusters):\n",
        "    best_num_clusters = None\n",
        "    best_score = -1\n",
        "    best_labels = None\n",
        "\n",
        "    for num_clusters in range(2, max_clusters + 1):\n",
        "        agg_clustering = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "        labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "        score = silhouette_score(X, labels)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_num_clusters = num_clusters\n",
        "            best_labels = labels\n",
        "\n",
        "    return best_num_clusters, best_labels, best_score\n",
        "\n",
        "# Definir el número máximo de clústeres para la búsqueda\n",
        "max_clusters = 10\n",
        "\n",
        "# Buscar el número óptimo de clústeres usando el conjunto de entrenamiento\n",
        "optimal_num_clusters, labels_train, best_score = find_optimal_clusters(X_train_prep, max_clusters)\n",
        "\n",
        "print(\"Número óptimo de clústeres:\", optimal_num_clusters)\n",
        "print(\"Mejor Silhouette Score en Entrenamiento:\", best_score)\n",
        "\n",
        "# Entrenar el modelo final en el conjunto de entrenamiento usando el número óptimo de clústeres\n",
        "agg_clustering_final = AgglomerativeClustering(n_clusters=optimal_num_clusters)\n",
        "\n",
        "labels_train = agg_clustering_final.fit_predict(X_train_prep)\n",
        "labels_val = agg_clustering_final.fit_predict(X_val_prep)\n",
        "labels_test = agg_clustering_final.fit_predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algoritmo AnDE (Adaptive Nearest Neighbors Density Estimation) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Algoritmo AnDE\n",
            "Umbral óptimo basado en la densidad: 0.34953294503378424\n",
            "Silhouette Score en el conjunto de entrenamiento: 0.7439612751877717\n",
            "Metrics for Training set (Algoritmo AnDE):\n",
            " - Silhouette Score: 0.7440\n",
            " - Davies-Bouldin Index: 2.0779\n",
            " - Calinski-Harabasz Index: 157.0921\n",
            " - Global Score: 63.4648\n",
            "\n",
            "Metrics for Validation set (Algoritmo AnDE):\n",
            " - Silhouette Score: 0.6827\n",
            " - Davies-Bouldin Index: 2.0878\n",
            " - Calinski-Harabasz Index: 53.4775\n",
            " - Global Score: 27.7401\n",
            "\n",
            "Metrics for Test set (Algoritmo AnDE):\n",
            " - Silhouette Score: 0.7369\n",
            " - Davies-Bouldin Index: 1.6303\n",
            " - Calinski-Harabasz Index: 91.8491\n",
            " - Global Score: 49.0589\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.422905</td>\n",
              "      <td>1.790767</td>\n",
              "      <td>157.836376</td>\n",
              "      <td>63.147762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.449208</td>\n",
              "      <td>1.874110</td>\n",
              "      <td>54.310631</td>\n",
              "      <td>27.688509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.478050</td>\n",
              "      <td>1.767702</td>\n",
              "      <td>66.484695</td>\n",
              "      <td>34.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.663067</td>\n",
              "      <td>1.989653</td>\n",
              "      <td>176.490160</td>\n",
              "      <td>70.053615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.634664</td>\n",
              "      <td>2.093181</td>\n",
              "      <td>55.244726</td>\n",
              "      <td>27.439626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.688177</td>\n",
              "      <td>1.641412</td>\n",
              "      <td>94.229176</td>\n",
              "      <td>48.855808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.836252</td>\n",
              "      <td>1.348159</td>\n",
              "      <td>197.325390</td>\n",
              "      <td>90.576674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.888725</td>\n",
              "      <td>0.074304</td>\n",
              "      <td>82.044252</td>\n",
              "      <td>74.255095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.620584</td>\n",
              "      <td>1.498707</td>\n",
              "      <td>100.826994</td>\n",
              "      <td>52.306950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.743961</td>\n",
              "      <td>2.077912</td>\n",
              "      <td>157.092062</td>\n",
              "      <td>63.464839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.682671</td>\n",
              "      <td>2.087814</td>\n",
              "      <td>53.477549</td>\n",
              "      <td>27.740134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.736900</td>\n",
              "      <td>1.630349</td>\n",
              "      <td>91.849113</td>\n",
              "      <td>49.058886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Model            Set  \\\n",
              "0                                         KMeans       Training   \n",
              "1                                         KMeans     Validation   \n",
              "2                                         KMeans           Test   \n",
              "3                                         KMeans  Entrenamiento   \n",
              "4                                         KMeans     Evaluación   \n",
              "5                                         KMeans           Test   \n",
              "6                                     Mean Shift       Training   \n",
              "7                                     Mean Shift     Validation   \n",
              "8                                     Mean Shift           Test   \n",
              "9                                         DBSCAN       Training   \n",
              "10                                        DBSCAN     Validation   \n",
              "11                                        DBSCAN           Test   \n",
              "12                                Clustering GMM       Training   \n",
              "13                                Clustering GMM     Validation   \n",
              "14                                Clustering GMM           Test   \n",
              "15                                           LDA       Training   \n",
              "16                                           LDA     Validation   \n",
              "17                                           LDA           Test   \n",
              "18  Análisis de Componentes Independientes (ICA)       Training   \n",
              "19  Análisis de Componentes Independientes (ICA)     Validation   \n",
              "20  Análisis de Componentes Independientes (ICA)           Test   \n",
              "21                       Agrupamiento Jerárquico       Training   \n",
              "22                       Agrupamiento Jerárquico     Validation   \n",
              "23                       Agrupamiento Jerárquico           Test   \n",
              "24                                Algoritmo AnDE       Training   \n",
              "25                                Algoritmo AnDE     Validation   \n",
              "26                                Algoritmo AnDE           Test   \n",
              "\n",
              "    Silhouette Score  Davies-Bouldin Index  Calinski-Harabasz Index  \\\n",
              "0           0.589238              1.635342               215.772165   \n",
              "1           0.617695              1.765477                65.453656   \n",
              "2           0.653144              1.411056               112.564337   \n",
              "3           0.427582              1.065326               215.248011   \n",
              "4           0.512824              1.149159                72.724670   \n",
              "5           0.492667              0.975618               140.325935   \n",
              "6           0.458234              0.573478               114.974797   \n",
              "7           0.482059              0.698659                74.694969   \n",
              "8           0.441301              0.801412                68.315078   \n",
              "9           0.834946              1.721699               170.523605   \n",
              "10          0.820430              1.678743                52.039884   \n",
              "11          0.823251              1.194508                88.500184   \n",
              "12          0.422905              1.790767               157.836376   \n",
              "13          0.449208              1.874110                54.310631   \n",
              "14          0.478050              1.767702                66.484695   \n",
              "15          0.309113              1.759360               133.199381   \n",
              "16          0.277450              1.845891                41.114890   \n",
              "17          0.344773              1.756416                53.936624   \n",
              "18          0.663067              1.989653               176.490160   \n",
              "19          0.634664              2.093181                55.244726   \n",
              "20          0.688177              1.641412                94.229176   \n",
              "21          0.836252              1.348159               197.325390   \n",
              "22          0.888725              0.074304                82.044252   \n",
              "23          0.620584              1.498707               100.826994   \n",
              "24          0.743961              2.077912               157.092062   \n",
              "25          0.682671              2.087814                53.477549   \n",
              "26          0.736900              1.630349                91.849113   \n",
              "\n",
              "    Global Score  \n",
              "0      87.822329  \n",
              "1      36.021524  \n",
              "2      58.222912  \n",
              "3      93.364851  \n",
              "4      44.483306  \n",
              "5      72.465829  \n",
              "6      69.737530  \n",
              "7      54.621658  \n",
              "8      50.103171  \n",
              "9      75.395314  \n",
              "10     36.374742  \n",
              "11     56.645778  \n",
              "12     63.147762  \n",
              "13     27.688509  \n",
              "14     34.000696  \n",
              "15     53.562338  \n",
              "16     20.897624  \n",
              "17     27.784813  \n",
              "18     70.053615  \n",
              "19     27.439626  \n",
              "20     48.855808  \n",
              "21     90.576674  \n",
              "22     74.255095  \n",
              "23     52.306950  \n",
              "24     63.464839  \n",
              "25     27.740134  \n",
              "26     49.058886  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "class AnDE:\n",
        "    def __init__(self, k=5, threshold=0.5):\n",
        "        self.k = k\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.nbrs = NearestNeighbors(n_neighbors=self.k).fit(X)\n",
        "        self.densities = self.nbrs.kneighbors_graph(X).sum(axis=1)\n",
        "\n",
        "    def score_samples(self, X):\n",
        "        distances, _ = self.nbrs.kneighbors(X)\n",
        "        density_of_neighbors = self.densities.flatten()\n",
        "        density_of_neighbors[density_of_neighbors == 0] = np.finfo(float).eps\n",
        "        return 1 / (density_of_neighbors.mean() / distances.mean(axis=1))\n",
        "\n",
        "model_name = 'Algoritmo AnDE'\n",
        "print(model_name)\n",
        "\n",
        "# Entrenar el modelo AnDE\n",
        "ande_model = AnDE()\n",
        "ande_model.fit(X_train_prep)\n",
        "\n",
        "# Calcular la densidad para cada punto en el conjunto de entrenamiento\n",
        "densities_train = ande_model.score_samples(X_train_prep)\n",
        "\n",
        "# Calcular el umbral óptimo basado en la densidad\n",
        "threshold = np.percentile(densities_train, 95)\n",
        "\n",
        "# Predicción de anomalías en el conjunto de entrenamiento\n",
        "anomalies_train = densities_train > threshold\n",
        "\n",
        "# Calcular el Silhouette Score para evaluar la calidad del modelo\n",
        "silhouette_score_train = silhouette_score(X_train_prep, anomalies_train)\n",
        "\n",
        "print(\"Umbral óptimo basado en la densidad:\", threshold)\n",
        "print(\"Silhouette Score en el conjunto de entrenamiento:\", silhouette_score_train)\n",
        "\n",
        "# Predicción de anomalías en los conjuntos de validación y prueba\n",
        "anomalies_val = ande_model.score_samples(X_val_prep) > threshold\n",
        "anomalies_test = ande_model.score_samples(X_test_prep) > threshold\n",
        "\n",
        "# Mostrar estadísticas de anomalías\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, anomalies_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, anomalies_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, anomalies_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detección de Anomalías (Isolation Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detección de Anomalías (Isolation Forest)\n",
            "Métricas de evaluación en el conjunto de entrenamiento:\n",
            "Accuracy: 0.5362718089990818\n",
            "Precision: 1.0\n",
            "Recall: 0.1775244299674267\n",
            "F1 Score: 0.301521438450899\n",
            "\n",
            "Métricas de evaluación en el conjunto de validación:\n",
            "Accuracy: 0.5851648351648352\n",
            "Precision: 1.0\n",
            "Recall: 0.21761658031088082\n",
            "F1 Score: 0.3574468085106383\n",
            "\n",
            "Métricas de evaluación en el conjunto de prueba:\n",
            "Accuracy: 0.5576923076923077\n",
            "Precision: 1.0\n",
            "Recall: 0.18686868686868688\n",
            "F1 Score: 0.3148936170212766\n",
            "Metrics for Training set (Detección de Anomalías (Isolation Forest)):\n",
            " - Silhouette Score: 0.6577\n",
            " - Davies-Bouldin Index: 1.9636\n",
            " - Calinski-Harabasz Index: 177.3739\n",
            " - Global Score: 70.6920\n",
            "\n",
            "Metrics for Validation set (Detección de Anomalías (Isolation Forest)):\n",
            " - Silhouette Score: 0.6336\n",
            " - Davies-Bouldin Index: 2.0642\n",
            " - Calinski-Harabasz Index: 56.0600\n",
            " - Global Score: 28.1772\n",
            "\n",
            "Metrics for Test set (Detección de Anomalías (Isolation Forest)):\n",
            " - Silhouette Score: 0.6652\n",
            " - Davies-Bouldin Index: 1.6925\n",
            " - Calinski-Harabasz Index: 88.0443\n",
            " - Global Score: 45.5603\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.422905</td>\n",
              "      <td>1.790767</td>\n",
              "      <td>157.836376</td>\n",
              "      <td>63.147762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.449208</td>\n",
              "      <td>1.874110</td>\n",
              "      <td>54.310631</td>\n",
              "      <td>27.688509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.478050</td>\n",
              "      <td>1.767702</td>\n",
              "      <td>66.484695</td>\n",
              "      <td>34.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.663067</td>\n",
              "      <td>1.989653</td>\n",
              "      <td>176.490160</td>\n",
              "      <td>70.053615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.634664</td>\n",
              "      <td>2.093181</td>\n",
              "      <td>55.244726</td>\n",
              "      <td>27.439626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.688177</td>\n",
              "      <td>1.641412</td>\n",
              "      <td>94.229176</td>\n",
              "      <td>48.855808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.836252</td>\n",
              "      <td>1.348159</td>\n",
              "      <td>197.325390</td>\n",
              "      <td>90.576674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.888725</td>\n",
              "      <td>0.074304</td>\n",
              "      <td>82.044252</td>\n",
              "      <td>74.255095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.620584</td>\n",
              "      <td>1.498707</td>\n",
              "      <td>100.826994</td>\n",
              "      <td>52.306950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.743961</td>\n",
              "      <td>2.077912</td>\n",
              "      <td>157.092062</td>\n",
              "      <td>63.464839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.682671</td>\n",
              "      <td>2.087814</td>\n",
              "      <td>53.477549</td>\n",
              "      <td>27.740134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.736900</td>\n",
              "      <td>1.630349</td>\n",
              "      <td>91.849113</td>\n",
              "      <td>49.058886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Model            Set  \\\n",
              "0                                         KMeans       Training   \n",
              "1                                         KMeans     Validation   \n",
              "2                                         KMeans           Test   \n",
              "3                                         KMeans  Entrenamiento   \n",
              "4                                         KMeans     Evaluación   \n",
              "5                                         KMeans           Test   \n",
              "6                                     Mean Shift       Training   \n",
              "7                                     Mean Shift     Validation   \n",
              "8                                     Mean Shift           Test   \n",
              "9                                         DBSCAN       Training   \n",
              "10                                        DBSCAN     Validation   \n",
              "11                                        DBSCAN           Test   \n",
              "12                                Clustering GMM       Training   \n",
              "13                                Clustering GMM     Validation   \n",
              "14                                Clustering GMM           Test   \n",
              "15                                           LDA       Training   \n",
              "16                                           LDA     Validation   \n",
              "17                                           LDA           Test   \n",
              "18  Análisis de Componentes Independientes (ICA)       Training   \n",
              "19  Análisis de Componentes Independientes (ICA)     Validation   \n",
              "20  Análisis de Componentes Independientes (ICA)           Test   \n",
              "21                       Agrupamiento Jerárquico       Training   \n",
              "22                       Agrupamiento Jerárquico     Validation   \n",
              "23                       Agrupamiento Jerárquico           Test   \n",
              "24                                Algoritmo AnDE       Training   \n",
              "25                                Algoritmo AnDE     Validation   \n",
              "26                                Algoritmo AnDE           Test   \n",
              "27     Detección de Anomalías (Isolation Forest)       Training   \n",
              "28     Detección de Anomalías (Isolation Forest)     Validation   \n",
              "29     Detección de Anomalías (Isolation Forest)           Test   \n",
              "\n",
              "    Silhouette Score  Davies-Bouldin Index  Calinski-Harabasz Index  \\\n",
              "0           0.589238              1.635342               215.772165   \n",
              "1           0.617695              1.765477                65.453656   \n",
              "2           0.653144              1.411056               112.564337   \n",
              "3           0.427582              1.065326               215.248011   \n",
              "4           0.512824              1.149159                72.724670   \n",
              "5           0.492667              0.975618               140.325935   \n",
              "6           0.458234              0.573478               114.974797   \n",
              "7           0.482059              0.698659                74.694969   \n",
              "8           0.441301              0.801412                68.315078   \n",
              "9           0.834946              1.721699               170.523605   \n",
              "10          0.820430              1.678743                52.039884   \n",
              "11          0.823251              1.194508                88.500184   \n",
              "12          0.422905              1.790767               157.836376   \n",
              "13          0.449208              1.874110                54.310631   \n",
              "14          0.478050              1.767702                66.484695   \n",
              "15          0.309113              1.759360               133.199381   \n",
              "16          0.277450              1.845891                41.114890   \n",
              "17          0.344773              1.756416                53.936624   \n",
              "18          0.663067              1.989653               176.490160   \n",
              "19          0.634664              2.093181                55.244726   \n",
              "20          0.688177              1.641412                94.229176   \n",
              "21          0.836252              1.348159               197.325390   \n",
              "22          0.888725              0.074304                82.044252   \n",
              "23          0.620584              1.498707               100.826994   \n",
              "24          0.743961              2.077912               157.092062   \n",
              "25          0.682671              2.087814                53.477549   \n",
              "26          0.736900              1.630349                91.849113   \n",
              "27          0.657663              1.963624               177.373949   \n",
              "28          0.633634              2.064200                56.060025   \n",
              "29          0.665247              1.692517                88.044333   \n",
              "\n",
              "    Global Score  \n",
              "0      87.822329  \n",
              "1      36.021524  \n",
              "2      58.222912  \n",
              "3      93.364851  \n",
              "4      44.483306  \n",
              "5      72.465829  \n",
              "6      69.737530  \n",
              "7      54.621658  \n",
              "8      50.103171  \n",
              "9      75.395314  \n",
              "10     36.374742  \n",
              "11     56.645778  \n",
              "12     63.147762  \n",
              "13     27.688509  \n",
              "14     34.000696  \n",
              "15     53.562338  \n",
              "16     20.897624  \n",
              "17     27.784813  \n",
              "18     70.053615  \n",
              "19     27.439626  \n",
              "20     48.855808  \n",
              "21     90.576674  \n",
              "22     74.255095  \n",
              "23     52.306950  \n",
              "24     63.464839  \n",
              "25     27.740134  \n",
              "26     49.058886  \n",
              "27     70.691969  \n",
              "28     28.177234  \n",
              "29     45.560276  "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model_name = 'Detección de Anomalías (Isolation Forest)'\n",
        "print(model_name)\n",
        "\n",
        "# Entrenar el modelo Isolation Forest\n",
        "model_isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "model_isolation_forest.fit(X_train_prep)\n",
        "\n",
        "# Predicción de anomalías en el conjunto de entrenamiento, validación y prueba\n",
        "anomalies_train = model_isolation_forest.predict(X_train_prep)\n",
        "anomalies_val = model_isolation_forest.predict(X_val_prep)\n",
        "anomalies_test = model_isolation_forest.predict(X_test_prep)\n",
        "\n",
        "# Convertir las predicciones a etiquetas binarias (0 para normal, 1 para anomalía)\n",
        "anomalies_train_labels = np.where(anomalies_train == -1, 1, 0)\n",
        "anomalies_val_labels = np.where(anomalies_val == -1, 1, 0)\n",
        "anomalies_test_labels = np.where(anomalies_test == -1, 1, 0)\n",
        "\n",
        "# Calcular métricas de evaluación\n",
        "accuracy_train = accuracy_score(y_train, anomalies_train_labels)\n",
        "precision_train = precision_score(y_train, anomalies_train_labels)\n",
        "recall_train = recall_score(y_train, anomalies_train_labels)\n",
        "f1_train = f1_score(y_train, anomalies_train_labels)\n",
        "\n",
        "accuracy_val = accuracy_score(y_val, anomalies_val_labels)\n",
        "precision_val = precision_score(y_val, anomalies_val_labels)\n",
        "recall_val = recall_score(y_val, anomalies_val_labels)\n",
        "f1_val = f1_score(y_val, anomalies_val_labels)\n",
        "\n",
        "accuracy_test = accuracy_score(y_test, anomalies_test_labels)\n",
        "precision_test = precision_score(y_test, anomalies_test_labels)\n",
        "recall_test = recall_score(y_test, anomalies_test_labels)\n",
        "f1_test = f1_score(y_test, anomalies_test_labels)\n",
        "\n",
        "# Mostrar métricas de evaluación\n",
        "print(\"Métricas de evaluación en el conjunto de entrenamiento:\")\n",
        "print(\"Accuracy:\", accuracy_train)\n",
        "print(\"Precision:\", precision_train)\n",
        "print(\"Recall:\", recall_train)\n",
        "print(\"F1 Score:\", f1_train)\n",
        "\n",
        "print(\"\\nMétricas de evaluación en el conjunto de validación:\")\n",
        "print(\"Accuracy:\", accuracy_val)\n",
        "print(\"Precision:\", precision_val)\n",
        "print(\"Recall:\", recall_val)\n",
        "print(\"F1 Score:\", f1_val)\n",
        "\n",
        "print(\"\\nMétricas de evaluación en el conjunto de prueba:\")\n",
        "print(\"Accuracy:\", accuracy_test)\n",
        "print(\"Precision:\", precision_test)\n",
        "print(\"Recall:\", recall_test)\n",
        "print(\"F1 Score:\", f1_test)\n",
        "\n",
        "# Mostrar estadísticas de anomalías\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, anomalies_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, anomalies_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, anomalies_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reducción de dimensionalidad SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Varianza explicada acumulada: 1.0000000000000016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Training set (Reducción de Dimensionalidad SVD):\n",
            " - Silhouette Score: 0.7968\n",
            " - Davies-Bouldin Index: 1.7805\n",
            " - Calinski-Harabasz Index: 171.6366\n",
            " - Global Score: 74.1514\n",
            "\n",
            "Metrics for Validation set (Reducción de Dimensionalidad SVD):\n",
            " - Silhouette Score: 0.8887\n",
            " - Davies-Bouldin Index: 0.0743\n",
            " - Calinski-Harabasz Index: 82.0443\n",
            " - Global Score: 74.2551\n",
            "\n",
            "Metrics for Test set (Reducción de Dimensionalidad SVD):\n",
            " - Silhouette Score: 0.5589\n",
            " - Davies-Bouldin Index: 1.6042\n",
            " - Calinski-Harabasz Index: 86.3932\n",
            " - Global Score: 44.7097\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.422905</td>\n",
              "      <td>1.790767</td>\n",
              "      <td>157.836376</td>\n",
              "      <td>63.147762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.449208</td>\n",
              "      <td>1.874110</td>\n",
              "      <td>54.310631</td>\n",
              "      <td>27.688509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Clustering GMM</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.478050</td>\n",
              "      <td>1.767702</td>\n",
              "      <td>66.484695</td>\n",
              "      <td>34.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.663067</td>\n",
              "      <td>1.989653</td>\n",
              "      <td>176.490160</td>\n",
              "      <td>70.053615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.634664</td>\n",
              "      <td>2.093181</td>\n",
              "      <td>55.244726</td>\n",
              "      <td>27.439626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.688177</td>\n",
              "      <td>1.641412</td>\n",
              "      <td>94.229176</td>\n",
              "      <td>48.855808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.836252</td>\n",
              "      <td>1.348159</td>\n",
              "      <td>197.325390</td>\n",
              "      <td>90.576674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.888725</td>\n",
              "      <td>0.074304</td>\n",
              "      <td>82.044252</td>\n",
              "      <td>74.255095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Agrupamiento Jerárquico</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.620584</td>\n",
              "      <td>1.498707</td>\n",
              "      <td>100.826994</td>\n",
              "      <td>52.306950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.743961</td>\n",
              "      <td>2.077912</td>\n",
              "      <td>157.092062</td>\n",
              "      <td>63.464839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.682671</td>\n",
              "      <td>2.087814</td>\n",
              "      <td>53.477549</td>\n",
              "      <td>27.740134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.736900</td>\n",
              "      <td>1.630349</td>\n",
              "      <td>91.849113</td>\n",
              "      <td>49.058886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Reducción de Dimensionalidad SVD</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.796820</td>\n",
              "      <td>1.780466</td>\n",
              "      <td>171.636603</td>\n",
              "      <td>74.151448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Reducción de Dimensionalidad SVD</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.888725</td>\n",
              "      <td>0.074304</td>\n",
              "      <td>82.044252</td>\n",
              "      <td>74.255095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Reducción de Dimensionalidad SVD</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.558935</td>\n",
              "      <td>1.604217</td>\n",
              "      <td>86.393169</td>\n",
              "      <td>44.709680</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Model            Set  \\\n",
              "0                                         KMeans       Training   \n",
              "1                                         KMeans     Validation   \n",
              "2                                         KMeans           Test   \n",
              "3                                         KMeans  Entrenamiento   \n",
              "4                                         KMeans     Evaluación   \n",
              "5                                         KMeans           Test   \n",
              "6                                     Mean Shift       Training   \n",
              "7                                     Mean Shift     Validation   \n",
              "8                                     Mean Shift           Test   \n",
              "9                                         DBSCAN       Training   \n",
              "10                                        DBSCAN     Validation   \n",
              "11                                        DBSCAN           Test   \n",
              "12                                Clustering GMM       Training   \n",
              "13                                Clustering GMM     Validation   \n",
              "14                                Clustering GMM           Test   \n",
              "15                                           LDA       Training   \n",
              "16                                           LDA     Validation   \n",
              "17                                           LDA           Test   \n",
              "18  Análisis de Componentes Independientes (ICA)       Training   \n",
              "19  Análisis de Componentes Independientes (ICA)     Validation   \n",
              "20  Análisis de Componentes Independientes (ICA)           Test   \n",
              "21                       Agrupamiento Jerárquico       Training   \n",
              "22                       Agrupamiento Jerárquico     Validation   \n",
              "23                       Agrupamiento Jerárquico           Test   \n",
              "24                                Algoritmo AnDE       Training   \n",
              "25                                Algoritmo AnDE     Validation   \n",
              "26                                Algoritmo AnDE           Test   \n",
              "27     Detección de Anomalías (Isolation Forest)       Training   \n",
              "28     Detección de Anomalías (Isolation Forest)     Validation   \n",
              "29     Detección de Anomalías (Isolation Forest)           Test   \n",
              "30              Reducción de Dimensionalidad SVD       Training   \n",
              "31              Reducción de Dimensionalidad SVD     Validation   \n",
              "32              Reducción de Dimensionalidad SVD           Test   \n",
              "\n",
              "    Silhouette Score  Davies-Bouldin Index  Calinski-Harabasz Index  \\\n",
              "0           0.589238              1.635342               215.772165   \n",
              "1           0.617695              1.765477                65.453656   \n",
              "2           0.653144              1.411056               112.564337   \n",
              "3           0.427582              1.065326               215.248011   \n",
              "4           0.512824              1.149159                72.724670   \n",
              "5           0.492667              0.975618               140.325935   \n",
              "6           0.458234              0.573478               114.974797   \n",
              "7           0.482059              0.698659                74.694969   \n",
              "8           0.441301              0.801412                68.315078   \n",
              "9           0.834946              1.721699               170.523605   \n",
              "10          0.820430              1.678743                52.039884   \n",
              "11          0.823251              1.194508                88.500184   \n",
              "12          0.422905              1.790767               157.836376   \n",
              "13          0.449208              1.874110                54.310631   \n",
              "14          0.478050              1.767702                66.484695   \n",
              "15          0.309113              1.759360               133.199381   \n",
              "16          0.277450              1.845891                41.114890   \n",
              "17          0.344773              1.756416                53.936624   \n",
              "18          0.663067              1.989653               176.490160   \n",
              "19          0.634664              2.093181                55.244726   \n",
              "20          0.688177              1.641412                94.229176   \n",
              "21          0.836252              1.348159               197.325390   \n",
              "22          0.888725              0.074304                82.044252   \n",
              "23          0.620584              1.498707               100.826994   \n",
              "24          0.743961              2.077912               157.092062   \n",
              "25          0.682671              2.087814                53.477549   \n",
              "26          0.736900              1.630349                91.849113   \n",
              "27          0.657663              1.963624               177.373949   \n",
              "28          0.633634              2.064200                56.060025   \n",
              "29          0.665247              1.692517                88.044333   \n",
              "30          0.796820              1.780466               171.636603   \n",
              "31          0.888725              0.074304                82.044252   \n",
              "32          0.558935              1.604217                86.393169   \n",
              "\n",
              "    Global Score  \n",
              "0      87.822329  \n",
              "1      36.021524  \n",
              "2      58.222912  \n",
              "3      93.364851  \n",
              "4      44.483306  \n",
              "5      72.465829  \n",
              "6      69.737530  \n",
              "7      54.621658  \n",
              "8      50.103171  \n",
              "9      75.395314  \n",
              "10     36.374742  \n",
              "11     56.645778  \n",
              "12     63.147762  \n",
              "13     27.688509  \n",
              "14     34.000696  \n",
              "15     53.562338  \n",
              "16     20.897624  \n",
              "17     27.784813  \n",
              "18     70.053615  \n",
              "19     27.439626  \n",
              "20     48.855808  \n",
              "21     90.576674  \n",
              "22     74.255095  \n",
              "23     52.306950  \n",
              "24     63.464839  \n",
              "25     27.740134  \n",
              "26     49.058886  \n",
              "27     70.691969  \n",
              "28     28.177234  \n",
              "29     45.560276  \n",
              "30     74.151448  \n",
              "31     74.255095  \n",
              "32     44.709680  "
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "model_name = 'Reducción de Dimensionalidad SVD'\n",
        "\n",
        "# Definir el número máximo de componentes (ajustar en función de n_características)\n",
        "n_features = X_train_prep.shape[1]\n",
        "max_components = min(15, n_features)\n",
        "\n",
        "# Crear y ajustar el modelo SVD\n",
        "model_SVD = TruncatedSVD(n_components=max_components, random_state=42)\n",
        "model_SVD.fit(X_train_prep)\n",
        "\n",
        "# Calcular la varianza explicada acumulada\n",
        "explained_variance_ratio = model_SVD.explained_variance_ratio_\n",
        "cumulative_explained_variance = np.sum(explained_variance_ratio)\n",
        "print(\"Varianza explicada acumulada:\", cumulative_explained_variance)\n",
        "\n",
        "# Aplicar el modelo SVD a los datos de entrenamiento, validación y prueba\n",
        "X_train_reduced = model_SVD.transform(X_train_prep)\n",
        "X_val_reduced = model_SVD.transform(X_val_prep)\n",
        "X_test_reduced = model_SVD.transform(X_test_prep)\n",
        "\n",
        "# Usar MinMaxScaler para normalizar los datos antes de aplicar clustering\n",
        "scaler = MinMaxScaler()\n",
        "X_train_reduced_scaled = scaler.fit_transform(X_train_reduced)\n",
        "X_val_reduced_scaled = scaler.transform(X_val_reduced)\n",
        "X_test_reduced_scaled = scaler.transform(X_test_reduced)\n",
        "\n",
        "# Obtención del número de clusters optimo, escoger entre elbow o silhouette\n",
        "model_KM = KMeans(random_state=42)\n",
        "optimal_k = optimal_cluster_number(X_train_prep, X_val_prep, model_KM, method='silhouette')\n",
        "\n",
        "# Usar AgglomerativeClustering en lugar de KMeans\n",
        "model_AC = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
        "labels_train = model_AC.fit_predict(X_train_reduced_scaled)\n",
        "labels_val = model_AC.fit_predict(X_val_reduced_scaled)\n",
        "labels_test = model_AC.fit_predict(X_test_reduced_scaled)\n",
        "\n",
        "# Normalizar las etiquetas utilizando MinMaxScaler\n",
        "labels_train_scaled = scaler.fit_transform(labels_train.reshape(-1, 1))\n",
        "labels_val_scaled = scaler.transform(labels_val.reshape(-1, 1))\n",
        "labels_test_scaled = scaler.transform(labels_test.reshape(-1, 1))\n",
        "\n",
        "silhouette_norm = labels_train_scaled\n",
        "db_norm = 1 - labels_train_scaled\n",
        "ch_norm = labels_train_scaled\n",
        "global_score = (silhouette_norm + db_norm + ch_norm) / 3\n",
        "\n",
        "# Mostrar estadísticas de anomalías\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_reduced, labels_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_reduced, labels_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_reduced, labels_test, \"Test\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pruebas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5.2 Peso de cada variable en la predicción de la variable objetivo"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pesos del estudio original \n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verificación del \"Information Gain\" o peso de la variable frente a la variable objetivo a predecir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.5.2.1 Método 1 Variables continuas, variable objetivo binaria --> Correlación de Pearson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Teniendo variables predictivas numéricas continuas y una variable objetivo binaria, se hace uso del coeficiente de correlación de Pearson\n",
            "\tCorrelación de Pearson : \n",
            "Tumor type                              1.000000\n",
            "CancerSEEK Logistic Regression Score    0.732341\n",
            "OPN (pg/ml)                             0.458352\n",
            "Prolactin (pg/ml)                       0.324378\n",
            "TIMP-1 (pg/ml)                          0.300539\n",
            "GDF15 (ng/ml)                           0.247428\n",
            "HGF (pg/ml)                             0.242512\n",
            "Myeloperoxidase (ng/ml)                 0.221877\n",
            "FGF2 (pg/ml)                            0.192212\n",
            "IL-6 (pg/ml)                            0.186625\n",
            "Galectin-3 (ng/ml)                      0.180836\n",
            "Angiopoietin-2 (pg/ml)                  0.170233\n",
            "OPG (ng/ml)                             0.147721\n",
            "Omega score                             0.146759\n",
            "HE4 (pg/ml)                             0.139689\n",
            "Follistatin (pg/ml)                     0.137546\n",
            "CEA (pg/ml)                             0.126718\n",
            "G-CSF (pg/ml)                           0.125556\n",
            "Thrombospondin-2 (pg/ml)                0.113892\n",
            "SHBG (nM)                               0.103730\n",
            "Mesothelin (ng/ml)                      0.099196\n",
            "CA-125 (U/ml)                           0.095708\n",
            "CA 15-3 (U/ml)                          0.093855\n",
            "AFP (pg/ml)                             0.093452\n",
            "Midkine (pg/ml)                         0.090919\n",
            "IL-8 (pg/ml)                            0.080777\n",
            "CA19-9 (U/ml)                           0.078404\n",
            "CYFRA 21-1 (pg/ml)                      0.058790\n",
            "PAR (pg/ml)                             0.052930\n",
            "Kallikrein-6 (pg/ml)                    0.042339\n",
            "AXL (pg/ml)                             0.032260\n",
            "TGFa (pg/ml)                            0.028200\n",
            "TIMP-2 (pg/ml)                          0.019388\n",
            "sHER2/sEGFR2/sErbB2 (pg/ml)             0.015588\n",
            "DKK1 (ng/ml)                            0.015159\n",
            "CD44 (ng/ml)                            0.003697\n",
            "Endoglin (pg/ml)                       -0.011759\n",
            "sFas (pg/ml)                           -0.022677\n",
            "Leptin (pg/ml)                         -0.041766\n",
            "sPECAM-1 (pg/ml)                       -0.058772\n",
            "NSE (ng/ml)                            -0.099372\n",
            "sEGFR (pg/ml)                          -0.279246\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"[INFO] Teniendo variables predictivas numéricas continuas y una variable objetivo binaria, se hace uso del coeficiente de correlación de Pearson\")\n",
        "df_numericas = df.select_dtypes(include=['number'])\n",
        "correlaciones = df_numericas.corrwith(df['Tumor type']).sort_values(ascending=False)\n",
        "\n",
        "print(\"\\tCorrelación de Pearson : \")\n",
        "print(correlaciones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.5.2.2 Método 2 - Variables discretas (KBinsDiscretizer()), variable objetivo binaria --> Correlación "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CancerSEEK Logistic Regression Score    0.707618\n",
            "IL-8 (pg/ml)                            0.577839\n",
            "OPN (pg/ml)                             0.576522\n",
            "IL-6 (pg/ml)                            0.475775\n",
            "GDF15 (ng/ml)                           0.474454\n",
            "Prolactin (pg/ml)                       0.447670\n",
            "HGF (pg/ml)                             0.444292\n",
            "Omega score                             0.379398\n",
            "Myeloperoxidase (ng/ml)                 0.349025\n",
            "TGFa (pg/ml)                            0.326160\n",
            "sEGFR (pg/ml)                           0.320633\n",
            "TIMP-1 (pg/ml)                          0.301831\n",
            "CEA (pg/ml)                             0.301361\n",
            "CA-125 (U/ml)                           0.288792\n",
            "CA19-9 (U/ml)                           0.275198\n",
            "dtype: float64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
            "  warnings.warn(\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        }
      ],
      "source": [
        "# Prueba discretización rápida + correlaciones \n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "df_numericas = df.select_dtypes(include=['number'])\n",
        "# Inicializar el discretizador basado en cuantiles\n",
        "discretizer = KBinsDiscretizer(n_bins=12, encode='ordinal', strategy='quantile')\n",
        "\n",
        "# Discretizar las variables numéricas continuas\n",
        "df_numericas_discretas = pd.DataFrame(discretizer.fit_transform(df_numericas), columns=df_numericas.columns)\n",
        "\n",
        "# Calcular el coeficiente de correlación entre las variables numéricas discretas y la variable objetivo binaria\n",
        "correlaciones_discretas = df_numericas_discretas.corrwith(df['Tumor type'])\n",
        "\n",
        "# Ordenar las correlaciones de mayor a menor\n",
        "correlaciones_discretas_ordenadas = correlaciones_discretas.abs().sort_values(ascending=False)\n",
        "\n",
        "# Obtener las top 10 variables numéricas discretas con las correlaciones más altas\n",
        "top_15_correlaciones_discretas = correlaciones_discretas_ordenadas.nlargest(15)\n",
        "\n",
        "# Imprimir las top 10 correlaciones\n",
        "print(top_15_correlaciones_discretas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.5.2.3 Método 3 - Variables discretizado (Arbol de decisión (max_depth = 15)), variable objetivo binaria --> Correlación "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tumor type                              1.000000\n",
            "CancerSEEK Logistic Regression Score    0.713090\n",
            "OPN (pg/ml)                             0.575480\n",
            "IL-6 (pg/ml)                            0.483620\n",
            "IL-8 (pg/ml)                            0.464828\n",
            "HGF (pg/ml)                             0.454991\n",
            "Prolactin (pg/ml)                       0.453270\n",
            "Omega score                             0.378112\n",
            "GDF15 (ng/ml)                           0.365248\n",
            "CYFRA 21-1 (pg/ml)                      0.356245\n",
            "Myeloperoxidase (ng/ml)                 0.351481\n",
            "sEGFR (pg/ml)                           0.319982\n",
            "CA-125 (U/ml)                           0.312094\n",
            "CEA (pg/ml)                             0.308045\n",
            "TIMP-1 (pg/ml)                          0.301340\n",
            "CA19-9 (U/ml)                           0.266444\n",
            "Angiopoietin-2 (pg/ml)                  0.233881\n",
            "HE4 (pg/ml)                             0.232766\n",
            "Galectin-3 (ng/ml)                      0.232425\n",
            "Midkine (pg/ml)                         0.222265\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "def discretizar_df_arboles(df, max_depth=15, rango_discretizacion=(-np.inf, np.inf)):\n",
        "    df_discretizado = pd.DataFrame()\n",
        "    \n",
        "    # Iterar sobre todas las columnas del dataframe original\n",
        "    for columna in df.columns:\n",
        "        if df[columna].dtype.kind in 'biufc': # (CHULO) Comprueba si el tipo de datos de la columna es numérico ('b' para booleano, 'i' para entero, 'u' para sin signo, 'f' para flotante o 'c' para complejo)\n",
        "            # Si la columna es numérica, realizar la discretización\n",
        "            dt = DecisionTreeRegressor(max_depth=max_depth)\n",
        "            dt.fit(df[columna].values.reshape(-1, 1), df[columna])\n",
        "            puntos_corte = dt.tree_.threshold[dt.tree_.threshold != -2] # Extrae los puntos de corte del árbol de decisión para la columna numérica específica, ignorando aquellos puntos de corte asociados con nodos hoja (-2)\n",
        "            puntos_corte = np.sort(puntos_corte)\n",
        "            puntos_corte = np.concatenate(([rango_discretizacion[0]], puntos_corte, [rango_discretizacion[1]]))\n",
        "            # print(f\"\\t Columna : {columna} \\n Puntos de Corte : \\n {puntos_corte}\")\n",
        "            df_discretizado[f'{columna}'] = pd.cut(df[columna], bins=puntos_corte, labels=range(len(puntos_corte)-1))\n",
        "        else:\n",
        "            # Si la columna no es numérica, simplemente copiarla al dataframe resultante\n",
        "            df_discretizado[columna] = df[columna]\n",
        "            \n",
        "    return df_discretizado\n",
        "\n",
        "df_discret = discretizar_df_arboles(df.select_dtypes(include=['number']))\n",
        "\n",
        "# Calcular el coeficiente de correlación entre las variables numéricas discretas y la variable objetivo binaria\n",
        "correlaciones_discretas = df_discret.corrwith(df_discret['Tumor type'])\n",
        "\n",
        "# Ordenar las correlaciones de mayor a menor\n",
        "correlaciones_discretas_ordenadas = correlaciones_discretas.abs().sort_values(ascending=False)\n",
        "\n",
        "# Obtener las top 20 variables numéricas discretas con las correlaciones más altas\n",
        "top_20_correlaciones_discretas = correlaciones_discretas_ordenadas.nlargest(20)\n",
        "\n",
        "# Imprimir las top 20 correlaciones\n",
        "print(top_20_correlaciones_discretas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.5.2.4 Método 4 [PAPER] - Variables discretizado (Random forest (n_estimators=250, criterion='gini') + cross validation), variable objetivo binaria --> Correlación "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CancerSEEK Logistic Regression Score: 0.18547443639083727\n",
            "IL-8 (pg/ml): 0.1124115495995762\n",
            "IL-6 (pg/ml): 0.07563466315302167\n",
            "OPN (pg/ml): 0.06728892311955806\n",
            "HGF (pg/ml): 0.04674800784002754\n",
            "GDF15 (ng/ml): 0.04323351703044246\n",
            "Prolactin (pg/ml): 0.040686400587885516\n",
            "CYFRA 21-1 (pg/ml): 0.040582147942437294\n",
            "TGFa (pg/ml): 0.03904465156909732\n",
            "NSE (ng/ml): 0.022904407060325458\n",
            "CA-125 (U/ml): 0.02208948346420362\n",
            "CA19-9 (U/ml): 0.02179087903175963\n",
            "HE4 (pg/ml): 0.020789379612865096\n",
            "Omega score: 0.019195387606985664\n",
            "sFas (pg/ml): 0.01740764352720825\n",
            "sEGFR (pg/ml): 0.01681997139711323\n",
            "Thrombospondin-2 (pg/ml): 0.01621309379801383\n",
            "DKK1 (ng/ml): 0.013233950063477365\n",
            "G-CSF (pg/ml): 0.012234318141973462\n",
            "Myeloperoxidase (ng/ml): 0.011138298599647023\n",
            "CD44 (ng/ml): 0.010941727491012323\n",
            "OPG (ng/ml): 0.010401283222553124\n",
            "TIMP-1 (pg/ml): 0.009762696536302472\n",
            "CEA (pg/ml): 0.009553752399121167\n",
            "AFP (pg/ml): 0.009313187318336377\n",
            "sHER2/sEGFR2/sErbB2 (pg/ml): 0.008948935711945829\n",
            "Midkine (pg/ml): 0.008185485461156165\n",
            "TIMP-2 (pg/ml): 0.008043887969100999\n",
            "Galectin-3 (ng/ml): 0.007992528602118056\n",
            "FGF2 (pg/ml): 0.007878196268811826\n",
            "Follistatin (pg/ml): 0.0067846812156106065\n",
            "Kallikrein-6 (pg/ml): 0.006520641021987033\n",
            "Mesothelin (ng/ml): 0.006200942451939808\n",
            "sPECAM-1 (pg/ml): 0.006029025389922847\n",
            "SHBG (nM): 0.006005671808243508\n",
            "PAR (pg/ml): 0.005924961515913776\n",
            "Angiopoietin-2 (pg/ml): 0.005886733687288174\n",
            "Leptin (pg/ml): 0.005667717089358752\n",
            "AXL (pg/ml): 0.005346703142962232\n",
            "CA 15-3 (U/ml): 0.004976253392856239\n",
            "Endoglin (pg/ml): 0.004713878767002766\n"
          ]
        }
      ],
      "source": [
        "# Mismo principio del papear : 5-fold cross-validations are run on the random forests of 250 Gini decision trees for 300 times to give the means and standard deviations as visualized on the error bars.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract features (X) and target variable (y)\n",
        "X = df.select_dtypes(include=['number']).drop(columns=['Tumor type']).values  # Features\n",
        "y = df['Tumor type'].values  # Target variable\n",
        "feature_names = df.select_dtypes(include=['number']).drop(columns=['Tumor type']).columns  # Feature names\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=250, criterion='gini')\n",
        "\n",
        "# Train the Random Forest model on the entire dataset\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Step 4: Perform Cross-Validation to get impurity decrease for each fold\n",
        "cv_scores = []\n",
        "for train_index, test_index in KFold(n_splits=5, shuffle=True, random_state=42).split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    # Train a model on the training data\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    \n",
        "    # Get impurity decrease for this fold\n",
        "    fold_importances = rf_classifier.feature_importances_\n",
        "    \n",
        "    # Append impurity decrease to the list\n",
        "    cv_scores.append(fold_importances)\n",
        "\n",
        "# Convert list of arrays to a NumPy array\n",
        "cv_scores = np.array(cv_scores)\n",
        "\n",
        "# Calculate mean of feature importances for each feature\n",
        "mean_feature_importances = np.mean(cv_scores, axis=0)\n",
        "\n",
        "# Combine feature names and mean importances\n",
        "feature_importance_pairs = list(zip(feature_names, mean_feature_importances))\n",
        "\n",
        "# Sort features by mean importance (highest to lowest)\n",
        "sorted_feature_importance_pairs = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display feature names and their corresponding importances\n",
        "for feature, importance in sorted_feature_importance_pairs:\n",
        "    print(f\"{feature}: {importance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.5.2.5 Métodos de prueba --> Discretizar uniformemente y por percentiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discretizar_frecuencia_uniforme(data, num_bins):\n",
        "    bins = pd.cut(data, bins=num_bins, labels=False)\n",
        "    return bins\n",
        "\n",
        "def discretizar_percentiles(data, num_bins):\n",
        "    bins = pd.qcut(data, q=num_bins, labels=False, duplicates='drop')\n",
        "    return bins\n",
        "def discretizar_uniforme_prueba_1(df):\n",
        "    df_discretizado_prueba1 = pd.DataFrame()\n",
        "    for columna in df.select_dtypes(include=['number']).columns:\n",
        "            if df[columna].dtype.kind in 'biufc': # (CHULO) Comprueba si el tipo de datos de la columna es numérico ('b' para booleano, 'i' para entero, 'u' para sin signo, 'f' para flotante o 'c' para complejo)\n",
        "                num_bins = int(1 + np.log2(len(df[columna])) + np.log2(np.std(df[columna])))\n",
        "                print(f\" Feature : {columna}, número de bins : {num_bins}\")\n",
        "                df_discretizado_prueba1[columna] = discretizar_frecuencia_uniforme(df[columna], num_bins)\n",
        "            else:\n",
        "                # Si la columna no es numérica, simplemente copiarla al dataframe resultante\n",
        "                df_discretizado_prueba1[columna] = df[columna]\n",
        "\n",
        "    # Calcular el coeficiente de correlación entre las variables numéricas discretas y la variable objetivo binaria\n",
        "    correlaciones_discretas_p1 = df_discretizado_prueba1.corrwith(df_discretizado_prueba1['Tumor type'])\n",
        "\n",
        "    # Ordenar las correlaciones de mayor a menor\n",
        "    correlaciones_discretas_ordenadas_p1 = correlaciones_discretas_p1.abs().sort_values(ascending=False)\n",
        "\n",
        "    # Obtener las top 20 variables numéricas discretas con las correlaciones más altas\n",
        "    top_20_correlaciones_discretas_p1 = correlaciones_discretas_ordenadas_p1.nlargest(20)\n",
        "\n",
        "    # Imprimir las top 20 correlaciones\n",
        "    print(top_20_correlaciones_discretas_p1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def discretizar_percentiles_prueba_1(df):\n",
        "    df_discretizado_prueba1 = pd.DataFrame()\n",
        "    for columna in df.select_dtypes(include=['number']).columns:\n",
        "        if df[columna].dtype.kind in 'biufc': # Comprueba si el tipo de datos de la columna es numérico\n",
        "            num_bins = int(1 + np.log2(len(df[columna])) + np.log2(np.std(df[columna])))\n",
        "\n",
        "            # Verificar si hay NaNs en los datos originales\n",
        "            if df[columna].isnull().any():\n",
        "                print(f\"Advertencia: La columna '{columna}' contiene valores NaN.\")\n",
        "\n",
        "            # Discretizar solo si no hay NaNs en los datos originales\n",
        "            if not df[columna].isnull().any():\n",
        "                df_discretizado_prueba1[columna] = discretizar_percentiles(df[columna], num_bins)\n",
        "                print(f\"Feature : {columna}, número de bins : {num_bins}\")\n",
        "\n",
        "        else:\n",
        "            # Si la columna no es numérica, simplemente copiarla al dataframe resultante\n",
        "            df_discretizado_prueba1[columna] = df[columna]\n",
        "\n",
        "    print(df_discretizado_prueba1.head(10))\n",
        "\n",
        "    # Verificar si hay NaNs en los datos discretizados\n",
        "    if df_discretizado_prueba1.isnull().any().any():\n",
        "        print(\"Advertencia: Se han generado valores NaN durante la discretización.\")\n",
        "\n",
        "    # Calcular el coeficiente de correlación solo si no hay NaNs en los datos discretizados\n",
        "    if not df_discretizado_prueba1.isnull().any().any():\n",
        "        correlaciones_discretas_p1 = df_discretizado_prueba1.corrwith(df_discretizado_prueba1['Tumor type'])\n",
        "\n",
        "        # Ordenar las correlaciones de mayor a menor\n",
        "        correlaciones_discretas_ordenadas_p1 = correlaciones_discretas_p1.abs().sort_values(ascending=False)\n",
        "\n",
        "        # Obtener las top 20 variables numéricas discretas con las correlaciones más altas\n",
        "        top_20_correlaciones_discretas_p1 = correlaciones_discretas_ordenadas_p1.nlargest(20)\n",
        "\n",
        "        # Imprimir las top 20 correlaciones\n",
        "        print(top_20_correlaciones_discretas_p1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Feature : Tumor type, número de bins : 10\n",
            " Feature : AFP (pg/ml), número de bins : 27\n",
            " Feature : Angiopoietin-2 (pg/ml), número de bins : 22\n",
            " Feature : AXL (pg/ml), número de bins : 22\n",
            " Feature : CA-125 (U/ml), número de bins : 19\n",
            " Feature : CA 15-3 (U/ml), número de bins : 17\n",
            " Feature : CA19-9 (U/ml), número de bins : 20\n",
            " Feature : CD44 (ng/ml), número de bins : 15\n",
            " Feature : CEA (pg/ml), número de bins : 26\n",
            " Feature : CYFRA 21-1 (pg/ml), número de bins : 27\n",
            " Feature : DKK1 (ng/ml), número de bins : 10\n",
            " Feature : Endoglin (pg/ml), número de bins : 21\n",
            " Feature : FGF2 (pg/ml), número de bins : 17\n",
            " Feature : Follistatin (pg/ml), número de bins : 21\n",
            " Feature : Galectin-3 (ng/ml), número de bins : 15\n",
            " Feature : G-CSF (pg/ml), número de bins : 20\n",
            " Feature : GDF15 (ng/ml), número de bins : 11\n",
            " Feature : HE4 (pg/ml), número de bins : 24\n",
            " Feature : HGF (pg/ml), número de bins : 20\n",
            " Feature : IL-6 (pg/ml), número de bins : 18\n",
            " Feature : IL-8 (pg/ml), número de bins : 19\n",
            " Feature : Kallikrein-6 (pg/ml), número de bins : 23\n",
            " Feature : Leptin (pg/ml), número de bins : 27\n",
            " Feature : Mesothelin (ng/ml), número de bins : 16\n",
            " Feature : Midkine (pg/ml), número de bins : 22\n",
            " Feature : Myeloperoxidase (ng/ml), número de bins : 17\n",
            " Feature : NSE (ng/ml), número de bins : 16\n",
            " Feature : OPG (ng/ml), número de bins : 11\n",
            " Feature : OPN (pg/ml), número de bins : 27\n",
            " Feature : PAR (pg/ml), número de bins : 24\n",
            " Feature : Prolactin (pg/ml), número de bins : 27\n",
            " Feature : sEGFR (pg/ml), número de bins : 22\n",
            " Feature : sFas (pg/ml), número de bins : 23\n",
            " Feature : SHBG (nM), número de bins : 17\n",
            " Feature : sHER2/sEGFR2/sErbB2 (pg/ml), número de bins : 23\n",
            " Feature : sPECAM-1 (pg/ml), número de bins : 22\n",
            " Feature : TGFa (pg/ml), número de bins : 19\n",
            " Feature : Thrombospondin-2 (pg/ml), número de bins : 25\n",
            " Feature : TIMP-1 (pg/ml), número de bins : 27\n",
            " Feature : TIMP-2 (pg/ml), número de bins : 25\n",
            " Feature : Omega score, número de bins : 16\n",
            " Feature : CancerSEEK Logistic Regression Score, número de bins : 10\n",
            "Tumor type                              1.000000\n",
            "CancerSEEK Logistic Regression Score    0.728883\n",
            "OPN (pg/ml)                             0.453184\n",
            "Prolactin (pg/ml)                       0.314408\n",
            "TIMP-1 (pg/ml)                          0.298884\n",
            "sEGFR (pg/ml)                           0.277738\n",
            "FGF2 (pg/ml)                            0.187579\n",
            "Myeloperoxidase (ng/ml)                 0.183743\n",
            "HGF (pg/ml)                             0.160781\n",
            "Angiopoietin-2 (pg/ml)                  0.160395\n",
            "Galectin-3 (ng/ml)                      0.158942\n",
            "OPG (ng/ml)                             0.152232\n",
            "Follistatin (pg/ml)                     0.139337\n",
            "HE4 (pg/ml)                             0.125086\n",
            "Omega score                             0.117354\n",
            "GDF15 (ng/ml)                           0.108558\n",
            "CEA (pg/ml)                             0.105057\n",
            "SHBG (nM)                               0.104768\n",
            "NSE (ng/ml)                             0.099467\n",
            "Thrombospondin-2 (pg/ml)                0.099423\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "discretizar_uniforme_prueba_1(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature : Tumor type, número de bins : 10\n",
            "Feature : AFP (pg/ml), número de bins : 27\n",
            "Feature : Angiopoietin-2 (pg/ml), número de bins : 22\n",
            "Feature : AXL (pg/ml), número de bins : 22\n",
            "Feature : CA-125 (U/ml), número de bins : 19\n",
            "Feature : CA 15-3 (U/ml), número de bins : 17\n",
            "Feature : CA19-9 (U/ml), número de bins : 20\n",
            "Feature : CD44 (ng/ml), número de bins : 15\n",
            "Feature : CEA (pg/ml), número de bins : 26\n",
            "Feature : CYFRA 21-1 (pg/ml), número de bins : 27\n",
            "Feature : DKK1 (ng/ml), número de bins : 10\n",
            "Feature : Endoglin (pg/ml), número de bins : 21\n",
            "Feature : FGF2 (pg/ml), número de bins : 17\n",
            "Feature : Follistatin (pg/ml), número de bins : 21\n",
            "Feature : Galectin-3 (ng/ml), número de bins : 15\n",
            "Feature : G-CSF (pg/ml), número de bins : 20\n",
            "Feature : GDF15 (ng/ml), número de bins : 11\n",
            "Feature : HE4 (pg/ml), número de bins : 24\n",
            "Feature : HGF (pg/ml), número de bins : 20\n",
            "Feature : IL-6 (pg/ml), número de bins : 18\n",
            "Feature : IL-8 (pg/ml), número de bins : 19\n",
            "Feature : Kallikrein-6 (pg/ml), número de bins : 23\n",
            "Feature : Leptin (pg/ml), número de bins : 27\n",
            "Feature : Mesothelin (ng/ml), número de bins : 16\n",
            "Feature : Midkine (pg/ml), número de bins : 22\n",
            "Feature : Myeloperoxidase (ng/ml), número de bins : 17\n",
            "Feature : NSE (ng/ml), número de bins : 16\n",
            "Feature : OPG (ng/ml), número de bins : 11\n",
            "Feature : OPN (pg/ml), número de bins : 27\n",
            "Feature : PAR (pg/ml), número de bins : 24\n",
            "Feature : Prolactin (pg/ml), número de bins : 27\n",
            "Feature : sEGFR (pg/ml), número de bins : 22\n",
            "Feature : sFas (pg/ml), número de bins : 23\n",
            "Feature : SHBG (nM), número de bins : 17\n",
            "Feature : sHER2/sEGFR2/sErbB2 (pg/ml), número de bins : 23\n",
            "Feature : sPECAM-1 (pg/ml), número de bins : 22\n",
            "Feature : TGFa (pg/ml), número de bins : 19\n",
            "Feature : Thrombospondin-2 (pg/ml), número de bins : 25\n",
            "Feature : TIMP-1 (pg/ml), número de bins : 27\n",
            "Feature : TIMP-2 (pg/ml), número de bins : 25\n",
            "Feature : Omega score, número de bins : 16\n",
            "Feature : CancerSEEK Logistic Regression Score, número de bins : 10\n",
            "   Tumor type  AFP (pg/ml)  Angiopoietin-2 (pg/ml)  AXL (pg/ml)  \\\n",
            "0           0           19                      21           18   \n",
            "1           0            0                      21           15   \n",
            "2           0           24                      16           20   \n",
            "3           0            0                      12           10   \n",
            "4           0            4                      15           10   \n",
            "5           0            0                       7           10   \n",
            "6           0           20                      14           18   \n",
            "7           0           15                      13           13   \n",
            "8           0           16                       6            2   \n",
            "9           0           25                      14            7   \n",
            "\n",
            "   CA-125 (U/ml)  CA 15-3 (U/ml)  CA19-9 (U/ml)  CD44 (ng/ml)  CEA (pg/ml)  \\\n",
            "0             11              12              8             1            4   \n",
            "1             14               6             18            12           24   \n",
            "2              3              11              8             5           12   \n",
            "3             11               5              8             0           19   \n",
            "4              3               8              8             3            6   \n",
            "5              3               2             18             0           14   \n",
            "6             13              11              8             3            8   \n",
            "7              3               7              8             3            3   \n",
            "8             12               3             17             0           11   \n",
            "9              3               4              8             2            9   \n",
            "\n",
            "   CYFRA 21-1 (pg/ml)  ...  sFas (pg/ml)  SHBG (nM)  \\\n",
            "0                   2  ...             4          8   \n",
            "1                   2  ...             4         11   \n",
            "2                  11  ...             4         16   \n",
            "3                   2  ...             4          3   \n",
            "4                   2  ...             4         11   \n",
            "5                   2  ...             4         16   \n",
            "6                  21  ...            22          9   \n",
            "7                   2  ...             4         16   \n",
            "8                  11  ...             4         15   \n",
            "9                   2  ...             4         16   \n",
            "\n",
            "   sHER2/sEGFR2/sErbB2 (pg/ml)  sPECAM-1 (pg/ml)  TGFa (pg/ml)  \\\n",
            "0                           18                20             2   \n",
            "1                           12                13             2   \n",
            "2                            3                 3            18   \n",
            "3                           14                13             2   \n",
            "4                           12                16             2   \n",
            "5                           16                18             2   \n",
            "6                           21                18            17   \n",
            "7                            9                 6             2   \n",
            "8                            5                 6             2   \n",
            "9                            7                10             2   \n",
            "\n",
            "   Thrombospondin-2 (pg/ml)  TIMP-1 (pg/ml)  TIMP-2 (pg/ml)  Omega score  \\\n",
            "0                        24              12              13           13   \n",
            "1                        24              18              15           13   \n",
            "2                        19               1               4           11   \n",
            "3                        16               0               2           12   \n",
            "4                        13              12              19           11   \n",
            "5                        21               8              19           13   \n",
            "6                        22               0               1            8   \n",
            "7                         8               0               3            4   \n",
            "8                         8               0               4            6   \n",
            "9                        11               0               0            7   \n",
            "\n",
            "   CancerSEEK Logistic Regression Score  \n",
            "0                                     6  \n",
            "1                                     6  \n",
            "2                                     6  \n",
            "3                                     5  \n",
            "4                                     4  \n",
            "5                                     7  \n",
            "6                                     4  \n",
            "7                                     5  \n",
            "8                                     5  \n",
            "9                                     4  \n",
            "\n",
            "[10 rows x 42 columns]\n",
            "Tumor type               NaN\n",
            "AFP (pg/ml)              NaN\n",
            "Angiopoietin-2 (pg/ml)   NaN\n",
            "AXL (pg/ml)              NaN\n",
            "CA-125 (U/ml)            NaN\n",
            "CA 15-3 (U/ml)           NaN\n",
            "CA19-9 (U/ml)            NaN\n",
            "CD44 (ng/ml)             NaN\n",
            "CEA (pg/ml)              NaN\n",
            "CYFRA 21-1 (pg/ml)       NaN\n",
            "DKK1 (ng/ml)             NaN\n",
            "Endoglin (pg/ml)         NaN\n",
            "FGF2 (pg/ml)             NaN\n",
            "Follistatin (pg/ml)      NaN\n",
            "Galectin-3 (ng/ml)       NaN\n",
            "G-CSF (pg/ml)            NaN\n",
            "GDF15 (ng/ml)            NaN\n",
            "HE4 (pg/ml)              NaN\n",
            "HGF (pg/ml)              NaN\n",
            "IL-6 (pg/ml)             NaN\n",
            "dtype: float64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "c:\\Users\\FloCr\\.anaconda\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        }
      ],
      "source": [
        "# No funciona, muestra todas las corelaciones como NaN\n",
        "discretizar_percentiles_prueba_1(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5.3 Comparación de resultados de information gain\n",
        "\n",
        "- Método 1 - Variables continuas, variable objetivo binaria --> Correlación de Pearson\n",
        "- Método 2 - Variables discretas (KBinsDiscretizer()), variable objetivo binaria --> Correlación \n",
        "- Método 3 - Variables discretizado (Arbol de decisión (max_depth = 15)), variable objetivo binaria --> Correlación\n",
        "\n",
        " ------------- Método 1 ---------------------------------- Método 2 --------------------------- Método 3 -----------\n",
        "\n",
        "|Variable|Valor|Variable|Valor|Variable|Valor|\n",
        "|------------------------|--------------|-----------|-----------|--------------|------------|\n",
        "| OPN (pg/ml)          |                 0.458352 | IL-8 (pg/ml)                 |           0.578037 |OPN (pg/ml)                  |         0.575480\n",
        "Prolactin (pg/ml)       |                0.324378 | OPN (pg/ml)                   |          0.571791 |IL-6 (pg/ml)                  |           0.483620\n",
        "TIMP-1 (pg/ml)          |                0.300539 | IL-6 (pg/ml)                  |          0.480112 |IL-8 (pg/ml)                  |          0.464828\n",
        "GDF15 (ng/ml)           |                0.247428 | GDF15 (ng/ml)                 |          0.474477 |HGF (pg/ml)                |          0.454991\n",
        "HGF (pg/ml)              |               0.242512 | Prolactin (pg/ml)              |         0.452567 |Prolactin (pg/ml)              |         0.453270\n",
        "Myeloperoxidase (ng/ml)  |               0.221877 | HGF (pg/ml)                   |          0.447965 |Omega score                    |        0.378112\n",
        "FGF2 (pg/ml)              |              0.192212 | Omega score                   |          0.363410 |GDF15 (ng/ml)                   |         0.365248\n",
        "IL-6 (pg/ml)              |              0.186625 | Myeloperoxidase (ng/ml)       |          0.344990 |CYFRA 21-1 (pg/ml)         |         0.356245\n",
        "Galectin-3 (ng/ml)        |              0.180836 | sEGFR (pg/ml)                 |          0.320824 |Myeloperoxidase (ng/ml)                 |          0.351481\n",
        "Angiopoietin-2 (pg/ml)     |             0.170233 | CA19-9 (U/ml)                 |          0.311742 |sEGFR (pg/ml)                   |        0.319982\n",
        "OPG (ng/ml)                |             0.147721 | TGFa (pg/ml)                   |         0.302511 |CA-125 (U/ml)                  |          0.312094\n",
        "Omega score                |             0.146759 | TIMP-1 (pg/ml)                |          0.302504 |CEA (pg/ml)                |            0.308045\n",
        "HE4 (pg/ml)                 |            0.139689 | CEA (pg/ml)                    |         0.300193 |TIMP-1 (pg/ml)                  |          0.301340\n",
        "Follistatin (pg/ml)         |            0.137546 | CA-125 (U/ml)                 |         0.295434  |CA19-9 (U/ml)               |          0.266444\n",
        "CEA (pg/ml)                 |            0.126718 | | | Angiopoietin-2 (pg/ml) |0.233881"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PAPER : \"Contrary to the information gain ranking on Table S1, the cancer\n",
        "antigen markers are no longer the top predictive features. Instead, we observe the opposite trend for the\n",
        "purity and accuracy measurements; such a phenomenon exemplifies the underlying complex behavior for\n",
        "cancer detection\"\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
